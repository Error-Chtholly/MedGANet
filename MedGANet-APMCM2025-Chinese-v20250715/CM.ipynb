{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b18575bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据前5行:\n",
      "   N_Days  Status  Age  Ascites  Hepatomegaly  Spiders  Edema  Bilirubin  \\\n",
      "0     400       2   59      2.0           2.0      2.0      2       14.5   \n",
      "1    4500       0   56      0.0           2.0      2.0      0        1.1   \n",
      "2    1012       2   70      0.0           0.0      0.0      1        1.4   \n",
      "3    1925       2   55      0.0           2.0      2.0      1        1.8   \n",
      "4    1504       1   38      0.0           2.0      2.0      0        3.4   \n",
      "\n",
      "   Albumin  Copper    SGOT  Tryglicerides  Platelets  Prothrombin  Stage  \n",
      "0     2.60   156.0  137.95          172.0      190.0         12.2    4.0  \n",
      "1     4.14    54.0  113.52           88.0      221.0         10.6    3.0  \n",
      "2     3.48   210.0   96.10           55.0      151.0         12.0    4.0  \n",
      "3     2.54    64.0   60.63           92.0      183.0         10.3    4.0  \n",
      "4     3.53   143.0  113.15           72.0      136.0         10.9    3.0  \n",
      "\n",
      "类别分布:\n",
      "Stage\n",
      "3.0    161\n",
      "4.0    144\n",
      "2.0     92\n",
      "1.0     21\n",
      "Name: count, dtype: int64\n",
      "数据形状: X=(418, 9), y=(418,)\n",
      "类别分布: [ 21  92 161 144]\n",
      "\n",
      "==================================================\n",
      "Fold 1/10\n",
      "==================================================\n",
      "\n",
      "Fold 1 Test Metrics:\n",
      "Accuracy: 0.5952\n",
      "F1 Score: 0.5907\n",
      "Precision: 0.5985\n",
      "Recall: 0.5952\n",
      "AUPRC: 0.6702\n",
      "AUROC: 0.7938\n",
      "\n",
      "==================================================\n",
      "Fold 2/10\n",
      "==================================================\n",
      "\n",
      "Fold 2 Test Metrics:\n",
      "Accuracy: 0.4762\n",
      "F1 Score: 0.4491\n",
      "Precision: 0.4822\n",
      "Recall: 0.4762\n",
      "AUPRC: 0.4999\n",
      "AUROC: 0.6302\n",
      "\n",
      "==================================================\n",
      "Fold 3/10\n",
      "==================================================\n",
      "\n",
      "Fold 3 Test Metrics:\n",
      "Accuracy: 0.4524\n",
      "F1 Score: 0.4626\n",
      "Precision: 0.4755\n",
      "Recall: 0.4524\n",
      "AUPRC: 0.5003\n",
      "AUROC: 0.6621\n",
      "\n",
      "==================================================\n",
      "Fold 4/10\n",
      "==================================================\n",
      "\n",
      "Fold 4 Test Metrics:\n",
      "Accuracy: 0.4524\n",
      "F1 Score: 0.4175\n",
      "Precision: 0.4030\n",
      "Recall: 0.4524\n",
      "AUPRC: 0.5199\n",
      "AUROC: 0.6750\n",
      "\n",
      "==================================================\n",
      "Fold 5/10\n",
      "==================================================\n",
      "\n",
      "Fold 5 Test Metrics:\n",
      "Accuracy: 0.5000\n",
      "F1 Score: 0.4791\n",
      "Precision: 0.4717\n",
      "Recall: 0.5000\n",
      "AUPRC: 0.4764\n",
      "AUROC: 0.6444\n",
      "\n",
      "==================================================\n",
      "Fold 6/10\n",
      "==================================================\n",
      "\n",
      "Fold 6 Test Metrics:\n",
      "Accuracy: 0.5238\n",
      "F1 Score: 0.5038\n",
      "Precision: 0.4988\n",
      "Recall: 0.5238\n",
      "AUPRC: 0.6110\n",
      "AUROC: 0.7368\n",
      "\n",
      "==================================================\n",
      "Fold 7/10\n",
      "==================================================\n",
      "\n",
      "Fold 7 Test Metrics:\n",
      "Accuracy: 0.3571\n",
      "F1 Score: 0.3410\n",
      "Precision: 0.3472\n",
      "Recall: 0.3571\n",
      "AUPRC: 0.4211\n",
      "AUROC: 0.5855\n",
      "\n",
      "==================================================\n",
      "Fold 8/10\n",
      "==================================================\n",
      "\n",
      "Fold 8 Test Metrics:\n",
      "Accuracy: 0.4286\n",
      "F1 Score: 0.3916\n",
      "Precision: 0.3718\n",
      "Recall: 0.4286\n",
      "AUPRC: 0.4373\n",
      "AUROC: 0.6068\n",
      "\n",
      "==================================================\n",
      "Fold 9/10\n",
      "==================================================\n",
      "\n",
      "Fold 9 Test Metrics:\n",
      "Accuracy: 0.4634\n",
      "F1 Score: 0.4516\n",
      "Precision: 0.4421\n",
      "Recall: 0.4634\n",
      "AUPRC: 0.5601\n",
      "AUROC: 0.6489\n",
      "\n",
      "==================================================\n",
      "Fold 10/10\n",
      "==================================================\n",
      "\n",
      "Fold 10 Test Metrics:\n",
      "Accuracy: 0.3415\n",
      "F1 Score: 0.3385\n",
      "Precision: 0.3432\n",
      "Recall: 0.3415\n",
      "AUPRC: 0.4064\n",
      "AUROC: 0.5038\n",
      "\n",
      "Saved best model from fold 1 with AUROC 0.7938 as cirrhosis_xgb.pkl\n",
      "Saved corresponding scaler as scaler.pkl\n",
      "Plotting confusion matrix for best fold: 1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import (accuracy_score, f1_score, precision_score,\n",
    "                             recall_score, average_precision_score,\n",
    "                             precision_recall_curve, auc, roc_auc_score, confusion_matrix)\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import interpolate\n",
    "import joblib\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "def calculate_metrics(y_true, y_pred, y_scores):\n",
    "    # 检查NaN值\n",
    "    if np.isnan(y_scores).any():\n",
    "        y_scores = np.nan_to_num(y_scores)\n",
    "\n",
    "    # 初始化存储每个类别的指标\n",
    "    class_metrics = []\n",
    "\n",
    "    for class_idx in range(4):  # 4个类别\n",
    "        # 确保y_true是numpy数组\n",
    "        y_true_np = np.array(y_true)\n",
    "        y_pred_np = np.array(y_pred)\n",
    "\n",
    "        # 二分类指标计算\n",
    "        y_true_class = (y_true_np == class_idx).astype(int)\n",
    "        y_pred_class = (y_pred_np == class_idx).astype(int)\n",
    "        y_scores_class = y_scores[:, class_idx]\n",
    "\n",
    "        try:\n",
    "            accuracy = accuracy_score(y_true_class, y_pred_class)\n",
    "            f1 = f1_score(y_true_class, y_pred_class, zero_division=0)\n",
    "            precision = precision_score(y_true_class, y_pred_class, zero_division=0)\n",
    "            recall = recall_score(y_true_class, y_pred_class, zero_division=0)\n",
    "\n",
    "            # 处理AUPRC计算\n",
    "            if len(np.unique(y_true_class)) > 1:\n",
    "                auprc = average_precision_score(y_true_class, y_scores_class)\n",
    "            else:\n",
    "                auprc = 0.0\n",
    "\n",
    "            # 处理AUROC计算\n",
    "            if len(np.unique(y_true_class)) > 1:\n",
    "                auroc = roc_auc_score(y_true_class, y_scores_class)\n",
    "            else:\n",
    "                auroc = 0.0\n",
    "\n",
    "            class_metrics.append({\n",
    "                'accuracy': accuracy,\n",
    "                'f1': f1,\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'auprc': auprc,\n",
    "                'auroc': auroc\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"计算类别{class_idx}指标时出错: {str(e)}\")\n",
    "            class_metrics.append({\n",
    "                'accuracy': 0,\n",
    "                'f1': 0,\n",
    "                'precision': 0,\n",
    "                'recall': 0,\n",
    "                'auprc': 0,\n",
    "                'auroc': 0\n",
    "            })\n",
    "\n",
    "    # 计算加权平均指标\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_true, y_pred),\n",
    "        'f1': f1_score(y_true, y_pred, average='weighted', zero_division=0),\n",
    "        'precision': precision_score(y_true, y_pred, average='weighted', zero_division=0),\n",
    "        'recall': recall_score(y_true, y_pred, average='weighted', zero_division=0),\n",
    "        'auprc': average_precision_score(y_true, y_scores, average='weighted'),\n",
    "        'auroc': roc_auc_score(y_true, y_scores, multi_class='ovr', average='weighted'),\n",
    "        'class_metrics': class_metrics\n",
    "    }\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def interpolate_pr_curve(precision, recall):\n",
    "    \"\"\"插值PR曲线到固定长度的点\"\"\"\n",
    "    f = interpolate.interp1d(recall, precision, bounds_error=False, fill_value=(1.0, 0.0))\n",
    "    new_recall = np.linspace(0, 1, 100)\n",
    "    new_precision = f(new_recall)\n",
    "    return new_precision, new_recall\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, fold, dpi=720):\n",
    "    \"\"\"绘制混淆矩阵\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    cm_percentage = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100  # 百分比表示\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    ax = sns.heatmap(cm_percentage, annot=False, fmt='.2f', cmap='Blues', square=True, cbar=False,\n",
    "                     linewidths=2, linecolor='black')\n",
    "\n",
    "    # 在每个格子中显示个数和百分比\n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            # 判断字体颜色，深色背景用白色字体，浅色背景用黑色字体\n",
    "            text_color = 'white' if cm_percentage[i, j] > 50 else 'black'\n",
    "\n",
    "            # 将个数和百分百分比显示在格子中\n",
    "            ax.text(j + 0.5, i + 0.5, f'{cm[i, j]}\\n({cm_percentage[i, j]:.2f}%)',\n",
    "                    color=text_color, ha='center', va='center', fontsize=14, fontweight='bold')\n",
    "\n",
    "    # 添加中文标签\n",
    "    plt.xlabel('预测类别', fontsize=16, fontweight='bold')\n",
    "    plt.ylabel('实际类别', fontsize=16, fontweight='bold')\n",
    "    plt.xticks(ticks=np.arange(4) + 0.5, labels=np.arange(1, 5), fontsize=14, fontweight='bold')\n",
    "    plt.yticks(ticks=np.arange(4) + 0.5, labels=np.arange(1, 5), fontsize=14, fontweight='bold')\n",
    "\n",
    "    # 调整布局，减少空白边缘\n",
    "    plt.subplots_adjust(left=0.1, right=0.9, top=0.9, bottom=0.1)\n",
    "\n",
    "    # 保存混淆矩阵图\n",
    "    plt.savefig(f'CI_XGB_best_fold_confusion_matrix_fold{fold}.png', dpi=dpi)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def train_test_split(X, y, splits=10, batch_size=32):\n",
    "    # 检查数据\n",
    "    print(f\"数据形状: X={X.shape}, y={y.shape}\")\n",
    "    print(f\"类别分布: {np.bincount(y)}\")\n",
    "\n",
    "    # 处理可能的NaN值\n",
    "    X = np.nan_to_num(X)\n",
    "    y = np.nan_to_num(y).astype(int)\n",
    "\n",
    "    k_fold = StratifiedKFold(n_splits=splits, shuffle=True, random_state=2025)\n",
    "    results = []\n",
    "    best_model_info = {'val_score': -float('inf'), 'model': None, 'fold': -1}\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(k_fold.split(X, y)):\n",
    "        print(f'\\n{\"=\" * 50}')\n",
    "        print(f'Fold {fold + 1}/{splits}')\n",
    "        print(f'{\"=\" * 50}')\n",
    "\n",
    "        # 数据切分\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "        # 标准化\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "        # 使用XGBoost替代RandomForest\n",
    "        xgb_model = xgb.XGBClassifier(n_estimators=100, random_state=2025, n_jobs=-1, objective='multi:softprob', num_class=4)\n",
    "\n",
    "        # 训练模型\n",
    "        xgb_model.fit(X_train, y_train)\n",
    "\n",
    "        # 预测\n",
    "        y_pred = xgb_model.predict_proba(X_test)  # 预测类别概率\n",
    "        y_pred_class = np.argmax(y_pred, axis=1)\n",
    "\n",
    "        # 计算指标\n",
    "        metrics = calculate_metrics(y_test, y_pred_class, y_pred)\n",
    "\n",
    "        # 检查是否为最佳模型\n",
    "        current_score = metrics['auroc']  # 使用AUROC作为选择最佳模型的标准\n",
    "        if current_score > best_model_info['val_score']:\n",
    "            best_model_info['val_score'] = current_score\n",
    "            best_model_info['model'] = xgb_model\n",
    "            best_model_info['fold'] = fold + 1\n",
    "            best_model_info['scaler'] = scaler\n",
    "\n",
    "        # 保存结果\n",
    "        results.append(metrics)\n",
    "\n",
    "        # 打印当前折的结果\n",
    "        print(f'\\nFold {fold + 1} Test Metrics:')\n",
    "        print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
    "        print(f\"F1 Score: {metrics['f1']:.4f}\")\n",
    "        print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "        print(f\"Recall: {metrics['recall']:.4f}\")\n",
    "        print(f\"AUPRC: {metrics['auprc']:.4f}\")\n",
    "        print(f\"AUROC: {metrics['auroc']:.4f}\")\n",
    "\n",
    "    # 保存最佳模型\n",
    "    joblib.dump(best_model_info['model'], 'cirrhosis_xgb.pkl')\n",
    "    joblib.dump(best_model_info['scaler'], 'scaler.pkl')\n",
    "\n",
    "    print(f\"\\nSaved best model from fold {best_model_info['fold']} with AUROC {best_model_info['val_score']:.4f} as cirrhosis_xgb.pkl\")\n",
    "    print(\"Saved corresponding scaler as scaler.pkl\")\n",
    "\n",
    "    # 绘制最佳折的混淆矩阵\n",
    "    print(f\"Plotting confusion matrix for best fold: {best_model_info['fold']}\")\n",
    "    plot_confusion_matrix(y_test, y_pred_class, best_model_info['fold'])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 加载数据\n",
    "    data = pd.read_csv('preparations/cirrhosis_output.csv')  # 请替换为您的实际文件路径\n",
    "\n",
    "    # 检查数据\n",
    "    print(\"数据前5行:\")\n",
    "    print(data.head())\n",
    "    print(\"\\n类别分布:\")\n",
    "    print(data['Stage'].value_counts())\n",
    "\n",
    "    # 分离特征和目标\n",
    "    feature_cols = ['N_Days', 'Age', 'Bilirubin', 'Albumin', 'Copper', 'SGOT',\n",
    "                    'Tryglicerides', 'Platelets', 'Prothrombin']\n",
    "    X = data[feature_cols].values\n",
    "    y = data['Stage'].values - 1  # 将类别转换为0-3\n",
    "\n",
    "    # 转换为numpy数组\n",
    "    X = X.astype(np.float32)\n",
    "    y = y.astype(np.int64)\n",
    "\n",
    "    # 运行训练和评估\n",
    "    train_test_split(X, y, splits=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "528f5f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据前5行:\n",
      "   N_Days  Status  Age  Ascites  Hepatomegaly  Spiders  Edema  Bilirubin  \\\n",
      "0     400       2   59      2.0           2.0      2.0      2       14.5   \n",
      "1    4500       0   56      0.0           2.0      2.0      0        1.1   \n",
      "2    1012       2   70      0.0           0.0      0.0      1        1.4   \n",
      "3    1925       2   55      0.0           2.0      2.0      1        1.8   \n",
      "4    1504       1   38      0.0           2.0      2.0      0        3.4   \n",
      "\n",
      "   Albumin  Copper    SGOT  Tryglicerides  Platelets  Prothrombin  Stage  \n",
      "0     2.60   156.0  137.95          172.0      190.0         12.2    4.0  \n",
      "1     4.14    54.0  113.52           88.0      221.0         10.6    3.0  \n",
      "2     3.48   210.0   96.10           55.0      151.0         12.0    4.0  \n",
      "3     2.54    64.0   60.63           92.0      183.0         10.3    4.0  \n",
      "4     3.53   143.0  113.15           72.0      136.0         10.9    3.0  \n",
      "\n",
      "类别分布:\n",
      "Stage\n",
      "3.0    161\n",
      "4.0    144\n",
      "2.0     92\n",
      "1.0     21\n",
      "Name: count, dtype: int64\n",
      "数据形状: X=(418, 9), y=(418,)\n",
      "类别分布: [ 21  92 161 144]\n",
      "\n",
      "==================================================\n",
      "Fold 1/10\n",
      "==================================================\n",
      "epoch 0  | loss: 1.59251 | val_0_accuracy: 0.38095 |  0:00:00s\n",
      "epoch 1  | loss: 1.34225 | val_0_accuracy: 0.40476 |  0:00:00s\n",
      "epoch 2  | loss: 1.22876 | val_0_accuracy: 0.5     |  0:00:00s\n",
      "epoch 3  | loss: 1.18744 | val_0_accuracy: 0.47619 |  0:00:01s\n",
      "epoch 4  | loss: 1.1686  | val_0_accuracy: 0.5     |  0:00:01s\n",
      "epoch 5  | loss: 1.16666 | val_0_accuracy: 0.59524 |  0:00:01s\n",
      "epoch 6  | loss: 1.11915 | val_0_accuracy: 0.52381 |  0:00:01s\n",
      "epoch 7  | loss: 1.13766 | val_0_accuracy: 0.54762 |  0:00:01s\n",
      "epoch 8  | loss: 1.09175 | val_0_accuracy: 0.54762 |  0:00:02s\n",
      "epoch 9  | loss: 1.17163 | val_0_accuracy: 0.61905 |  0:00:02s\n",
      "epoch 10 | loss: 1.11026 | val_0_accuracy: 0.47619 |  0:00:02s\n",
      "epoch 11 | loss: 1.10085 | val_0_accuracy: 0.54762 |  0:00:02s\n",
      "epoch 12 | loss: 1.11085 | val_0_accuracy: 0.52381 |  0:00:03s\n",
      "epoch 13 | loss: 1.14476 | val_0_accuracy: 0.59524 |  0:00:03s\n",
      "epoch 14 | loss: 1.09755 | val_0_accuracy: 0.61905 |  0:00:03s\n",
      "epoch 15 | loss: 1.07414 | val_0_accuracy: 0.64286 |  0:00:04s\n",
      "epoch 16 | loss: 1.06948 | val_0_accuracy: 0.59524 |  0:00:04s\n",
      "epoch 17 | loss: 1.07077 | val_0_accuracy: 0.57143 |  0:00:04s\n",
      "epoch 18 | loss: 1.09263 | val_0_accuracy: 0.61905 |  0:00:04s\n",
      "epoch 19 | loss: 1.08262 | val_0_accuracy: 0.59524 |  0:00:05s\n",
      "epoch 20 | loss: 1.06205 | val_0_accuracy: 0.61905 |  0:00:05s\n",
      "epoch 21 | loss: 1.08699 | val_0_accuracy: 0.57143 |  0:00:05s\n",
      "epoch 22 | loss: 1.06791 | val_0_accuracy: 0.64286 |  0:00:05s\n",
      "epoch 23 | loss: 1.06969 | val_0_accuracy: 0.57143 |  0:00:05s\n",
      "epoch 24 | loss: 1.03973 | val_0_accuracy: 0.54762 |  0:00:06s\n",
      "epoch 25 | loss: 1.05192 | val_0_accuracy: 0.57143 |  0:00:06s\n",
      "epoch 26 | loss: 1.05799 | val_0_accuracy: 0.61905 |  0:00:06s\n",
      "epoch 27 | loss: 1.0752  | val_0_accuracy: 0.57143 |  0:00:07s\n",
      "epoch 28 | loss: 1.01243 | val_0_accuracy: 0.52381 |  0:00:07s\n",
      "epoch 29 | loss: 1.06846 | val_0_accuracy: 0.59524 |  0:00:07s\n",
      "epoch 30 | loss: 1.09022 | val_0_accuracy: 0.64286 |  0:00:08s\n",
      "epoch 31 | loss: 1.09705 | val_0_accuracy: 0.61905 |  0:00:08s\n",
      "epoch 32 | loss: 1.06804 | val_0_accuracy: 0.61905 |  0:00:08s\n",
      "epoch 33 | loss: 1.05978 | val_0_accuracy: 0.54762 |  0:00:08s\n",
      "epoch 34 | loss: 1.09899 | val_0_accuracy: 0.61905 |  0:00:09s\n",
      "epoch 35 | loss: 1.0415  | val_0_accuracy: 0.59524 |  0:00:09s\n",
      "epoch 36 | loss: 1.06682 | val_0_accuracy: 0.64286 |  0:00:09s\n",
      "epoch 37 | loss: 1.11451 | val_0_accuracy: 0.66667 |  0:00:09s\n",
      "epoch 38 | loss: 1.00543 | val_0_accuracy: 0.66667 |  0:00:10s\n",
      "epoch 39 | loss: 1.05217 | val_0_accuracy: 0.61905 |  0:00:10s\n",
      "epoch 40 | loss: 1.04474 | val_0_accuracy: 0.59524 |  0:00:10s\n",
      "epoch 41 | loss: 1.05015 | val_0_accuracy: 0.54762 |  0:00:11s\n",
      "epoch 42 | loss: 1.04774 | val_0_accuracy: 0.52381 |  0:00:11s\n",
      "epoch 43 | loss: 1.06231 | val_0_accuracy: 0.59524 |  0:00:11s\n",
      "epoch 44 | loss: 1.04488 | val_0_accuracy: 0.42857 |  0:00:11s\n",
      "epoch 45 | loss: 1.04088 | val_0_accuracy: 0.47619 |  0:00:12s\n",
      "epoch 46 | loss: 1.05045 | val_0_accuracy: 0.52381 |  0:00:12s\n",
      "epoch 47 | loss: 1.04596 | val_0_accuracy: 0.61905 |  0:00:12s\n",
      "epoch 48 | loss: 1.07009 | val_0_accuracy: 0.57143 |  0:00:13s\n",
      "epoch 49 | loss: 1.04699 | val_0_accuracy: 0.54762 |  0:00:13s\n",
      "epoch 50 | loss: 1.06121 | val_0_accuracy: 0.5     |  0:00:13s\n",
      "epoch 51 | loss: 1.06955 | val_0_accuracy: 0.54762 |  0:00:14s\n",
      "epoch 52 | loss: 1.03618 | val_0_accuracy: 0.5     |  0:00:14s\n",
      "epoch 53 | loss: 0.97972 | val_0_accuracy: 0.54762 |  0:00:14s\n",
      "epoch 54 | loss: 0.99504 | val_0_accuracy: 0.54762 |  0:00:14s\n",
      "epoch 55 | loss: 1.02763 | val_0_accuracy: 0.54762 |  0:00:15s\n",
      "epoch 56 | loss: 1.0403  | val_0_accuracy: 0.59524 |  0:00:15s\n",
      "epoch 57 | loss: 1.03811 | val_0_accuracy: 0.59524 |  0:00:15s\n",
      "epoch 58 | loss: 1.02683 | val_0_accuracy: 0.64286 |  0:00:16s\n",
      "epoch 59 | loss: 1.02239 | val_0_accuracy: 0.66667 |  0:00:16s\n",
      "epoch 60 | loss: 0.99451 | val_0_accuracy: 0.64286 |  0:00:16s\n",
      "epoch 61 | loss: 0.99313 | val_0_accuracy: 0.61905 |  0:00:16s\n",
      "epoch 62 | loss: 1.00143 | val_0_accuracy: 0.59524 |  0:00:17s\n",
      "epoch 63 | loss: 1.01461 | val_0_accuracy: 0.57143 |  0:00:17s\n",
      "epoch 64 | loss: 1.01176 | val_0_accuracy: 0.64286 |  0:00:17s\n",
      "epoch 65 | loss: 0.98865 | val_0_accuracy: 0.64286 |  0:00:17s\n",
      "epoch 66 | loss: 0.98205 | val_0_accuracy: 0.57143 |  0:00:18s\n",
      "epoch 67 | loss: 0.97627 | val_0_accuracy: 0.54762 |  0:00:18s\n",
      "epoch 68 | loss: 0.9897  | val_0_accuracy: 0.61905 |  0:00:18s\n",
      "epoch 69 | loss: 0.97126 | val_0_accuracy: 0.66667 |  0:00:18s\n",
      "epoch 70 | loss: 1.00779 | val_0_accuracy: 0.59524 |  0:00:19s\n",
      "epoch 71 | loss: 1.05029 | val_0_accuracy: 0.52381 |  0:00:19s\n",
      "epoch 72 | loss: 1.01474 | val_0_accuracy: 0.54762 |  0:00:19s\n",
      "epoch 73 | loss: 1.02944 | val_0_accuracy: 0.54762 |  0:00:19s\n",
      "epoch 74 | loss: 0.99644 | val_0_accuracy: 0.57143 |  0:00:20s\n",
      "epoch 75 | loss: 1.02487 | val_0_accuracy: 0.61905 |  0:00:20s\n",
      "epoch 76 | loss: 0.98349 | val_0_accuracy: 0.59524 |  0:00:20s\n",
      "epoch 77 | loss: 1.00129 | val_0_accuracy: 0.54762 |  0:00:21s\n",
      "epoch 78 | loss: 0.96909 | val_0_accuracy: 0.52381 |  0:00:21s\n",
      "epoch 79 | loss: 0.96975 | val_0_accuracy: 0.57143 |  0:00:21s\n",
      "epoch 80 | loss: 1.00744 | val_0_accuracy: 0.54762 |  0:00:21s\n",
      "epoch 81 | loss: 0.94047 | val_0_accuracy: 0.59524 |  0:00:22s\n",
      "epoch 82 | loss: 0.97629 | val_0_accuracy: 0.52381 |  0:00:22s\n",
      "epoch 83 | loss: 0.9543  | val_0_accuracy: 0.52381 |  0:00:23s\n",
      "epoch 84 | loss: 0.98244 | val_0_accuracy: 0.5     |  0:00:23s\n",
      "epoch 85 | loss: 0.96162 | val_0_accuracy: 0.54762 |  0:00:24s\n",
      "epoch 86 | loss: 0.97207 | val_0_accuracy: 0.54762 |  0:00:24s\n",
      "epoch 87 | loss: 0.98424 | val_0_accuracy: 0.57143 |  0:00:25s\n",
      "\n",
      "Early stopping occurred at epoch 87 with best_epoch = 37 and best_val_0_accuracy = 0.66667\n",
      "\n",
      "Fold 1 Test Metrics:\n",
      "Accuracy: 0.6667\n",
      "F1 Score: 0.6344\n",
      "Precision: 0.6168\n",
      "Recall: 0.6667\n",
      "AUPRC: 0.6188\n",
      "AUROC: 0.7788\n",
      "\n",
      "==================================================\n",
      "Fold 2/10\n",
      "==================================================\n",
      "epoch 0  | loss: 1.70362 | val_0_accuracy: 0.28571 |  0:00:00s\n",
      "epoch 1  | loss: 1.37734 | val_0_accuracy: 0.45238 |  0:00:00s\n",
      "epoch 2  | loss: 1.22565 | val_0_accuracy: 0.33333 |  0:00:01s\n",
      "epoch 3  | loss: 1.17822 | val_0_accuracy: 0.35714 |  0:00:01s\n",
      "epoch 4  | loss: 1.14905 | val_0_accuracy: 0.35714 |  0:00:02s\n",
      "epoch 5  | loss: 1.15407 | val_0_accuracy: 0.42857 |  0:00:02s\n",
      "epoch 6  | loss: 1.15745 | val_0_accuracy: 0.47619 |  0:00:02s\n",
      "epoch 7  | loss: 1.12611 | val_0_accuracy: 0.47619 |  0:00:03s\n",
      "epoch 8  | loss: 1.08065 | val_0_accuracy: 0.52381 |  0:00:03s\n",
      "epoch 9  | loss: 1.07011 | val_0_accuracy: 0.45238 |  0:00:04s\n",
      "epoch 10 | loss: 1.07943 | val_0_accuracy: 0.54762 |  0:00:04s\n",
      "epoch 11 | loss: 1.03016 | val_0_accuracy: 0.38095 |  0:00:04s\n",
      "epoch 12 | loss: 1.02545 | val_0_accuracy: 0.35714 |  0:00:05s\n",
      "epoch 13 | loss: 1.05574 | val_0_accuracy: 0.40476 |  0:00:05s\n",
      "epoch 14 | loss: 1.01307 | val_0_accuracy: 0.40476 |  0:00:06s\n",
      "epoch 15 | loss: 1.03743 | val_0_accuracy: 0.5     |  0:00:06s\n",
      "epoch 16 | loss: 1.05564 | val_0_accuracy: 0.42857 |  0:00:06s\n",
      "epoch 17 | loss: 1.04156 | val_0_accuracy: 0.45238 |  0:00:07s\n",
      "epoch 18 | loss: 1.02003 | val_0_accuracy: 0.45238 |  0:00:07s\n",
      "epoch 19 | loss: 1.03634 | val_0_accuracy: 0.45238 |  0:00:08s\n",
      "epoch 20 | loss: 1.07578 | val_0_accuracy: 0.45238 |  0:00:08s\n",
      "epoch 21 | loss: 1.03151 | val_0_accuracy: 0.42857 |  0:00:08s\n",
      "epoch 22 | loss: 1.07133 | val_0_accuracy: 0.38095 |  0:00:08s\n",
      "epoch 23 | loss: 1.03224 | val_0_accuracy: 0.45238 |  0:00:09s\n",
      "epoch 24 | loss: 1.02034 | val_0_accuracy: 0.45238 |  0:00:09s\n",
      "epoch 25 | loss: 0.98193 | val_0_accuracy: 0.42857 |  0:00:09s\n",
      "epoch 26 | loss: 0.99588 | val_0_accuracy: 0.45238 |  0:00:10s\n",
      "epoch 27 | loss: 1.06388 | val_0_accuracy: 0.45238 |  0:00:10s\n",
      "epoch 28 | loss: 1.04822 | val_0_accuracy: 0.5     |  0:00:10s\n",
      "epoch 29 | loss: 1.01838 | val_0_accuracy: 0.5     |  0:00:11s\n",
      "epoch 30 | loss: 1.02216 | val_0_accuracy: 0.47619 |  0:00:11s\n",
      "epoch 31 | loss: 0.99966 | val_0_accuracy: 0.45238 |  0:00:11s\n",
      "epoch 32 | loss: 1.00038 | val_0_accuracy: 0.42857 |  0:00:11s\n",
      "epoch 33 | loss: 0.97732 | val_0_accuracy: 0.5     |  0:00:12s\n",
      "epoch 34 | loss: 0.95744 | val_0_accuracy: 0.47619 |  0:00:12s\n",
      "epoch 35 | loss: 0.99348 | val_0_accuracy: 0.47619 |  0:00:12s\n",
      "epoch 36 | loss: 1.07588 | val_0_accuracy: 0.45238 |  0:00:13s\n",
      "epoch 37 | loss: 1.03935 | val_0_accuracy: 0.47619 |  0:00:13s\n",
      "epoch 38 | loss: 1.0506  | val_0_accuracy: 0.5     |  0:00:13s\n",
      "epoch 39 | loss: 0.99389 | val_0_accuracy: 0.47619 |  0:00:14s\n",
      "epoch 40 | loss: 1.0237  | val_0_accuracy: 0.47619 |  0:00:14s\n",
      "epoch 41 | loss: 0.98291 | val_0_accuracy: 0.52381 |  0:00:14s\n",
      "epoch 42 | loss: 1.01199 | val_0_accuracy: 0.54762 |  0:00:15s\n",
      "epoch 43 | loss: 1.00636 | val_0_accuracy: 0.57143 |  0:00:15s\n",
      "epoch 44 | loss: 1.00488 | val_0_accuracy: 0.52381 |  0:00:15s\n",
      "epoch 45 | loss: 1.02744 | val_0_accuracy: 0.52381 |  0:00:16s\n",
      "epoch 46 | loss: 0.98649 | val_0_accuracy: 0.47619 |  0:00:16s\n",
      "epoch 47 | loss: 1.01147 | val_0_accuracy: 0.5     |  0:00:16s\n",
      "epoch 48 | loss: 0.98514 | val_0_accuracy: 0.54762 |  0:00:17s\n",
      "epoch 49 | loss: 0.97169 | val_0_accuracy: 0.57143 |  0:00:17s\n",
      "epoch 50 | loss: 0.99749 | val_0_accuracy: 0.57143 |  0:00:17s\n",
      "epoch 51 | loss: 1.0187  | val_0_accuracy: 0.57143 |  0:00:17s\n",
      "epoch 52 | loss: 0.93469 | val_0_accuracy: 0.57143 |  0:00:18s\n",
      "epoch 53 | loss: 0.95179 | val_0_accuracy: 0.52381 |  0:00:18s\n",
      "epoch 54 | loss: 0.96859 | val_0_accuracy: 0.54762 |  0:00:18s\n",
      "epoch 55 | loss: 1.01621 | val_0_accuracy: 0.54762 |  0:00:19s\n",
      "epoch 56 | loss: 0.96431 | val_0_accuracy: 0.57143 |  0:00:19s\n",
      "epoch 57 | loss: 0.95675 | val_0_accuracy: 0.57143 |  0:00:19s\n",
      "epoch 58 | loss: 0.95525 | val_0_accuracy: 0.5     |  0:00:20s\n",
      "epoch 59 | loss: 1.00054 | val_0_accuracy: 0.52381 |  0:00:20s\n",
      "epoch 60 | loss: 0.95124 | val_0_accuracy: 0.52381 |  0:00:20s\n",
      "epoch 61 | loss: 0.94426 | val_0_accuracy: 0.54762 |  0:00:21s\n",
      "epoch 62 | loss: 0.95976 | val_0_accuracy: 0.52381 |  0:00:21s\n",
      "epoch 63 | loss: 0.97076 | val_0_accuracy: 0.52381 |  0:00:21s\n",
      "epoch 64 | loss: 0.9321  | val_0_accuracy: 0.54762 |  0:00:22s\n",
      "epoch 65 | loss: 0.95132 | val_0_accuracy: 0.54762 |  0:00:22s\n",
      "epoch 66 | loss: 0.93898 | val_0_accuracy: 0.47619 |  0:00:22s\n",
      "epoch 67 | loss: 0.97607 | val_0_accuracy: 0.47619 |  0:00:23s\n",
      "epoch 68 | loss: 0.94766 | val_0_accuracy: 0.45238 |  0:00:23s\n",
      "epoch 69 | loss: 0.92913 | val_0_accuracy: 0.52381 |  0:00:23s\n",
      "epoch 70 | loss: 0.96918 | val_0_accuracy: 0.52381 |  0:00:24s\n",
      "epoch 71 | loss: 0.99246 | val_0_accuracy: 0.45238 |  0:00:24s\n",
      "epoch 72 | loss: 0.93254 | val_0_accuracy: 0.40476 |  0:00:24s\n",
      "epoch 73 | loss: 0.92481 | val_0_accuracy: 0.40476 |  0:00:25s\n",
      "epoch 74 | loss: 0.92994 | val_0_accuracy: 0.40476 |  0:00:25s\n",
      "epoch 75 | loss: 0.91958 | val_0_accuracy: 0.52381 |  0:00:25s\n",
      "epoch 76 | loss: 0.93863 | val_0_accuracy: 0.52381 |  0:00:26s\n",
      "epoch 77 | loss: 0.95522 | val_0_accuracy: 0.42857 |  0:00:26s\n",
      "epoch 78 | loss: 0.90156 | val_0_accuracy: 0.47619 |  0:00:26s\n",
      "epoch 79 | loss: 0.89549 | val_0_accuracy: 0.45238 |  0:00:27s\n",
      "epoch 80 | loss: 0.9018  | val_0_accuracy: 0.47619 |  0:00:27s\n",
      "epoch 81 | loss: 0.90652 | val_0_accuracy: 0.5     |  0:00:27s\n",
      "epoch 82 | loss: 0.93172 | val_0_accuracy: 0.45238 |  0:00:27s\n",
      "epoch 83 | loss: 0.91855 | val_0_accuracy: 0.40476 |  0:00:28s\n",
      "epoch 84 | loss: 0.93445 | val_0_accuracy: 0.38095 |  0:00:28s\n",
      "epoch 85 | loss: 0.96565 | val_0_accuracy: 0.42857 |  0:00:28s\n",
      "epoch 86 | loss: 0.89979 | val_0_accuracy: 0.35714 |  0:00:29s\n",
      "epoch 87 | loss: 0.93045 | val_0_accuracy: 0.40476 |  0:00:29s\n",
      "epoch 88 | loss: 0.93368 | val_0_accuracy: 0.40476 |  0:00:29s\n",
      "epoch 89 | loss: 0.9362  | val_0_accuracy: 0.40476 |  0:00:30s\n",
      "epoch 90 | loss: 0.86878 | val_0_accuracy: 0.42857 |  0:00:30s\n",
      "epoch 91 | loss: 0.91052 | val_0_accuracy: 0.38095 |  0:00:30s\n",
      "epoch 92 | loss: 0.90152 | val_0_accuracy: 0.45238 |  0:00:30s\n",
      "epoch 93 | loss: 0.90715 | val_0_accuracy: 0.38095 |  0:00:31s\n",
      "\n",
      "Early stopping occurred at epoch 93 with best_epoch = 43 and best_val_0_accuracy = 0.57143\n",
      "\n",
      "Fold 2 Test Metrics:\n",
      "Accuracy: 0.5714\n",
      "F1 Score: 0.5503\n",
      "Precision: 0.5448\n",
      "Recall: 0.5714\n",
      "AUPRC: 0.5688\n",
      "AUROC: 0.7058\n",
      "\n",
      "==================================================\n",
      "Fold 3/10\n",
      "==================================================\n",
      "epoch 0  | loss: 1.63177 | val_0_accuracy: 0.40476 |  0:00:00s\n",
      "epoch 1  | loss: 1.32329 | val_0_accuracy: 0.45238 |  0:00:00s\n",
      "epoch 2  | loss: 1.23989 | val_0_accuracy: 0.38095 |  0:00:00s\n",
      "epoch 3  | loss: 1.2393  | val_0_accuracy: 0.52381 |  0:00:01s\n",
      "epoch 4  | loss: 1.14118 | val_0_accuracy: 0.42857 |  0:00:01s\n",
      "epoch 5  | loss: 1.16348 | val_0_accuracy: 0.40476 |  0:00:01s\n",
      "epoch 6  | loss: 1.14934 | val_0_accuracy: 0.38095 |  0:00:02s\n",
      "epoch 7  | loss: 1.15041 | val_0_accuracy: 0.40476 |  0:00:02s\n",
      "epoch 8  | loss: 1.12087 | val_0_accuracy: 0.38095 |  0:00:02s\n",
      "epoch 9  | loss: 1.11936 | val_0_accuracy: 0.45238 |  0:00:02s\n",
      "epoch 10 | loss: 1.11763 | val_0_accuracy: 0.47619 |  0:00:03s\n",
      "epoch 11 | loss: 1.09243 | val_0_accuracy: 0.5     |  0:00:03s\n",
      "epoch 12 | loss: 1.08539 | val_0_accuracy: 0.5     |  0:00:03s\n",
      "epoch 13 | loss: 1.13182 | val_0_accuracy: 0.47619 |  0:00:03s\n",
      "epoch 14 | loss: 1.1285  | val_0_accuracy: 0.45238 |  0:00:04s\n",
      "epoch 15 | loss: 1.09927 | val_0_accuracy: 0.42857 |  0:00:04s\n",
      "epoch 16 | loss: 1.06995 | val_0_accuracy: 0.45238 |  0:00:04s\n",
      "epoch 17 | loss: 1.11301 | val_0_accuracy: 0.45238 |  0:00:05s\n",
      "epoch 18 | loss: 1.08645 | val_0_accuracy: 0.47619 |  0:00:05s\n",
      "epoch 19 | loss: 1.1009  | val_0_accuracy: 0.42857 |  0:00:05s\n",
      "epoch 20 | loss: 1.09219 | val_0_accuracy: 0.45238 |  0:00:05s\n",
      "epoch 21 | loss: 1.09108 | val_0_accuracy: 0.47619 |  0:00:06s\n",
      "epoch 22 | loss: 1.06419 | val_0_accuracy: 0.40476 |  0:00:06s\n",
      "epoch 23 | loss: 1.06516 | val_0_accuracy: 0.40476 |  0:00:06s\n",
      "epoch 24 | loss: 1.08916 | val_0_accuracy: 0.33333 |  0:00:07s\n",
      "epoch 25 | loss: 1.06227 | val_0_accuracy: 0.42857 |  0:00:07s\n",
      "epoch 26 | loss: 1.06894 | val_0_accuracy: 0.40476 |  0:00:07s\n",
      "epoch 27 | loss: 1.04709 | val_0_accuracy: 0.42857 |  0:00:08s\n",
      "epoch 28 | loss: 1.03559 | val_0_accuracy: 0.45238 |  0:00:08s\n",
      "epoch 29 | loss: 1.0619  | val_0_accuracy: 0.42857 |  0:00:09s\n",
      "epoch 30 | loss: 1.06643 | val_0_accuracy: 0.42857 |  0:00:09s\n",
      "epoch 31 | loss: 1.05759 | val_0_accuracy: 0.42857 |  0:00:09s\n",
      "epoch 32 | loss: 1.04914 | val_0_accuracy: 0.42857 |  0:00:09s\n",
      "epoch 33 | loss: 1.0798  | val_0_accuracy: 0.40476 |  0:00:10s\n",
      "epoch 34 | loss: 1.03071 | val_0_accuracy: 0.5     |  0:00:10s\n",
      "epoch 35 | loss: 1.01676 | val_0_accuracy: 0.47619 |  0:00:10s\n",
      "epoch 36 | loss: 1.03276 | val_0_accuracy: 0.52381 |  0:00:11s\n",
      "epoch 37 | loss: 1.05525 | val_0_accuracy: 0.5     |  0:00:11s\n",
      "epoch 38 | loss: 1.06488 | val_0_accuracy: 0.5     |  0:00:11s\n",
      "epoch 39 | loss: 1.02046 | val_0_accuracy: 0.52381 |  0:00:12s\n",
      "epoch 40 | loss: 1.05124 | val_0_accuracy: 0.38095 |  0:00:12s\n",
      "epoch 41 | loss: 1.10975 | val_0_accuracy: 0.45238 |  0:00:12s\n",
      "epoch 42 | loss: 1.04709 | val_0_accuracy: 0.47619 |  0:00:12s\n",
      "epoch 43 | loss: 1.05741 | val_0_accuracy: 0.47619 |  0:00:13s\n",
      "epoch 44 | loss: 1.02601 | val_0_accuracy: 0.5     |  0:00:13s\n",
      "epoch 45 | loss: 1.08463 | val_0_accuracy: 0.54762 |  0:00:13s\n",
      "epoch 46 | loss: 1.05645 | val_0_accuracy: 0.52381 |  0:00:14s\n",
      "epoch 47 | loss: 1.06511 | val_0_accuracy: 0.47619 |  0:00:14s\n",
      "epoch 48 | loss: 1.04391 | val_0_accuracy: 0.45238 |  0:00:14s\n",
      "epoch 49 | loss: 1.04409 | val_0_accuracy: 0.5     |  0:00:15s\n",
      "epoch 50 | loss: 1.05695 | val_0_accuracy: 0.57143 |  0:00:15s\n",
      "epoch 51 | loss: 1.03506 | val_0_accuracy: 0.54762 |  0:00:15s\n",
      "epoch 52 | loss: 1.01332 | val_0_accuracy: 0.61905 |  0:00:16s\n",
      "epoch 53 | loss: 1.04369 | val_0_accuracy: 0.5     |  0:00:16s\n",
      "epoch 54 | loss: 1.03125 | val_0_accuracy: 0.47619 |  0:00:17s\n",
      "epoch 55 | loss: 1.01726 | val_0_accuracy: 0.54762 |  0:00:17s\n",
      "epoch 56 | loss: 1.0003  | val_0_accuracy: 0.5     |  0:00:17s\n",
      "epoch 57 | loss: 1.03188 | val_0_accuracy: 0.5     |  0:00:18s\n",
      "epoch 58 | loss: 1.02149 | val_0_accuracy: 0.45238 |  0:00:18s\n",
      "epoch 59 | loss: 0.99458 | val_0_accuracy: 0.45238 |  0:00:19s\n",
      "epoch 60 | loss: 1.02114 | val_0_accuracy: 0.42857 |  0:00:19s\n",
      "epoch 61 | loss: 1.01219 | val_0_accuracy: 0.42857 |  0:00:20s\n",
      "epoch 62 | loss: 0.99239 | val_0_accuracy: 0.54762 |  0:00:20s\n",
      "epoch 63 | loss: 0.9583  | val_0_accuracy: 0.57143 |  0:00:20s\n",
      "epoch 64 | loss: 1.00656 | val_0_accuracy: 0.5     |  0:00:21s\n",
      "epoch 65 | loss: 0.98    | val_0_accuracy: 0.5     |  0:00:21s\n",
      "epoch 66 | loss: 1.00047 | val_0_accuracy: 0.5     |  0:00:21s\n",
      "epoch 67 | loss: 0.99293 | val_0_accuracy: 0.52381 |  0:00:22s\n",
      "epoch 68 | loss: 1.00084 | val_0_accuracy: 0.45238 |  0:00:22s\n",
      "epoch 69 | loss: 1.00205 | val_0_accuracy: 0.42857 |  0:00:22s\n",
      "epoch 70 | loss: 1.00335 | val_0_accuracy: 0.47619 |  0:00:23s\n",
      "epoch 71 | loss: 1.03693 | val_0_accuracy: 0.5     |  0:00:23s\n",
      "epoch 72 | loss: 1.00716 | val_0_accuracy: 0.40476 |  0:00:23s\n",
      "epoch 73 | loss: 1.00731 | val_0_accuracy: 0.40476 |  0:00:23s\n",
      "epoch 74 | loss: 0.96764 | val_0_accuracy: 0.45238 |  0:00:24s\n",
      "epoch 75 | loss: 1.02783 | val_0_accuracy: 0.45238 |  0:00:24s\n",
      "epoch 76 | loss: 1.05452 | val_0_accuracy: 0.40476 |  0:00:24s\n",
      "epoch 77 | loss: 1.02008 | val_0_accuracy: 0.45238 |  0:00:25s\n",
      "epoch 78 | loss: 0.98215 | val_0_accuracy: 0.45238 |  0:00:25s\n",
      "epoch 79 | loss: 1.00301 | val_0_accuracy: 0.5     |  0:00:25s\n",
      "epoch 80 | loss: 0.97008 | val_0_accuracy: 0.42857 |  0:00:26s\n",
      "epoch 81 | loss: 0.97347 | val_0_accuracy: 0.5     |  0:00:26s\n",
      "epoch 82 | loss: 1.00002 | val_0_accuracy: 0.52381 |  0:00:27s\n",
      "epoch 83 | loss: 0.98482 | val_0_accuracy: 0.47619 |  0:00:27s\n",
      "epoch 84 | loss: 0.93176 | val_0_accuracy: 0.47619 |  0:00:27s\n",
      "epoch 85 | loss: 0.97726 | val_0_accuracy: 0.45238 |  0:00:27s\n",
      "epoch 86 | loss: 0.99322 | val_0_accuracy: 0.47619 |  0:00:28s\n",
      "epoch 87 | loss: 0.97269 | val_0_accuracy: 0.47619 |  0:00:28s\n",
      "epoch 88 | loss: 0.97218 | val_0_accuracy: 0.40476 |  0:00:28s\n",
      "epoch 89 | loss: 0.9997  | val_0_accuracy: 0.45238 |  0:00:29s\n",
      "epoch 90 | loss: 0.9902  | val_0_accuracy: 0.47619 |  0:00:29s\n",
      "epoch 91 | loss: 0.94824 | val_0_accuracy: 0.42857 |  0:00:29s\n",
      "epoch 92 | loss: 0.93898 | val_0_accuracy: 0.47619 |  0:00:30s\n",
      "epoch 93 | loss: 0.95645 | val_0_accuracy: 0.54762 |  0:00:30s\n",
      "epoch 94 | loss: 0.97798 | val_0_accuracy: 0.5     |  0:00:30s\n",
      "epoch 95 | loss: 0.97029 | val_0_accuracy: 0.42857 |  0:00:31s\n",
      "epoch 96 | loss: 0.97408 | val_0_accuracy: 0.54762 |  0:00:31s\n",
      "epoch 97 | loss: 1.00849 | val_0_accuracy: 0.5     |  0:00:31s\n",
      "epoch 98 | loss: 0.95474 | val_0_accuracy: 0.35714 |  0:00:31s\n",
      "epoch 99 | loss: 0.98053 | val_0_accuracy: 0.38095 |  0:00:32s\n",
      "Stop training because you reached max_epochs = 100 with best_epoch = 52 and best_val_0_accuracy = 0.61905\n",
      "\n",
      "Fold 3 Test Metrics:\n",
      "Accuracy: 0.6190\n",
      "F1 Score: 0.6065\n",
      "Precision: 0.6197\n",
      "Recall: 0.6190\n",
      "AUPRC: 0.6160\n",
      "AUROC: 0.7225\n",
      "\n",
      "==================================================\n",
      "Fold 4/10\n",
      "==================================================\n",
      "epoch 0  | loss: 1.64998 | val_0_accuracy: 0.2619  |  0:00:00s\n",
      "epoch 1  | loss: 1.31667 | val_0_accuracy: 0.2381  |  0:00:00s\n",
      "epoch 2  | loss: 1.27173 | val_0_accuracy: 0.35714 |  0:00:00s\n",
      "epoch 3  | loss: 1.19845 | val_0_accuracy: 0.5     |  0:00:01s\n",
      "epoch 4  | loss: 1.1922  | val_0_accuracy: 0.52381 |  0:00:01s\n",
      "epoch 5  | loss: 1.18732 | val_0_accuracy: 0.47619 |  0:00:01s\n",
      "epoch 6  | loss: 1.13478 | val_0_accuracy: 0.45238 |  0:00:02s\n",
      "epoch 7  | loss: 1.15207 | val_0_accuracy: 0.47619 |  0:00:02s\n",
      "epoch 8  | loss: 1.17583 | val_0_accuracy: 0.47619 |  0:00:02s\n",
      "epoch 9  | loss: 1.13603 | val_0_accuracy: 0.54762 |  0:00:03s\n",
      "epoch 10 | loss: 1.1348  | val_0_accuracy: 0.40476 |  0:00:03s\n",
      "epoch 11 | loss: 1.1256  | val_0_accuracy: 0.47619 |  0:00:03s\n",
      "epoch 12 | loss: 1.0776  | val_0_accuracy: 0.38095 |  0:00:03s\n",
      "epoch 13 | loss: 1.11188 | val_0_accuracy: 0.40476 |  0:00:04s\n",
      "epoch 14 | loss: 1.08718 | val_0_accuracy: 0.52381 |  0:00:04s\n",
      "epoch 15 | loss: 1.09664 | val_0_accuracy: 0.45238 |  0:00:05s\n",
      "epoch 16 | loss: 1.09221 | val_0_accuracy: 0.5     |  0:00:05s\n",
      "epoch 17 | loss: 1.08124 | val_0_accuracy: 0.40476 |  0:00:05s\n",
      "epoch 18 | loss: 1.04345 | val_0_accuracy: 0.45238 |  0:00:06s\n",
      "epoch 19 | loss: 1.04918 | val_0_accuracy: 0.47619 |  0:00:06s\n",
      "epoch 20 | loss: 1.08602 | val_0_accuracy: 0.42857 |  0:00:06s\n",
      "epoch 21 | loss: 1.05892 | val_0_accuracy: 0.52381 |  0:00:07s\n",
      "epoch 22 | loss: 1.05117 | val_0_accuracy: 0.54762 |  0:00:07s\n",
      "epoch 23 | loss: 1.01827 | val_0_accuracy: 0.54762 |  0:00:07s\n",
      "epoch 24 | loss: 1.03989 | val_0_accuracy: 0.38095 |  0:00:08s\n",
      "epoch 25 | loss: 1.05413 | val_0_accuracy: 0.33333 |  0:00:08s\n",
      "epoch 26 | loss: 1.02159 | val_0_accuracy: 0.33333 |  0:00:09s\n",
      "epoch 27 | loss: 1.01718 | val_0_accuracy: 0.35714 |  0:00:09s\n",
      "epoch 28 | loss: 1.04504 | val_0_accuracy: 0.42857 |  0:00:10s\n",
      "epoch 29 | loss: 1.04913 | val_0_accuracy: 0.30952 |  0:00:10s\n",
      "epoch 30 | loss: 1.01689 | val_0_accuracy: 0.30952 |  0:00:10s\n",
      "epoch 31 | loss: 1.02384 | val_0_accuracy: 0.40476 |  0:00:11s\n",
      "epoch 32 | loss: 1.01395 | val_0_accuracy: 0.38095 |  0:00:11s\n",
      "epoch 33 | loss: 1.0279  | val_0_accuracy: 0.40476 |  0:00:11s\n",
      "epoch 34 | loss: 1.08172 | val_0_accuracy: 0.47619 |  0:00:11s\n",
      "epoch 35 | loss: 1.03809 | val_0_accuracy: 0.47619 |  0:00:12s\n",
      "epoch 36 | loss: 1.06091 | val_0_accuracy: 0.45238 |  0:00:12s\n",
      "epoch 37 | loss: 1.04193 | val_0_accuracy: 0.54762 |  0:00:12s\n",
      "epoch 38 | loss: 1.01422 | val_0_accuracy: 0.52381 |  0:00:13s\n",
      "epoch 39 | loss: 1.01137 | val_0_accuracy: 0.5     |  0:00:13s\n",
      "epoch 40 | loss: 1.01596 | val_0_accuracy: 0.45238 |  0:00:13s\n",
      "epoch 41 | loss: 1.01889 | val_0_accuracy: 0.5     |  0:00:14s\n",
      "epoch 42 | loss: 0.99435 | val_0_accuracy: 0.38095 |  0:00:14s\n",
      "epoch 43 | loss: 1.00016 | val_0_accuracy: 0.42857 |  0:00:14s\n",
      "epoch 44 | loss: 1.0399  | val_0_accuracy: 0.42857 |  0:00:15s\n",
      "epoch 45 | loss: 0.98919 | val_0_accuracy: 0.45238 |  0:00:15s\n",
      "epoch 46 | loss: 0.99151 | val_0_accuracy: 0.42857 |  0:00:15s\n",
      "epoch 47 | loss: 1.00708 | val_0_accuracy: 0.45238 |  0:00:15s\n",
      "epoch 48 | loss: 1.03627 | val_0_accuracy: 0.42857 |  0:00:16s\n",
      "epoch 49 | loss: 1.00191 | val_0_accuracy: 0.35714 |  0:00:16s\n",
      "epoch 50 | loss: 0.97328 | val_0_accuracy: 0.33333 |  0:00:17s\n",
      "epoch 51 | loss: 0.98887 | val_0_accuracy: 0.38095 |  0:00:17s\n",
      "epoch 52 | loss: 0.96826 | val_0_accuracy: 0.35714 |  0:00:17s\n",
      "epoch 53 | loss: 1.01751 | val_0_accuracy: 0.42857 |  0:00:17s\n",
      "epoch 54 | loss: 0.98834 | val_0_accuracy: 0.42857 |  0:00:18s\n",
      "epoch 55 | loss: 0.9747  | val_0_accuracy: 0.45238 |  0:00:18s\n",
      "epoch 56 | loss: 0.97957 | val_0_accuracy: 0.45238 |  0:00:18s\n",
      "epoch 57 | loss: 0.98526 | val_0_accuracy: 0.45238 |  0:00:19s\n",
      "epoch 58 | loss: 0.97275 | val_0_accuracy: 0.47619 |  0:00:19s\n",
      "epoch 59 | loss: 0.96342 | val_0_accuracy: 0.45238 |  0:00:19s\n",
      "\n",
      "Early stopping occurred at epoch 59 with best_epoch = 9 and best_val_0_accuracy = 0.54762\n",
      "\n",
      "Fold 4 Test Metrics:\n",
      "Accuracy: 0.5476\n",
      "F1 Score: 0.5296\n",
      "Precision: 0.5174\n",
      "Recall: 0.5476\n",
      "AUPRC: 0.5595\n",
      "AUROC: 0.6970\n",
      "\n",
      "==================================================\n",
      "Fold 5/10\n",
      "==================================================\n",
      "epoch 0  | loss: 1.66555 | val_0_accuracy: 0.38095 |  0:00:00s\n",
      "epoch 1  | loss: 1.3811  | val_0_accuracy: 0.40476 |  0:00:00s\n",
      "epoch 2  | loss: 1.22633 | val_0_accuracy: 0.42857 |  0:00:00s\n",
      "epoch 3  | loss: 1.11338 | val_0_accuracy: 0.5     |  0:00:01s\n",
      "epoch 4  | loss: 1.19923 | val_0_accuracy: 0.42857 |  0:00:01s\n",
      "epoch 5  | loss: 1.2019  | val_0_accuracy: 0.47619 |  0:00:01s\n",
      "epoch 6  | loss: 1.17623 | val_0_accuracy: 0.45238 |  0:00:02s\n",
      "epoch 7  | loss: 1.13219 | val_0_accuracy: 0.5     |  0:00:02s\n",
      "epoch 8  | loss: 1.10682 | val_0_accuracy: 0.57143 |  0:00:02s\n",
      "epoch 9  | loss: 1.10299 | val_0_accuracy: 0.5     |  0:00:03s\n",
      "epoch 10 | loss: 1.14407 | val_0_accuracy: 0.45238 |  0:00:03s\n",
      "epoch 11 | loss: 1.12008 | val_0_accuracy: 0.47619 |  0:00:03s\n",
      "epoch 12 | loss: 1.10979 | val_0_accuracy: 0.5     |  0:00:04s\n",
      "epoch 13 | loss: 1.10078 | val_0_accuracy: 0.52381 |  0:00:04s\n",
      "epoch 14 | loss: 1.078   | val_0_accuracy: 0.52381 |  0:00:04s\n",
      "epoch 15 | loss: 1.08782 | val_0_accuracy: 0.52381 |  0:00:04s\n",
      "epoch 16 | loss: 1.05628 | val_0_accuracy: 0.5     |  0:00:04s\n",
      "epoch 17 | loss: 1.09145 | val_0_accuracy: 0.54762 |  0:00:05s\n",
      "epoch 18 | loss: 1.10132 | val_0_accuracy: 0.47619 |  0:00:05s\n",
      "epoch 19 | loss: 1.07014 | val_0_accuracy: 0.42857 |  0:00:05s\n",
      "epoch 20 | loss: 1.09058 | val_0_accuracy: 0.40476 |  0:00:06s\n",
      "epoch 21 | loss: 1.06381 | val_0_accuracy: 0.45238 |  0:00:06s\n",
      "epoch 22 | loss: 1.04719 | val_0_accuracy: 0.5     |  0:00:06s\n",
      "epoch 23 | loss: 1.03888 | val_0_accuracy: 0.5     |  0:00:06s\n",
      "epoch 24 | loss: 1.05834 | val_0_accuracy: 0.42857 |  0:00:07s\n",
      "epoch 25 | loss: 1.04173 | val_0_accuracy: 0.5     |  0:00:07s\n",
      "epoch 26 | loss: 1.0519  | val_0_accuracy: 0.5     |  0:00:07s\n",
      "epoch 27 | loss: 1.03604 | val_0_accuracy: 0.5     |  0:00:08s\n",
      "epoch 28 | loss: 1.04702 | val_0_accuracy: 0.45238 |  0:00:08s\n",
      "epoch 29 | loss: 1.06614 | val_0_accuracy: 0.42857 |  0:00:08s\n",
      "epoch 30 | loss: 1.0432  | val_0_accuracy: 0.42857 |  0:00:09s\n",
      "epoch 31 | loss: 1.01736 | val_0_accuracy: 0.40476 |  0:00:09s\n",
      "epoch 32 | loss: 1.03854 | val_0_accuracy: 0.47619 |  0:00:09s\n",
      "epoch 33 | loss: 1.05665 | val_0_accuracy: 0.47619 |  0:00:09s\n",
      "epoch 34 | loss: 1.00098 | val_0_accuracy: 0.45238 |  0:00:10s\n",
      "epoch 35 | loss: 0.97604 | val_0_accuracy: 0.42857 |  0:00:10s\n",
      "epoch 36 | loss: 1.03493 | val_0_accuracy: 0.40476 |  0:00:10s\n",
      "epoch 37 | loss: 1.03474 | val_0_accuracy: 0.47619 |  0:00:11s\n",
      "epoch 38 | loss: 1.0363  | val_0_accuracy: 0.5     |  0:00:11s\n",
      "epoch 39 | loss: 0.99665 | val_0_accuracy: 0.47619 |  0:00:11s\n",
      "epoch 40 | loss: 0.97887 | val_0_accuracy: 0.52381 |  0:00:11s\n",
      "epoch 41 | loss: 0.97474 | val_0_accuracy: 0.5     |  0:00:12s\n",
      "epoch 42 | loss: 0.9828  | val_0_accuracy: 0.47619 |  0:00:12s\n",
      "epoch 43 | loss: 0.97909 | val_0_accuracy: 0.54762 |  0:00:12s\n",
      "epoch 44 | loss: 0.99555 | val_0_accuracy: 0.45238 |  0:00:13s\n",
      "epoch 45 | loss: 0.95726 | val_0_accuracy: 0.45238 |  0:00:13s\n",
      "epoch 46 | loss: 0.93631 | val_0_accuracy: 0.42857 |  0:00:13s\n",
      "epoch 47 | loss: 0.97703 | val_0_accuracy: 0.54762 |  0:00:13s\n",
      "epoch 48 | loss: 0.96083 | val_0_accuracy: 0.57143 |  0:00:14s\n",
      "epoch 49 | loss: 0.97711 | val_0_accuracy: 0.57143 |  0:00:14s\n",
      "epoch 50 | loss: 0.93916 | val_0_accuracy: 0.47619 |  0:00:14s\n",
      "epoch 51 | loss: 0.96899 | val_0_accuracy: 0.52381 |  0:00:14s\n",
      "epoch 52 | loss: 0.99963 | val_0_accuracy: 0.54762 |  0:00:15s\n",
      "epoch 53 | loss: 0.95861 | val_0_accuracy: 0.47619 |  0:00:15s\n",
      "epoch 54 | loss: 0.97336 | val_0_accuracy: 0.42857 |  0:00:15s\n",
      "epoch 55 | loss: 0.98988 | val_0_accuracy: 0.45238 |  0:00:16s\n",
      "epoch 56 | loss: 0.96963 | val_0_accuracy: 0.5     |  0:00:16s\n",
      "epoch 57 | loss: 0.95495 | val_0_accuracy: 0.47619 |  0:00:16s\n",
      "epoch 58 | loss: 0.96696 | val_0_accuracy: 0.47619 |  0:00:16s\n",
      "\n",
      "Early stopping occurred at epoch 58 with best_epoch = 8 and best_val_0_accuracy = 0.57143\n",
      "\n",
      "Fold 5 Test Metrics:\n",
      "Accuracy: 0.5714\n",
      "F1 Score: 0.5305\n",
      "Precision: 0.5362\n",
      "Recall: 0.5714\n",
      "AUPRC: 0.5036\n",
      "AUROC: 0.6365\n",
      "\n",
      "==================================================\n",
      "Fold 6/10\n",
      "==================================================\n",
      "epoch 0  | loss: 1.61037 | val_0_accuracy: 0.5     |  0:00:00s\n",
      "epoch 1  | loss: 1.27805 | val_0_accuracy: 0.2381  |  0:00:00s\n",
      "epoch 2  | loss: 1.25193 | val_0_accuracy: 0.54762 |  0:00:01s\n",
      "epoch 3  | loss: 1.23569 | val_0_accuracy: 0.54762 |  0:00:01s\n",
      "epoch 4  | loss: 1.19026 | val_0_accuracy: 0.40476 |  0:00:02s\n",
      "epoch 5  | loss: 1.15253 | val_0_accuracy: 0.5     |  0:00:02s\n",
      "epoch 6  | loss: 1.16473 | val_0_accuracy: 0.5     |  0:00:02s\n",
      "epoch 7  | loss: 1.14434 | val_0_accuracy: 0.45238 |  0:00:02s\n",
      "epoch 8  | loss: 1.12158 | val_0_accuracy: 0.47619 |  0:00:03s\n",
      "epoch 9  | loss: 1.12478 | val_0_accuracy: 0.42857 |  0:00:03s\n",
      "epoch 10 | loss: 1.12458 | val_0_accuracy: 0.52381 |  0:00:04s\n",
      "epoch 11 | loss: 1.07963 | val_0_accuracy: 0.54762 |  0:00:04s\n",
      "epoch 12 | loss: 1.07753 | val_0_accuracy: 0.54762 |  0:00:05s\n",
      "epoch 13 | loss: 1.07259 | val_0_accuracy: 0.59524 |  0:00:05s\n",
      "epoch 14 | loss: 1.03653 | val_0_accuracy: 0.54762 |  0:00:05s\n",
      "epoch 15 | loss: 1.094   | val_0_accuracy: 0.64286 |  0:00:06s\n",
      "epoch 16 | loss: 1.10628 | val_0_accuracy: 0.66667 |  0:00:06s\n",
      "epoch 17 | loss: 1.09131 | val_0_accuracy: 0.64286 |  0:00:06s\n",
      "epoch 18 | loss: 1.06413 | val_0_accuracy: 0.66667 |  0:00:06s\n",
      "epoch 19 | loss: 1.11792 | val_0_accuracy: 0.64286 |  0:00:07s\n",
      "epoch 20 | loss: 1.05967 | val_0_accuracy: 0.59524 |  0:00:07s\n",
      "epoch 21 | loss: 1.07294 | val_0_accuracy: 0.54762 |  0:00:08s\n",
      "epoch 22 | loss: 1.04244 | val_0_accuracy: 0.47619 |  0:00:08s\n",
      "epoch 23 | loss: 1.0405  | val_0_accuracy: 0.54762 |  0:00:08s\n",
      "epoch 24 | loss: 1.05767 | val_0_accuracy: 0.52381 |  0:00:09s\n",
      "epoch 25 | loss: 1.06422 | val_0_accuracy: 0.52381 |  0:00:09s\n",
      "epoch 26 | loss: 1.02034 | val_0_accuracy: 0.54762 |  0:00:09s\n",
      "epoch 27 | loss: 1.05456 | val_0_accuracy: 0.52381 |  0:00:09s\n",
      "epoch 28 | loss: 1.01079 | val_0_accuracy: 0.57143 |  0:00:10s\n",
      "epoch 29 | loss: 1.05164 | val_0_accuracy: 0.57143 |  0:00:10s\n",
      "epoch 30 | loss: 0.99844 | val_0_accuracy: 0.54762 |  0:00:10s\n",
      "epoch 31 | loss: 1.08186 | val_0_accuracy: 0.57143 |  0:00:11s\n",
      "epoch 32 | loss: 1.02402 | val_0_accuracy: 0.57143 |  0:00:11s\n",
      "epoch 33 | loss: 0.98209 | val_0_accuracy: 0.57143 |  0:00:12s\n",
      "epoch 34 | loss: 1.0286  | val_0_accuracy: 0.64286 |  0:00:12s\n",
      "epoch 35 | loss: 1.005   | val_0_accuracy: 0.61905 |  0:00:12s\n",
      "epoch 36 | loss: 0.99945 | val_0_accuracy: 0.52381 |  0:00:13s\n",
      "epoch 37 | loss: 0.98819 | val_0_accuracy: 0.59524 |  0:00:13s\n",
      "epoch 38 | loss: 1.06058 | val_0_accuracy: 0.57143 |  0:00:14s\n",
      "epoch 39 | loss: 1.04639 | val_0_accuracy: 0.59524 |  0:00:14s\n",
      "epoch 40 | loss: 1.03627 | val_0_accuracy: 0.61905 |  0:00:15s\n",
      "epoch 41 | loss: 0.98322 | val_0_accuracy: 0.59524 |  0:00:15s\n",
      "epoch 42 | loss: 1.02977 | val_0_accuracy: 0.57143 |  0:00:16s\n",
      "epoch 43 | loss: 0.99765 | val_0_accuracy: 0.57143 |  0:00:16s\n",
      "epoch 44 | loss: 1.02862 | val_0_accuracy: 0.57143 |  0:00:17s\n",
      "epoch 45 | loss: 1.05952 | val_0_accuracy: 0.57143 |  0:00:17s\n",
      "epoch 46 | loss: 1.03489 | val_0_accuracy: 0.52381 |  0:00:17s\n",
      "epoch 47 | loss: 1.00595 | val_0_accuracy: 0.54762 |  0:00:18s\n",
      "epoch 48 | loss: 1.01398 | val_0_accuracy: 0.54762 |  0:00:18s\n",
      "epoch 49 | loss: 1.01677 | val_0_accuracy: 0.5     |  0:00:18s\n",
      "epoch 50 | loss: 1.00339 | val_0_accuracy: 0.54762 |  0:00:18s\n",
      "epoch 51 | loss: 1.01515 | val_0_accuracy: 0.57143 |  0:00:19s\n",
      "epoch 52 | loss: 0.97167 | val_0_accuracy: 0.52381 |  0:00:19s\n",
      "epoch 53 | loss: 0.9374  | val_0_accuracy: 0.52381 |  0:00:19s\n",
      "epoch 54 | loss: 0.96774 | val_0_accuracy: 0.59524 |  0:00:20s\n",
      "epoch 55 | loss: 0.96611 | val_0_accuracy: 0.5     |  0:00:20s\n",
      "epoch 56 | loss: 0.99949 | val_0_accuracy: 0.47619 |  0:00:20s\n",
      "epoch 57 | loss: 1.0163  | val_0_accuracy: 0.5     |  0:00:21s\n",
      "epoch 58 | loss: 0.96925 | val_0_accuracy: 0.5     |  0:00:21s\n",
      "epoch 59 | loss: 0.97236 | val_0_accuracy: 0.57143 |  0:00:21s\n",
      "epoch 60 | loss: 0.94548 | val_0_accuracy: 0.52381 |  0:00:22s\n",
      "epoch 61 | loss: 0.98264 | val_0_accuracy: 0.57143 |  0:00:22s\n",
      "epoch 62 | loss: 0.96937 | val_0_accuracy: 0.54762 |  0:00:23s\n",
      "epoch 63 | loss: 0.9535  | val_0_accuracy: 0.54762 |  0:00:23s\n",
      "epoch 64 | loss: 0.96037 | val_0_accuracy: 0.54762 |  0:00:23s\n",
      "epoch 65 | loss: 0.99057 | val_0_accuracy: 0.5     |  0:00:24s\n",
      "epoch 66 | loss: 1.0055  | val_0_accuracy: 0.54762 |  0:00:24s\n",
      "\n",
      "Early stopping occurred at epoch 66 with best_epoch = 16 and best_val_0_accuracy = 0.66667\n",
      "\n",
      "Fold 6 Test Metrics:\n",
      "Accuracy: 0.6667\n",
      "F1 Score: 0.6120\n",
      "Precision: 0.6364\n",
      "Recall: 0.6667\n",
      "AUPRC: 0.6334\n",
      "AUROC: 0.7568\n",
      "\n",
      "==================================================\n",
      "Fold 7/10\n",
      "==================================================\n",
      "epoch 0  | loss: 1.72906 | val_0_accuracy: 0.38095 |  0:00:00s\n",
      "epoch 1  | loss: 1.32553 | val_0_accuracy: 0.5     |  0:00:01s\n",
      "epoch 2  | loss: 1.24136 | val_0_accuracy: 0.45238 |  0:00:01s\n",
      "epoch 3  | loss: 1.18359 | val_0_accuracy: 0.52381 |  0:00:02s\n",
      "epoch 4  | loss: 1.20418 | val_0_accuracy: 0.54762 |  0:00:02s\n",
      "epoch 5  | loss: 1.18555 | val_0_accuracy: 0.40476 |  0:00:03s\n",
      "epoch 6  | loss: 1.13829 | val_0_accuracy: 0.5     |  0:00:03s\n",
      "epoch 7  | loss: 1.14037 | val_0_accuracy: 0.40476 |  0:00:04s\n",
      "epoch 8  | loss: 1.0819  | val_0_accuracy: 0.5     |  0:00:04s\n",
      "epoch 9  | loss: 1.05422 | val_0_accuracy: 0.42857 |  0:00:04s\n",
      "epoch 10 | loss: 1.10599 | val_0_accuracy: 0.42857 |  0:00:05s\n",
      "epoch 11 | loss: 1.07965 | val_0_accuracy: 0.40476 |  0:00:05s\n",
      "epoch 12 | loss: 1.11937 | val_0_accuracy: 0.45238 |  0:00:06s\n",
      "epoch 13 | loss: 1.11882 | val_0_accuracy: 0.45238 |  0:00:06s\n",
      "epoch 14 | loss: 1.09974 | val_0_accuracy: 0.42857 |  0:00:06s\n",
      "epoch 15 | loss: 1.08033 | val_0_accuracy: 0.47619 |  0:00:07s\n",
      "epoch 16 | loss: 1.08816 | val_0_accuracy: 0.5     |  0:00:07s\n",
      "epoch 17 | loss: 1.02477 | val_0_accuracy: 0.5     |  0:00:07s\n",
      "epoch 18 | loss: 1.06437 | val_0_accuracy: 0.47619 |  0:00:08s\n",
      "epoch 19 | loss: 1.06697 | val_0_accuracy: 0.45238 |  0:00:08s\n",
      "epoch 20 | loss: 1.05613 | val_0_accuracy: 0.42857 |  0:00:08s\n",
      "epoch 21 | loss: 1.07956 | val_0_accuracy: 0.38095 |  0:00:09s\n",
      "epoch 22 | loss: 1.07541 | val_0_accuracy: 0.38095 |  0:00:09s\n",
      "epoch 23 | loss: 1.04539 | val_0_accuracy: 0.35714 |  0:00:10s\n",
      "epoch 24 | loss: 1.04128 | val_0_accuracy: 0.42857 |  0:00:10s\n",
      "epoch 25 | loss: 1.05151 | val_0_accuracy: 0.38095 |  0:00:10s\n",
      "epoch 26 | loss: 1.0624  | val_0_accuracy: 0.40476 |  0:00:11s\n",
      "epoch 27 | loss: 1.01532 | val_0_accuracy: 0.35714 |  0:00:11s\n",
      "epoch 28 | loss: 1.04462 | val_0_accuracy: 0.38095 |  0:00:11s\n",
      "epoch 29 | loss: 1.06578 | val_0_accuracy: 0.47619 |  0:00:12s\n",
      "epoch 30 | loss: 1.05843 | val_0_accuracy: 0.40476 |  0:00:12s\n",
      "epoch 31 | loss: 1.08182 | val_0_accuracy: 0.42857 |  0:00:12s\n",
      "epoch 32 | loss: 1.03102 | val_0_accuracy: 0.47619 |  0:00:12s\n",
      "epoch 33 | loss: 1.04456 | val_0_accuracy: 0.45238 |  0:00:13s\n",
      "epoch 34 | loss: 1.0195  | val_0_accuracy: 0.45238 |  0:00:13s\n",
      "epoch 35 | loss: 1.04185 | val_0_accuracy: 0.40476 |  0:00:13s\n",
      "epoch 36 | loss: 1.05403 | val_0_accuracy: 0.42857 |  0:00:13s\n",
      "epoch 37 | loss: 1.05067 | val_0_accuracy: 0.42857 |  0:00:14s\n",
      "epoch 38 | loss: 1.03686 | val_0_accuracy: 0.40476 |  0:00:14s\n",
      "epoch 39 | loss: 1.04141 | val_0_accuracy: 0.38095 |  0:00:14s\n",
      "epoch 40 | loss: 1.02068 | val_0_accuracy: 0.45238 |  0:00:14s\n",
      "epoch 41 | loss: 1.05689 | val_0_accuracy: 0.38095 |  0:00:15s\n",
      "epoch 42 | loss: 1.01939 | val_0_accuracy: 0.33333 |  0:00:15s\n",
      "epoch 43 | loss: 1.02523 | val_0_accuracy: 0.40476 |  0:00:15s\n",
      "epoch 44 | loss: 1.04564 | val_0_accuracy: 0.42857 |  0:00:16s\n",
      "epoch 45 | loss: 1.02934 | val_0_accuracy: 0.38095 |  0:00:16s\n",
      "epoch 46 | loss: 0.98975 | val_0_accuracy: 0.40476 |  0:00:16s\n",
      "epoch 47 | loss: 1.0423  | val_0_accuracy: 0.42857 |  0:00:16s\n",
      "epoch 48 | loss: 0.99693 | val_0_accuracy: 0.47619 |  0:00:17s\n",
      "epoch 49 | loss: 0.97619 | val_0_accuracy: 0.47619 |  0:00:17s\n",
      "epoch 50 | loss: 1.01429 | val_0_accuracy: 0.35714 |  0:00:17s\n",
      "epoch 51 | loss: 1.00072 | val_0_accuracy: 0.40476 |  0:00:17s\n",
      "epoch 52 | loss: 1.01934 | val_0_accuracy: 0.38095 |  0:00:18s\n",
      "epoch 53 | loss: 0.97319 | val_0_accuracy: 0.45238 |  0:00:18s\n",
      "epoch 54 | loss: 1.00875 | val_0_accuracy: 0.47619 |  0:00:18s\n",
      "\n",
      "Early stopping occurred at epoch 54 with best_epoch = 4 and best_val_0_accuracy = 0.54762\n",
      "\n",
      "Fold 7 Test Metrics:\n",
      "Accuracy: 0.5476\n",
      "F1 Score: 0.5273\n",
      "Precision: 0.5664\n",
      "Recall: 0.5476\n",
      "AUPRC: 0.5972\n",
      "AUROC: 0.7218\n",
      "\n",
      "==================================================\n",
      "Fold 8/10\n",
      "==================================================\n",
      "epoch 0  | loss: 1.56518 | val_0_accuracy: 0.19048 |  0:00:00s\n",
      "epoch 1  | loss: 1.33178 | val_0_accuracy: 0.33333 |  0:00:00s\n",
      "epoch 2  | loss: 1.24023 | val_0_accuracy: 0.33333 |  0:00:00s\n",
      "epoch 3  | loss: 1.20054 | val_0_accuracy: 0.35714 |  0:00:01s\n",
      "epoch 4  | loss: 1.13918 | val_0_accuracy: 0.40476 |  0:00:01s\n",
      "epoch 5  | loss: 1.13545 | val_0_accuracy: 0.33333 |  0:00:01s\n",
      "epoch 6  | loss: 1.1444  | val_0_accuracy: 0.47619 |  0:00:01s\n",
      "epoch 7  | loss: 1.1075  | val_0_accuracy: 0.47619 |  0:00:02s\n",
      "epoch 8  | loss: 1.10734 | val_0_accuracy: 0.52381 |  0:00:02s\n",
      "epoch 9  | loss: 1.08636 | val_0_accuracy: 0.52381 |  0:00:02s\n",
      "epoch 10 | loss: 1.05198 | val_0_accuracy: 0.5     |  0:00:02s\n",
      "epoch 11 | loss: 1.00446 | val_0_accuracy: 0.45238 |  0:00:02s\n",
      "epoch 12 | loss: 1.07849 | val_0_accuracy: 0.45238 |  0:00:03s\n",
      "epoch 13 | loss: 1.03921 | val_0_accuracy: 0.42857 |  0:00:03s\n",
      "epoch 14 | loss: 1.09239 | val_0_accuracy: 0.47619 |  0:00:03s\n",
      "epoch 15 | loss: 1.04893 | val_0_accuracy: 0.45238 |  0:00:03s\n",
      "epoch 16 | loss: 1.06645 | val_0_accuracy: 0.47619 |  0:00:04s\n",
      "epoch 17 | loss: 1.05538 | val_0_accuracy: 0.52381 |  0:00:04s\n",
      "epoch 18 | loss: 1.05467 | val_0_accuracy: 0.42857 |  0:00:04s\n",
      "epoch 19 | loss: 1.02082 | val_0_accuracy: 0.40476 |  0:00:04s\n",
      "epoch 20 | loss: 1.01815 | val_0_accuracy: 0.38095 |  0:00:05s\n",
      "epoch 21 | loss: 1.07636 | val_0_accuracy: 0.38095 |  0:00:05s\n",
      "epoch 22 | loss: 1.007   | val_0_accuracy: 0.33333 |  0:00:05s\n",
      "epoch 23 | loss: 1.01327 | val_0_accuracy: 0.35714 |  0:00:05s\n",
      "epoch 24 | loss: 1.0445  | val_0_accuracy: 0.35714 |  0:00:05s\n",
      "epoch 25 | loss: 1.0304  | val_0_accuracy: 0.42857 |  0:00:06s\n",
      "epoch 26 | loss: 0.97311 | val_0_accuracy: 0.42857 |  0:00:06s\n",
      "epoch 27 | loss: 1.00317 | val_0_accuracy: 0.45238 |  0:00:06s\n",
      "epoch 28 | loss: 1.01376 | val_0_accuracy: 0.42857 |  0:00:06s\n",
      "epoch 29 | loss: 0.97212 | val_0_accuracy: 0.45238 |  0:00:07s\n",
      "epoch 30 | loss: 0.99724 | val_0_accuracy: 0.40476 |  0:00:07s\n",
      "epoch 31 | loss: 0.98005 | val_0_accuracy: 0.35714 |  0:00:07s\n",
      "epoch 32 | loss: 0.97076 | val_0_accuracy: 0.35714 |  0:00:07s\n",
      "epoch 33 | loss: 1.00756 | val_0_accuracy: 0.45238 |  0:00:08s\n",
      "epoch 34 | loss: 1.0099  | val_0_accuracy: 0.47619 |  0:00:08s\n",
      "epoch 35 | loss: 0.98602 | val_0_accuracy: 0.47619 |  0:00:08s\n",
      "epoch 36 | loss: 0.9866  | val_0_accuracy: 0.47619 |  0:00:08s\n",
      "epoch 37 | loss: 0.98201 | val_0_accuracy: 0.35714 |  0:00:08s\n",
      "epoch 38 | loss: 0.98589 | val_0_accuracy: 0.38095 |  0:00:09s\n",
      "epoch 39 | loss: 1.01494 | val_0_accuracy: 0.45238 |  0:00:09s\n",
      "epoch 40 | loss: 0.9848  | val_0_accuracy: 0.38095 |  0:00:09s\n",
      "epoch 41 | loss: 1.01229 | val_0_accuracy: 0.40476 |  0:00:09s\n",
      "epoch 42 | loss: 0.96721 | val_0_accuracy: 0.33333 |  0:00:10s\n",
      "epoch 43 | loss: 0.9772  | val_0_accuracy: 0.33333 |  0:00:10s\n",
      "epoch 44 | loss: 1.00185 | val_0_accuracy: 0.40476 |  0:00:10s\n",
      "epoch 45 | loss: 0.97152 | val_0_accuracy: 0.52381 |  0:00:10s\n",
      "epoch 46 | loss: 0.97637 | val_0_accuracy: 0.42857 |  0:00:11s\n",
      "epoch 47 | loss: 0.97025 | val_0_accuracy: 0.45238 |  0:00:11s\n",
      "epoch 48 | loss: 0.96963 | val_0_accuracy: 0.45238 |  0:00:11s\n",
      "epoch 49 | loss: 0.95964 | val_0_accuracy: 0.38095 |  0:00:11s\n",
      "epoch 50 | loss: 0.9561  | val_0_accuracy: 0.38095 |  0:00:11s\n",
      "epoch 51 | loss: 0.92894 | val_0_accuracy: 0.40476 |  0:00:12s\n",
      "epoch 52 | loss: 0.90108 | val_0_accuracy: 0.47619 |  0:00:12s\n",
      "epoch 53 | loss: 0.93355 | val_0_accuracy: 0.45238 |  0:00:12s\n",
      "epoch 54 | loss: 0.93988 | val_0_accuracy: 0.45238 |  0:00:12s\n",
      "epoch 55 | loss: 0.90853 | val_0_accuracy: 0.40476 |  0:00:12s\n",
      "epoch 56 | loss: 0.96739 | val_0_accuracy: 0.42857 |  0:00:13s\n",
      "epoch 57 | loss: 0.93839 | val_0_accuracy: 0.47619 |  0:00:13s\n",
      "epoch 58 | loss: 0.88537 | val_0_accuracy: 0.47619 |  0:00:13s\n",
      "\n",
      "Early stopping occurred at epoch 58 with best_epoch = 8 and best_val_0_accuracy = 0.52381\n",
      "\n",
      "Fold 8 Test Metrics:\n",
      "Accuracy: 0.5238\n",
      "F1 Score: 0.4861\n",
      "Precision: 0.4749\n",
      "Recall: 0.5238\n",
      "AUPRC: 0.5261\n",
      "AUROC: 0.6770\n",
      "\n",
      "==================================================\n",
      "Fold 9/10\n",
      "==================================================\n",
      "epoch 0  | loss: 1.56826 | val_0_accuracy: 0.4878  |  0:00:00s\n",
      "epoch 1  | loss: 1.35401 | val_0_accuracy: 0.4878  |  0:00:00s\n",
      "epoch 2  | loss: 1.23974 | val_0_accuracy: 0.46341 |  0:00:00s\n",
      "epoch 3  | loss: 1.22852 | val_0_accuracy: 0.56098 |  0:00:00s\n",
      "epoch 4  | loss: 1.19223 | val_0_accuracy: 0.4878  |  0:00:01s\n",
      "epoch 5  | loss: 1.17364 | val_0_accuracy: 0.41463 |  0:00:01s\n",
      "epoch 6  | loss: 1.15442 | val_0_accuracy: 0.43902 |  0:00:01s\n",
      "epoch 7  | loss: 1.13481 | val_0_accuracy: 0.53659 |  0:00:01s\n",
      "epoch 8  | loss: 1.09821 | val_0_accuracy: 0.53659 |  0:00:02s\n",
      "epoch 9  | loss: 1.09265 | val_0_accuracy: 0.5122  |  0:00:02s\n",
      "epoch 10 | loss: 1.09355 | val_0_accuracy: 0.46341 |  0:00:02s\n",
      "epoch 11 | loss: 1.13594 | val_0_accuracy: 0.39024 |  0:00:02s\n",
      "epoch 12 | loss: 1.11321 | val_0_accuracy: 0.41463 |  0:00:02s\n",
      "epoch 13 | loss: 1.1323  | val_0_accuracy: 0.36585 |  0:00:03s\n",
      "epoch 14 | loss: 1.08033 | val_0_accuracy: 0.31707 |  0:00:03s\n",
      "epoch 15 | loss: 1.06767 | val_0_accuracy: 0.43902 |  0:00:03s\n",
      "epoch 16 | loss: 1.09006 | val_0_accuracy: 0.43902 |  0:00:03s\n",
      "epoch 17 | loss: 1.06439 | val_0_accuracy: 0.41463 |  0:00:04s\n",
      "epoch 18 | loss: 1.06151 | val_0_accuracy: 0.39024 |  0:00:04s\n",
      "epoch 19 | loss: 1.04    | val_0_accuracy: 0.41463 |  0:00:04s\n",
      "epoch 20 | loss: 1.05609 | val_0_accuracy: 0.34146 |  0:00:04s\n",
      "epoch 21 | loss: 1.08558 | val_0_accuracy: 0.41463 |  0:00:04s\n",
      "epoch 22 | loss: 1.0441  | val_0_accuracy: 0.46341 |  0:00:05s\n",
      "epoch 23 | loss: 1.03812 | val_0_accuracy: 0.46341 |  0:00:05s\n",
      "epoch 24 | loss: 1.02511 | val_0_accuracy: 0.46341 |  0:00:05s\n",
      "epoch 25 | loss: 1.09504 | val_0_accuracy: 0.58537 |  0:00:05s\n",
      "epoch 26 | loss: 1.06392 | val_0_accuracy: 0.56098 |  0:00:06s\n",
      "epoch 27 | loss: 1.06053 | val_0_accuracy: 0.56098 |  0:00:06s\n",
      "epoch 28 | loss: 1.04073 | val_0_accuracy: 0.4878  |  0:00:06s\n",
      "epoch 29 | loss: 1.05802 | val_0_accuracy: 0.5122  |  0:00:06s\n",
      "epoch 30 | loss: 1.02101 | val_0_accuracy: 0.5122  |  0:00:07s\n",
      "epoch 31 | loss: 1.05166 | val_0_accuracy: 0.56098 |  0:00:07s\n",
      "epoch 32 | loss: 1.01089 | val_0_accuracy: 0.56098 |  0:00:07s\n",
      "epoch 33 | loss: 1.00062 | val_0_accuracy: 0.4878  |  0:00:07s\n",
      "epoch 34 | loss: 1.03347 | val_0_accuracy: 0.46341 |  0:00:07s\n",
      "epoch 35 | loss: 1.03898 | val_0_accuracy: 0.46341 |  0:00:08s\n",
      "epoch 36 | loss: 1.0109  | val_0_accuracy: 0.4878  |  0:00:08s\n",
      "epoch 37 | loss: 1.00937 | val_0_accuracy: 0.4878  |  0:00:08s\n",
      "epoch 38 | loss: 1.03374 | val_0_accuracy: 0.60976 |  0:00:08s\n",
      "epoch 39 | loss: 0.99201 | val_0_accuracy: 0.53659 |  0:00:08s\n",
      "epoch 40 | loss: 0.99312 | val_0_accuracy: 0.53659 |  0:00:09s\n",
      "epoch 41 | loss: 1.00982 | val_0_accuracy: 0.5122  |  0:00:09s\n",
      "epoch 42 | loss: 0.99194 | val_0_accuracy: 0.56098 |  0:00:09s\n",
      "epoch 43 | loss: 1.0194  | val_0_accuracy: 0.46341 |  0:00:09s\n",
      "epoch 44 | loss: 1.05319 | val_0_accuracy: 0.5122  |  0:00:10s\n",
      "epoch 45 | loss: 0.97642 | val_0_accuracy: 0.53659 |  0:00:10s\n",
      "epoch 46 | loss: 1.03482 | val_0_accuracy: 0.46341 |  0:00:10s\n",
      "epoch 47 | loss: 0.98417 | val_0_accuracy: 0.4878  |  0:00:10s\n",
      "epoch 48 | loss: 1.00497 | val_0_accuracy: 0.4878  |  0:00:10s\n",
      "epoch 49 | loss: 1.02659 | val_0_accuracy: 0.5122  |  0:00:11s\n",
      "epoch 50 | loss: 0.99123 | val_0_accuracy: 0.60976 |  0:00:11s\n",
      "epoch 51 | loss: 0.97831 | val_0_accuracy: 0.60976 |  0:00:11s\n",
      "epoch 52 | loss: 0.95557 | val_0_accuracy: 0.60976 |  0:00:11s\n",
      "epoch 53 | loss: 0.95492 | val_0_accuracy: 0.56098 |  0:00:12s\n",
      "epoch 54 | loss: 0.99198 | val_0_accuracy: 0.60976 |  0:00:12s\n",
      "epoch 55 | loss: 0.96874 | val_0_accuracy: 0.5122  |  0:00:12s\n",
      "epoch 56 | loss: 0.97586 | val_0_accuracy: 0.5122  |  0:00:12s\n",
      "epoch 57 | loss: 1.01573 | val_0_accuracy: 0.5122  |  0:00:13s\n",
      "epoch 58 | loss: 1.02148 | val_0_accuracy: 0.53659 |  0:00:13s\n",
      "epoch 59 | loss: 1.00707 | val_0_accuracy: 0.58537 |  0:00:13s\n",
      "epoch 60 | loss: 1.03735 | val_0_accuracy: 0.58537 |  0:00:13s\n",
      "epoch 61 | loss: 1.0618  | val_0_accuracy: 0.53659 |  0:00:14s\n",
      "epoch 62 | loss: 0.98805 | val_0_accuracy: 0.5122  |  0:00:14s\n",
      "epoch 63 | loss: 1.00206 | val_0_accuracy: 0.53659 |  0:00:14s\n",
      "epoch 64 | loss: 0.96611 | val_0_accuracy: 0.53659 |  0:00:14s\n",
      "epoch 65 | loss: 1.00859 | val_0_accuracy: 0.5122  |  0:00:15s\n",
      "epoch 66 | loss: 0.96911 | val_0_accuracy: 0.56098 |  0:00:15s\n",
      "epoch 67 | loss: 1.00183 | val_0_accuracy: 0.46341 |  0:00:15s\n",
      "epoch 68 | loss: 0.95799 | val_0_accuracy: 0.43902 |  0:00:15s\n",
      "epoch 69 | loss: 0.97915 | val_0_accuracy: 0.4878  |  0:00:16s\n",
      "epoch 70 | loss: 0.99742 | val_0_accuracy: 0.5122  |  0:00:16s\n",
      "epoch 71 | loss: 0.9793  | val_0_accuracy: 0.58537 |  0:00:16s\n",
      "epoch 72 | loss: 0.98529 | val_0_accuracy: 0.58537 |  0:00:16s\n",
      "epoch 73 | loss: 1.00501 | val_0_accuracy: 0.58537 |  0:00:17s\n",
      "epoch 74 | loss: 0.97619 | val_0_accuracy: 0.58537 |  0:00:17s\n",
      "epoch 75 | loss: 0.9962  | val_0_accuracy: 0.58537 |  0:00:17s\n",
      "epoch 76 | loss: 0.98763 | val_0_accuracy: 0.5122  |  0:00:17s\n",
      "epoch 77 | loss: 0.95208 | val_0_accuracy: 0.53659 |  0:00:17s\n",
      "epoch 78 | loss: 0.99192 | val_0_accuracy: 0.53659 |  0:00:18s\n",
      "epoch 79 | loss: 1.02348 | val_0_accuracy: 0.53659 |  0:00:18s\n",
      "epoch 80 | loss: 0.93971 | val_0_accuracy: 0.5122  |  0:00:18s\n",
      "epoch 81 | loss: 0.92894 | val_0_accuracy: 0.5122  |  0:00:18s\n",
      "epoch 82 | loss: 0.9127  | val_0_accuracy: 0.4878  |  0:00:19s\n",
      "epoch 83 | loss: 0.95869 | val_0_accuracy: 0.53659 |  0:00:19s\n",
      "epoch 84 | loss: 0.94586 | val_0_accuracy: 0.56098 |  0:00:19s\n",
      "epoch 85 | loss: 0.963   | val_0_accuracy: 0.46341 |  0:00:19s\n",
      "epoch 86 | loss: 0.96568 | val_0_accuracy: 0.53659 |  0:00:20s\n",
      "epoch 87 | loss: 0.95469 | val_0_accuracy: 0.5122  |  0:00:20s\n",
      "epoch 88 | loss: 0.90932 | val_0_accuracy: 0.56098 |  0:00:20s\n",
      "\n",
      "Early stopping occurred at epoch 88 with best_epoch = 38 and best_val_0_accuracy = 0.60976\n",
      "\n",
      "Fold 9 Test Metrics:\n",
      "Accuracy: 0.6098\n",
      "F1 Score: 0.5736\n",
      "Precision: 0.5623\n",
      "Recall: 0.6098\n",
      "AUPRC: 0.5397\n",
      "AUROC: 0.7136\n",
      "\n",
      "==================================================\n",
      "Fold 10/10\n",
      "==================================================\n",
      "epoch 0  | loss: 1.65025 | val_0_accuracy: 0.31707 |  0:00:00s\n",
      "epoch 1  | loss: 1.28663 | val_0_accuracy: 0.34146 |  0:00:00s\n",
      "epoch 2  | loss: 1.23164 | val_0_accuracy: 0.36585 |  0:00:00s\n",
      "epoch 3  | loss: 1.22158 | val_0_accuracy: 0.36585 |  0:00:01s\n",
      "epoch 4  | loss: 1.15045 | val_0_accuracy: 0.34146 |  0:00:01s\n",
      "epoch 5  | loss: 1.13795 | val_0_accuracy: 0.39024 |  0:00:01s\n",
      "epoch 6  | loss: 1.14976 | val_0_accuracy: 0.34146 |  0:00:01s\n",
      "epoch 7  | loss: 1.10745 | val_0_accuracy: 0.36585 |  0:00:02s\n",
      "epoch 8  | loss: 1.09218 | val_0_accuracy: 0.31707 |  0:00:02s\n",
      "epoch 9  | loss: 1.12266 | val_0_accuracy: 0.39024 |  0:00:02s\n",
      "epoch 10 | loss: 1.10447 | val_0_accuracy: 0.31707 |  0:00:03s\n",
      "epoch 11 | loss: 1.10472 | val_0_accuracy: 0.34146 |  0:00:03s\n",
      "epoch 12 | loss: 1.07452 | val_0_accuracy: 0.31707 |  0:00:03s\n",
      "epoch 13 | loss: 1.07081 | val_0_accuracy: 0.36585 |  0:00:04s\n",
      "epoch 14 | loss: 1.05539 | val_0_accuracy: 0.36585 |  0:00:04s\n",
      "epoch 15 | loss: 1.07484 | val_0_accuracy: 0.39024 |  0:00:04s\n",
      "epoch 16 | loss: 1.01228 | val_0_accuracy: 0.41463 |  0:00:04s\n",
      "epoch 17 | loss: 1.08337 | val_0_accuracy: 0.39024 |  0:00:05s\n",
      "epoch 18 | loss: 1.06261 | val_0_accuracy: 0.34146 |  0:00:05s\n",
      "epoch 19 | loss: 1.02337 | val_0_accuracy: 0.34146 |  0:00:05s\n",
      "epoch 20 | loss: 1.06017 | val_0_accuracy: 0.39024 |  0:00:05s\n",
      "epoch 21 | loss: 1.05715 | val_0_accuracy: 0.41463 |  0:00:06s\n",
      "epoch 22 | loss: 1.03465 | val_0_accuracy: 0.39024 |  0:00:06s\n",
      "epoch 23 | loss: 1.01187 | val_0_accuracy: 0.39024 |  0:00:06s\n",
      "epoch 24 | loss: 1.00306 | val_0_accuracy: 0.39024 |  0:00:07s\n",
      "epoch 25 | loss: 0.98455 | val_0_accuracy: 0.36585 |  0:00:07s\n",
      "epoch 26 | loss: 1.04724 | val_0_accuracy: 0.46341 |  0:00:07s\n",
      "epoch 27 | loss: 0.99997 | val_0_accuracy: 0.43902 |  0:00:08s\n",
      "epoch 28 | loss: 0.99745 | val_0_accuracy: 0.43902 |  0:00:08s\n",
      "epoch 29 | loss: 0.977   | val_0_accuracy: 0.43902 |  0:00:08s\n",
      "epoch 30 | loss: 0.99789 | val_0_accuracy: 0.43902 |  0:00:09s\n",
      "epoch 31 | loss: 1.03095 | val_0_accuracy: 0.36585 |  0:00:09s\n",
      "epoch 32 | loss: 0.99719 | val_0_accuracy: 0.36585 |  0:00:09s\n",
      "epoch 33 | loss: 0.97636 | val_0_accuracy: 0.34146 |  0:00:09s\n",
      "epoch 34 | loss: 0.99103 | val_0_accuracy: 0.31707 |  0:00:10s\n",
      "epoch 35 | loss: 0.98314 | val_0_accuracy: 0.39024 |  0:00:10s\n",
      "epoch 36 | loss: 1.02405 | val_0_accuracy: 0.31707 |  0:00:10s\n",
      "epoch 37 | loss: 0.99563 | val_0_accuracy: 0.31707 |  0:00:10s\n",
      "epoch 38 | loss: 0.95727 | val_0_accuracy: 0.29268 |  0:00:11s\n",
      "epoch 39 | loss: 0.97351 | val_0_accuracy: 0.39024 |  0:00:11s\n",
      "epoch 40 | loss: 0.95286 | val_0_accuracy: 0.36585 |  0:00:11s\n",
      "epoch 41 | loss: 0.98565 | val_0_accuracy: 0.39024 |  0:00:12s\n",
      "epoch 42 | loss: 1.03675 | val_0_accuracy: 0.34146 |  0:00:12s\n",
      "epoch 43 | loss: 0.98239 | val_0_accuracy: 0.41463 |  0:00:12s\n",
      "epoch 44 | loss: 0.99357 | val_0_accuracy: 0.34146 |  0:00:13s\n",
      "epoch 45 | loss: 0.96789 | val_0_accuracy: 0.41463 |  0:00:13s\n",
      "epoch 46 | loss: 0.99082 | val_0_accuracy: 0.39024 |  0:00:13s\n",
      "epoch 47 | loss: 0.93172 | val_0_accuracy: 0.39024 |  0:00:14s\n",
      "epoch 48 | loss: 0.9477  | val_0_accuracy: 0.43902 |  0:00:14s\n",
      "epoch 49 | loss: 0.89554 | val_0_accuracy: 0.34146 |  0:00:14s\n",
      "epoch 50 | loss: 0.97609 | val_0_accuracy: 0.41463 |  0:00:14s\n",
      "epoch 51 | loss: 0.97012 | val_0_accuracy: 0.36585 |  0:00:15s\n",
      "epoch 52 | loss: 0.98001 | val_0_accuracy: 0.39024 |  0:00:15s\n",
      "epoch 53 | loss: 0.95683 | val_0_accuracy: 0.31707 |  0:00:15s\n",
      "epoch 54 | loss: 0.97667 | val_0_accuracy: 0.43902 |  0:00:15s\n",
      "epoch 55 | loss: 0.92156 | val_0_accuracy: 0.43902 |  0:00:16s\n",
      "epoch 56 | loss: 0.97684 | val_0_accuracy: 0.4878  |  0:00:16s\n",
      "epoch 57 | loss: 0.98432 | val_0_accuracy: 0.46341 |  0:00:16s\n",
      "epoch 58 | loss: 1.00054 | val_0_accuracy: 0.41463 |  0:00:16s\n",
      "epoch 59 | loss: 0.92703 | val_0_accuracy: 0.41463 |  0:00:17s\n",
      "epoch 60 | loss: 0.98843 | val_0_accuracy: 0.39024 |  0:00:17s\n",
      "epoch 61 | loss: 1.02251 | val_0_accuracy: 0.39024 |  0:00:17s\n",
      "epoch 62 | loss: 0.91965 | val_0_accuracy: 0.34146 |  0:00:17s\n",
      "epoch 63 | loss: 0.93754 | val_0_accuracy: 0.31707 |  0:00:18s\n",
      "epoch 64 | loss: 0.93877 | val_0_accuracy: 0.34146 |  0:00:18s\n",
      "epoch 65 | loss: 0.94164 | val_0_accuracy: 0.39024 |  0:00:18s\n",
      "epoch 66 | loss: 0.94198 | val_0_accuracy: 0.36585 |  0:00:18s\n",
      "epoch 67 | loss: 0.94928 | val_0_accuracy: 0.36585 |  0:00:19s\n",
      "epoch 68 | loss: 0.91805 | val_0_accuracy: 0.31707 |  0:00:19s\n",
      "epoch 69 | loss: 0.90293 | val_0_accuracy: 0.26829 |  0:00:19s\n",
      "epoch 70 | loss: 0.95391 | val_0_accuracy: 0.21951 |  0:00:20s\n",
      "epoch 71 | loss: 0.92547 | val_0_accuracy: 0.2439  |  0:00:20s\n",
      "epoch 72 | loss: 0.92075 | val_0_accuracy: 0.26829 |  0:00:20s\n",
      "epoch 73 | loss: 0.91649 | val_0_accuracy: 0.29268 |  0:00:20s\n",
      "epoch 74 | loss: 0.9565  | val_0_accuracy: 0.36585 |  0:00:21s\n",
      "epoch 75 | loss: 0.90032 | val_0_accuracy: 0.36585 |  0:00:21s\n",
      "epoch 76 | loss: 0.9331  | val_0_accuracy: 0.41463 |  0:00:21s\n",
      "epoch 77 | loss: 0.93933 | val_0_accuracy: 0.41463 |  0:00:21s\n",
      "epoch 78 | loss: 0.89966 | val_0_accuracy: 0.36585 |  0:00:22s\n",
      "epoch 79 | loss: 0.88568 | val_0_accuracy: 0.36585 |  0:00:22s\n",
      "epoch 80 | loss: 0.93462 | val_0_accuracy: 0.39024 |  0:00:22s\n",
      "epoch 81 | loss: 0.92178 | val_0_accuracy: 0.36585 |  0:00:22s\n",
      "epoch 82 | loss: 0.92874 | val_0_accuracy: 0.39024 |  0:00:23s\n",
      "epoch 83 | loss: 0.92387 | val_0_accuracy: 0.31707 |  0:00:23s\n",
      "epoch 84 | loss: 0.92584 | val_0_accuracy: 0.34146 |  0:00:23s\n",
      "epoch 85 | loss: 0.91649 | val_0_accuracy: 0.46341 |  0:00:24s\n",
      "epoch 86 | loss: 0.92678 | val_0_accuracy: 0.29268 |  0:00:24s\n",
      "epoch 87 | loss: 0.93822 | val_0_accuracy: 0.31707 |  0:00:24s\n",
      "epoch 88 | loss: 0.88736 | val_0_accuracy: 0.36585 |  0:00:24s\n",
      "epoch 89 | loss: 0.96372 | val_0_accuracy: 0.41463 |  0:00:25s\n",
      "epoch 90 | loss: 0.90595 | val_0_accuracy: 0.39024 |  0:00:25s\n",
      "epoch 91 | loss: 0.92327 | val_0_accuracy: 0.39024 |  0:00:25s\n",
      "epoch 92 | loss: 0.96067 | val_0_accuracy: 0.31707 |  0:00:25s\n",
      "epoch 93 | loss: 0.91745 | val_0_accuracy: 0.34146 |  0:00:26s\n",
      "epoch 94 | loss: 0.96713 | val_0_accuracy: 0.31707 |  0:00:26s\n",
      "epoch 95 | loss: 0.91957 | val_0_accuracy: 0.36585 |  0:00:26s\n",
      "epoch 96 | loss: 0.96208 | val_0_accuracy: 0.41463 |  0:00:26s\n",
      "epoch 97 | loss: 0.93632 | val_0_accuracy: 0.43902 |  0:00:27s\n",
      "epoch 98 | loss: 0.90766 | val_0_accuracy: 0.29268 |  0:00:27s\n",
      "epoch 99 | loss: 0.93721 | val_0_accuracy: 0.39024 |  0:00:27s\n",
      "Stop training because you reached max_epochs = 100 with best_epoch = 56 and best_val_0_accuracy = 0.4878\n",
      "\n",
      "Fold 10 Test Metrics:\n",
      "Accuracy: 0.4878\n",
      "F1 Score: 0.4679\n",
      "Precision: 0.4949\n",
      "Recall: 0.4878\n",
      "AUPRC: 0.4554\n",
      "AUROC: 0.5987\n",
      "Successfully saved model at cirrhosis_tabnet.pkl.zip\n",
      "\n",
      "Saved best model from fold 1 with AUROC 0.7788 as cirrhosis_tabnet.pkl\n",
      "Saved corresponding scaler as scaler.pkl\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import (accuracy_score, f1_score, precision_score,\n",
    "                             recall_score, average_precision_score,\n",
    "                             precision_recall_curve, auc, roc_auc_score)\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import interpolate\n",
    "import joblib\n",
    "import warnings\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "import torch\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 设置字体为黑体，确保中文可见\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "def calculate_metrics(y_true, y_pred, y_scores):\n",
    "    # 检查NaN值\n",
    "    if np.isnan(y_scores).any():\n",
    "        y_scores = np.nan_to_num(y_scores)\n",
    "\n",
    "    # 初始化存储每个类别的指标\n",
    "    class_metrics = []\n",
    "\n",
    "    for class_idx in range(4):  # 4个类别\n",
    "        # 确保y_true是numpy数组\n",
    "        y_true_np = np.array(y_true)\n",
    "        y_pred_np = np.array(y_pred)\n",
    "\n",
    "        # 二分类指标计算\n",
    "        y_true_class = (y_true_np == class_idx).astype(int)\n",
    "        y_pred_class = (y_pred_np == class_idx).astype(int)\n",
    "        y_scores_class = y_scores[:, class_idx]\n",
    "\n",
    "        try:\n",
    "            accuracy = accuracy_score(y_true_class, y_pred_class)\n",
    "            f1 = f1_score(y_true_class, y_pred_class, zero_division=0)\n",
    "            precision = precision_score(y_true_class, y_pred_class, zero_division=0)\n",
    "            recall = recall_score(y_true_class, y_pred_class, zero_division=0)\n",
    "\n",
    "            # 处理AUPRC计算\n",
    "            if len(np.unique(y_true_class)) > 1:\n",
    "                auprc = average_precision_score(y_true_class, y_scores_class)\n",
    "            else:\n",
    "                auprc = 0.0\n",
    "\n",
    "            # 处理AUROC计算\n",
    "            if len(np.unique(y_true_class)) > 1:\n",
    "                auroc = roc_auc_score(y_true_class, y_scores_class)\n",
    "            else:\n",
    "                auroc = 0.0\n",
    "\n",
    "            class_metrics.append({\n",
    "                'accuracy': accuracy,\n",
    "                'f1': f1,\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'auprc': auprc,\n",
    "                'auroc': auroc\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"计算类别{class_idx}指标时出错: {str(e)}\")\n",
    "            class_metrics.append({\n",
    "                'accuracy': 0,\n",
    "                'f1': 0,\n",
    "                'precision': 0,\n",
    "                'recall': 0,\n",
    "                'auprc': 0,\n",
    "                'auroc': 0\n",
    "            })\n",
    "\n",
    "    # 计算加权平均指标\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_true, y_pred),\n",
    "        'f1': f1_score(y_true, y_pred, average='weighted', zero_division=0),\n",
    "        'precision': precision_score(y_true, y_pred, average='weighted', zero_division=0),\n",
    "        'recall': recall_score(y_true, y_pred, average='weighted', zero_division=0),\n",
    "        'auprc': average_precision_score(y_true, y_scores, average='weighted'),\n",
    "        'auroc': roc_auc_score(y_true, y_scores, multi_class='ovr', average='weighted'),\n",
    "        'class_metrics': class_metrics\n",
    "    }\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def interpolate_pr_curve(precision, recall):\n",
    "    \"\"\"插值PR曲线到固定长度的点\"\"\"\n",
    "    f = interpolate.interp1d(recall, precision, bounds_error=False, fill_value=(1.0, 0.0))\n",
    "    new_recall = np.linspace(0, 1, 100)\n",
    "    new_precision = f(new_recall)\n",
    "    return new_precision, new_recall\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, fold, dpi=720):\n",
    "    \"\"\"绘制正方形混淆矩阵\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    cm_percentage = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100  # 百分比表示\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    ax = sns.heatmap(cm_percentage, annot=False, fmt='.2f', cmap='Blues', square=True, cbar=False,\n",
    "                     linewidths=2, linecolor='black')\n",
    "\n",
    "    # 在每个格子中显示个数和百分比\n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            # 判断字体颜色，深色背景用白色字体，浅色背景用黑色字体\n",
    "            text_color = 'white' if cm_percentage[i, j] > 50 else 'black'\n",
    "\n",
    "            # 将个数和百分百分比分行显示，个数在上，百分比在下\n",
    "            ax.text(j + 0.5, i + 0.5, f'{cm[i, j]}\\n({cm_percentage[i, j]:.2f}%)',\n",
    "                    color=text_color, ha='center', va='center', fontsize=14, fontweight='bold')\n",
    "\n",
    "    # 添加中文标签\n",
    "    plt.xlabel('预测类别', fontsize=16, fontweight='bold')\n",
    "    plt.ylabel('实际类别', fontsize=16, fontweight='bold')\n",
    "    plt.xticks(ticks=np.arange(4) + 0.5, labels=np.arange(1, 5), fontsize=14, fontweight='bold')\n",
    "    plt.yticks(ticks=np.arange(4) + 0.5, labels=np.arange(1, 5), fontsize=14, fontweight='bold')\n",
    "\n",
    "    # 调整布局，减少空白边缘\n",
    "    plt.subplots_adjust(left=0.1, right=0.9, top=0.9, bottom=0.1)\n",
    "\n",
    "    # 保存混淆矩阵图\n",
    "    plt.savefig(f'CI_TabNet_best_fold_confusion_matrix_fold{fold}.png', dpi=dpi)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_pr_curve(y_true, y_scores, fold):\n",
    "    try:\n",
    "        # 多分类PR曲线 - 使用每个类别的PR曲线\n",
    "        precision = dict()\n",
    "        recall = dict()\n",
    "        auprc = dict()\n",
    "\n",
    "        for i in range(4):  # 4个类别\n",
    "            precision[i], recall[i], _ = precision_recall_curve(y_true == i, y_scores[:, i])\n",
    "            auprc[i] = auc(recall[i], precision[i])\n",
    "\n",
    "        plt.figure()\n",
    "        for i in range(4):\n",
    "            plt.plot(recall[i], precision[i], label=f'Class {i} (AUPRC = {auprc[i]:.2f})')\n",
    "\n",
    "        plt.xlabel('Recall')\n",
    "        plt.ylabel('Precision')\n",
    "        plt.title(f'Precision-Recall Curve (Fold {fold})')\n",
    "        plt.legend()\n",
    "        plt.savefig(f'pr_curve_fold{fold}.png')\n",
    "        plt.close()\n",
    "        return precision, recall\n",
    "    except Exception as e:\n",
    "        print(f\"无法绘制Fold {fold}的PR曲线: {str(e)}\")\n",
    "        return None, None\n",
    "\n",
    "\n",
    "def train_test_split(X, y, splits=10, batch_size=32):\n",
    "    # 检查数据\n",
    "    print(f\"数据形状: X={X.shape}, y={y.shape}\")\n",
    "    print(f\"类别分布: {np.bincount(y)}\")\n",
    "\n",
    "    # 处理可能的NaN值\n",
    "    X = np.nan_to_num(X)\n",
    "    y = np.nan_to_num(y).astype(int)\n",
    "\n",
    "    k_fold = StratifiedKFold(n_splits=splits, shuffle=True, random_state=2025)\n",
    "    results = []\n",
    "    all_class_metrics = []  # 存储所有折的每个类别的指标\n",
    "    best_model_info = {'val_score': -float('inf'), 'model': None, 'fold': -1}\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(k_fold.split(X, y)):\n",
    "        print(f'\\n{\"=\" * 50}')\n",
    "        print(f'Fold {fold + 1}/{splits}')\n",
    "        print(f'{\"=\" * 50}')\n",
    "\n",
    "        # 分割数据\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "        # 标准化\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "        # 创建TabNet模型\n",
    "        model = TabNetClassifier(\n",
    "            n_d=8, n_a=8, n_steps=3, gamma=1.3, lambda_sparse=0,\n",
    "            optimizer_fn=torch.optim.Adam, optimizer_params=dict(lr=2e-2),\n",
    "            mask_type='sparsemax', # Optional: use sparsemax or sigmoid\n",
    "            scheduler_params=dict(step_size=50, gamma=0.9),\n",
    "            scheduler_fn=torch.optim.lr_scheduler.StepLR\n",
    "        )\n",
    "\n",
    "        # 训练模型\n",
    "        model.fit(X_train, y_train, eval_set=[(X_test, y_test)], patience=50, batch_size=batch_size)\n",
    "\n",
    "        # 预测\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_scores = model.predict_proba(X_test)  # Get probabilities for all classes\n",
    "\n",
    "        # 计算指标\n",
    "        metrics = calculate_metrics(y_test, y_pred, y_scores)\n",
    "        all_class_metrics.append(metrics['class_metrics'])\n",
    "\n",
    "        # 检查是否为最佳模型\n",
    "        current_score = metrics['auroc']  # 使用AUROC作为选择最佳模型的标准\n",
    "        if current_score > best_model_info['val_score']:\n",
    "            best_model_info['val_score'] = current_score\n",
    "            best_model_info['model'] = model  # TabNet模型可以直接保存\n",
    "            best_model_info['fold'] = fold + 1\n",
    "            best_model_info['scaler'] = scaler\n",
    "\n",
    "        # 绘制并保存当前折的PR曲线\n",
    "        plot_pr_curve(y_test, y_scores, fold + 1)\n",
    "\n",
    "        # 保存结果\n",
    "        results.append(metrics)\n",
    "\n",
    "        # 打印当前折的结果\n",
    "        print(f'\\nFold {fold + 1} Test Metrics:')\n",
    "        print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
    "        print(f\"F1 Score: {metrics['f1']:.4f}\")\n",
    "        print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "        print(f\"Recall: {metrics['recall']:.4f}\")\n",
    "        print(f\"AUPRC: {metrics['auprc']:.4f}\")\n",
    "        print(f\"AUROC: {metrics['auroc']:.4f}\")\n",
    "\n",
    "    # 保存最佳模型\n",
    "    best_model_info['model'].save_model('cirrhosis_tabnet.pkl')\n",
    "    joblib.dump(best_model_info['scaler'], 'scaler.pkl')\n",
    "\n",
    "    print(\n",
    "        f\"\\nSaved best model from fold {best_model_info['fold']} with AUROC {best_model_info['val_score']:.4f} as cirrhosis_tabnet.pkl\")\n",
    "    print(\"Saved corresponding scaler as scaler.pkl\")\n",
    "\n",
    "    # 绘制最佳模型的混淆矩阵\n",
    "    plot_confusion_matrix(y_test, y_pred, best_model_info['fold'], dpi=720)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 加载数据\n",
    "    data = pd.read_csv('preparations/cirrhosis_output.csv')  # 请替换为您的实际文件路径\n",
    "\n",
    "    # 检查数据\n",
    "    print(\"数据前5行:\")\n",
    "    print(data.head())\n",
    "    print(\"\\n类别分布:\")\n",
    "    print(data['Stage'].value_counts())\n",
    "\n",
    "    # 分离特征和目标\n",
    "    feature_cols = ['N_Days', 'Age', 'Bilirubin', 'Albumin', 'Copper', 'SGOT',\n",
    "                    'Tryglicerides', 'Platelets', 'Prothrombin']\n",
    "    X = data[feature_cols].values\n",
    "    y = data['Stage'].values - 1  # 将类别转换为0-3\n",
    "\n",
    "    # 转换为numpy数组\n",
    "    X = X.astype(np.float32)\n",
    "    y = y.astype(np.int64)\n",
    "\n",
    "    # 运行训练和评估\n",
    "    train_test_split(X, y, splits=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0bbc2117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据前5行:\n",
      "   N_Days  Status  Age  Ascites  Hepatomegaly  Spiders  Edema  Bilirubin  \\\n",
      "0     400       2   59      2.0           2.0      2.0      2       14.5   \n",
      "1    4500       0   56      0.0           2.0      2.0      0        1.1   \n",
      "2    1012       2   70      0.0           0.0      0.0      1        1.4   \n",
      "3    1925       2   55      0.0           2.0      2.0      1        1.8   \n",
      "4    1504       1   38      0.0           2.0      2.0      0        3.4   \n",
      "\n",
      "   Albumin  Copper    SGOT  Tryglicerides  Platelets  Prothrombin  Stage  \n",
      "0     2.60   156.0  137.95          172.0      190.0         12.2    4.0  \n",
      "1     4.14    54.0  113.52           88.0      221.0         10.6    3.0  \n",
      "2     3.48   210.0   96.10           55.0      151.0         12.0    4.0  \n",
      "3     2.54    64.0   60.63           92.0      183.0         10.3    4.0  \n",
      "4     3.53   143.0  113.15           72.0      136.0         10.9    3.0  \n",
      "\n",
      "类别分布:\n",
      "Stage\n",
      "3.0    161\n",
      "4.0    144\n",
      "2.0     92\n",
      "1.0     21\n",
      "Name: count, dtype: int64\n",
      "数据形状: X=(418, 9), y=(418,)\n",
      "类别分布: [ 21  92 161 144]\n",
      "\n",
      "==================================================\n",
      "Fold 1/10\n",
      "==================================================\n",
      "\n",
      "Fold 1 Test Metrics:\n",
      "Accuracy: 0.6429\n",
      "F1 Score: 0.6295\n",
      "Precision: 0.6471\n",
      "Recall: 0.6429\n",
      "AUPRC: 0.6567\n",
      "AUROC: 0.8051\n",
      "\n",
      "==================================================\n",
      "Fold 2/10\n",
      "==================================================\n",
      "\n",
      "Fold 2 Test Metrics:\n",
      "Accuracy: 0.5238\n",
      "F1 Score: 0.4604\n",
      "Precision: 0.4133\n",
      "Recall: 0.5238\n",
      "AUPRC: 0.4967\n",
      "AUROC: 0.6726\n",
      "\n",
      "==================================================\n",
      "Fold 3/10\n",
      "==================================================\n",
      "\n",
      "Fold 3 Test Metrics:\n",
      "Accuracy: 0.4762\n",
      "F1 Score: 0.4428\n",
      "Precision: 0.5016\n",
      "Recall: 0.4762\n",
      "AUPRC: 0.5453\n",
      "AUROC: 0.6840\n",
      "\n",
      "==================================================\n",
      "Fold 4/10\n",
      "==================================================\n",
      "\n",
      "Fold 4 Test Metrics:\n",
      "Accuracy: 0.5238\n",
      "F1 Score: 0.4946\n",
      "Precision: 0.6034\n",
      "Recall: 0.5238\n",
      "AUPRC: 0.5588\n",
      "AUROC: 0.6986\n",
      "\n",
      "==================================================\n",
      "Fold 5/10\n",
      "==================================================\n",
      "\n",
      "Fold 5 Test Metrics:\n",
      "Accuracy: 0.5000\n",
      "F1 Score: 0.4572\n",
      "Precision: 0.4504\n",
      "Recall: 0.5000\n",
      "AUPRC: 0.5730\n",
      "AUROC: 0.7082\n",
      "\n",
      "==================================================\n",
      "Fold 6/10\n",
      "==================================================\n",
      "\n",
      "Fold 6 Test Metrics:\n",
      "Accuracy: 0.5476\n",
      "F1 Score: 0.4886\n",
      "Precision: 0.5120\n",
      "Recall: 0.5476\n",
      "AUPRC: 0.6044\n",
      "AUROC: 0.7443\n",
      "\n",
      "==================================================\n",
      "Fold 7/10\n",
      "==================================================\n",
      "\n",
      "Fold 7 Test Metrics:\n",
      "Accuracy: 0.5238\n",
      "F1 Score: 0.4476\n",
      "Precision: 0.3980\n",
      "Recall: 0.5238\n",
      "AUPRC: 0.5132\n",
      "AUROC: 0.6929\n",
      "\n",
      "==================================================\n",
      "Fold 8/10\n",
      "==================================================\n",
      "\n",
      "Fold 8 Test Metrics:\n",
      "Accuracy: 0.5238\n",
      "F1 Score: 0.4650\n",
      "Precision: 0.4793\n",
      "Recall: 0.5238\n",
      "AUPRC: 0.4414\n",
      "AUROC: 0.6103\n",
      "\n",
      "==================================================\n",
      "Fold 9/10\n",
      "==================================================\n",
      "\n",
      "Fold 9 Test Metrics:\n",
      "Accuracy: 0.5854\n",
      "F1 Score: 0.5488\n",
      "Precision: 0.5776\n",
      "Recall: 0.5854\n",
      "AUPRC: 0.5544\n",
      "AUROC: 0.6887\n",
      "\n",
      "==================================================\n",
      "Fold 10/10\n",
      "==================================================\n",
      "\n",
      "Fold 10 Test Metrics:\n",
      "Accuracy: 0.3415\n",
      "F1 Score: 0.3158\n",
      "Precision: 0.3126\n",
      "Recall: 0.3415\n",
      "AUPRC: 0.4233\n",
      "AUROC: 0.5783\n",
      "\n",
      "Saved best model from fold 1 with AUROC 0.8051 as svm_model.pkl\n",
      "Saved corresponding scaler as scaler.pkl\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import (accuracy_score, f1_score, precision_score,\n",
    "                             recall_score, average_precision_score,\n",
    "                             precision_recall_curve, auc, roc_auc_score)\n",
    "from sklearn.svm import SVC\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import interpolate\n",
    "import joblib\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 设置字体为黑体，确保中文可见\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "\n",
    "def calculate_metrics(y_true, y_pred, y_scores):\n",
    "    # 检查NaN值\n",
    "    if np.isnan(y_scores).any():\n",
    "        y_scores = np.nan_to_num(y_scores)\n",
    "\n",
    "    # 初始化存储每个类别的指标\n",
    "    class_metrics = []\n",
    "\n",
    "    for class_idx in range(4):  # 4个类别\n",
    "        # 确保y_true是numpy数组\n",
    "        y_true_np = np.array(y_true)\n",
    "        y_pred_np = np.array(y_pred)\n",
    "\n",
    "        # 二分类指标计算\n",
    "        y_true_class = (y_true_np == class_idx).astype(int)\n",
    "        y_pred_class = (y_pred_np == class_idx).astype(int)\n",
    "        y_scores_class = y_scores[:, class_idx]\n",
    "\n",
    "        try:\n",
    "            accuracy = accuracy_score(y_true_class, y_pred_class)\n",
    "            f1 = f1_score(y_true_class, y_pred_class, zero_division=0)\n",
    "            precision = precision_score(y_true_class, y_pred_class, zero_division=0)\n",
    "            recall = recall_score(y_true_class, y_pred_class, zero_division=0)\n",
    "\n",
    "            # 处理AUPRC计算\n",
    "            if len(np.unique(y_true_class)) > 1:\n",
    "                auprc = average_precision_score(y_true_class, y_scores_class)\n",
    "            else:\n",
    "                auprc = 0.0\n",
    "\n",
    "            # 处理AUROC计算\n",
    "            if len(np.unique(y_true_class)) > 1:\n",
    "                auroc = roc_auc_score(y_true_class, y_scores_class)\n",
    "            else:\n",
    "                auroc = 0.0\n",
    "\n",
    "            class_metrics.append({\n",
    "                'accuracy': accuracy,\n",
    "                'f1': f1,\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'auprc': auprc,\n",
    "                'auroc': auroc\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"计算类别{class_idx}指标时出错: {str(e)}\")\n",
    "            class_metrics.append({\n",
    "                'accuracy': 0,\n",
    "                'f1': 0,\n",
    "                'precision': 0,\n",
    "                'recall': 0,\n",
    "                'auprc': 0,\n",
    "                'auroc': 0\n",
    "            })\n",
    "\n",
    "    # 计算加权平均指标\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_true, y_pred),\n",
    "        'f1': f1_score(y_true, y_pred, average='weighted', zero_division=0),\n",
    "        'precision': precision_score(y_true, y_pred, average='weighted', zero_division=0),\n",
    "        'recall': recall_score(y_true, y_pred, average='weighted', zero_division=0),\n",
    "        'auprc': average_precision_score(y_true, y_scores, average='weighted'),\n",
    "        'auroc': roc_auc_score(y_true, y_scores, multi_class='ovr', average='weighted'),\n",
    "        'class_metrics': class_metrics\n",
    "    }\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def interpolate_pr_curve(precision, recall):\n",
    "    \"\"\"插值PR曲线到固定长度的点\"\"\"\n",
    "    f = interpolate.interp1d(recall, precision, bounds_error=False, fill_value=(1.0, 0.0))\n",
    "    new_recall = np.linspace(0, 1, 100)\n",
    "    new_precision = f(new_recall)\n",
    "    return new_precision, new_recall\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, fold, dpi=720):\n",
    "    \"\"\"绘制正方形混淆矩阵\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    cm_percentage = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100  # 百分比表示\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    ax = sns.heatmap(cm_percentage, annot=False, fmt='.2f', cmap='Blues', square=True, cbar=False,\n",
    "                     linewidths=2, linecolor='black')\n",
    "\n",
    "    # 在每个格子中显示个数和百分比\n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            # 判断字体颜色，深色背景用白色字体，浅色背景用黑色字体\n",
    "            text_color = 'white' if cm_percentage[i, j] > 50 else 'black'\n",
    "\n",
    "            # 将个数和百分比分行显示，个数在上，百分比在下\n",
    "            ax.text(j + 0.5, i + 0.5, f'{cm[i, j]}\\n({cm_percentage[i, j]:.2f}%)',\n",
    "                    color=text_color, ha='center', va='center', fontsize=14, fontweight='bold')\n",
    "\n",
    "    # 添加中文标签\n",
    "    plt.xlabel('预测类别', fontsize=16, fontweight='bold')\n",
    "    plt.ylabel('实际类别', fontsize=16, fontweight='bold')\n",
    "    plt.xticks(ticks=np.arange(4) + 0.5, labels=np.arange(1, 5), fontsize=14, fontweight='bold')\n",
    "    plt.yticks(ticks=np.arange(4) + 0.5, labels=np.arange(1, 5), fontsize=14, fontweight='bold')\n",
    "\n",
    "    # 调整布局，减少空白边缘\n",
    "    plt.subplots_adjust(left=0.1, right=0.9, top=0.9, bottom=0.1)\n",
    "\n",
    "    # 保存混淆矩阵图\n",
    "    plt.savefig(f'best_fold_confusion_matrix_fold{fold}.png', dpi=dpi)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_pr_curve(y_true, y_scores, fold):\n",
    "    try:\n",
    "        # 多分类PR曲线 - 使用每个类别的PR曲线\n",
    "        precision = dict()\n",
    "        recall = dict()\n",
    "        auprc = dict()\n",
    "\n",
    "        for i in range(4):  # 4个类别\n",
    "            precision[i], recall[i], _ = precision_recall_curve(y_true == i, y_scores[:, i])\n",
    "            auprc[i] = auc(recall[i], precision[i])\n",
    "\n",
    "        plt.figure()\n",
    "        for i in range(4):\n",
    "            plt.plot(recall[i], precision[i], label=f'Class {i} (AUPRC = {auprc[i]:.2f})')\n",
    "\n",
    "        plt.xlabel('Recall')\n",
    "        plt.ylabel('Precision')\n",
    "        plt.title(f'Precision-Recall Curve (Fold {fold})')\n",
    "        plt.legend()\n",
    "        plt.savefig(f'pr_curve_fold{fold}.png')\n",
    "        plt.close()\n",
    "        return precision, recall\n",
    "    except Exception as e:\n",
    "        print(f\"无法绘制Fold {fold}的PR曲线: {str(e)}\")\n",
    "        return None, None\n",
    "\n",
    "\n",
    "def train_test_split(X, y, splits=10, batch_size=32):\n",
    "    # 检查数据\n",
    "    print(f\"数据形状: X={X.shape}, y={y.shape}\")\n",
    "    print(f\"类别分布: {np.bincount(y)}\")\n",
    "\n",
    "    # 处理可能的NaN值\n",
    "    X = np.nan_to_num(X)\n",
    "    y = np.nan_to_num(y).astype(int)\n",
    "\n",
    "    k_fold = StratifiedKFold(n_splits=splits, shuffle=True, random_state=2025)\n",
    "    results = []\n",
    "    best_model_info = {'val_score': -float('inf'), 'model': None, 'fold': -1}\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(k_fold.split(X, y)):\n",
    "        print(f'\\n{\"=\" * 50}')\n",
    "        print(f'Fold {fold + 1}/{splits}')\n",
    "        print(f'{\"=\" * 50}')\n",
    "\n",
    "        # 数据切分\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "        # 标准化\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "        # 创建SVM模型\n",
    "        model = SVC(kernel='rbf', C=1, gamma='scale', probability=True, random_state=2025)\n",
    "\n",
    "        # 训练模型\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # 预测\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_pred_proba = model.predict_proba(X_test)\n",
    "\n",
    "        # 计算指标\n",
    "        metrics = calculate_metrics(y_test, y_pred, y_pred_proba)\n",
    "\n",
    "        # 检查是否为最佳模型\n",
    "        current_score = metrics['auroc']  # 使用AUROC作为选择最佳模型的标准\n",
    "        if current_score > best_model_info['val_score']:\n",
    "            best_model_info['val_score'] = current_score\n",
    "            best_model_info['model'] = model\n",
    "            best_model_info['fold'] = fold + 1\n",
    "            best_model_info['scaler'] = scaler\n",
    "\n",
    "        # 绘制并保存当前折的PR曲线\n",
    "        plot_pr_curve(y_test, y_pred_proba, fold + 1)\n",
    "\n",
    "        # 保存结果\n",
    "        results.append(metrics)\n",
    "\n",
    "        # 打印当前折的结果\n",
    "        print(f'\\nFold {fold + 1} Test Metrics:')\n",
    "        print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
    "        print(f\"F1 Score: {metrics['f1']:.4f}\")\n",
    "        print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "        print(f\"Recall: {metrics['recall']:.4f}\")\n",
    "        print(f\"AUPRC: {metrics['auprc']:.4f}\")\n",
    "        print(f\"AUROC: {metrics['auroc']:.4f}\")\n",
    "\n",
    "    # 保存最佳模型\n",
    "    joblib.dump(best_model_info['model'], 'svm_model.pkl')\n",
    "    joblib.dump(best_model_info['scaler'], 'scaler.pkl')\n",
    "\n",
    "    # 打印最佳模型信息\n",
    "    print(f\"\\nSaved best model from fold {best_model_info['fold']} with AUROC {best_model_info['val_score']:.4f} as svm_model.pkl\")\n",
    "    print(\"Saved corresponding scaler as scaler.pkl\")\n",
    "\n",
    "    # 绘制并保存最佳模型的混淆矩阵图\n",
    "    plot_confusion_matrix(y_test, y_pred, best_model_info['fold'], dpi=720)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 加载数据\n",
    "    data = pd.read_csv('preparations/cirrhosis_output.csv')  # 请替换为您的实际文件路径\n",
    "\n",
    "    # 检查数据\n",
    "    print(\"数据前5行:\")\n",
    "    print(data.head())\n",
    "    print(\"\\n类别分布:\")\n",
    "    print(data['Stage'].value_counts())\n",
    "\n",
    "    # 分离特征和目标\n",
    "    feature_cols = ['N_Days', 'Age', 'Bilirubin', 'Albumin', 'Copper', 'SGOT',\n",
    "                    'Tryglicerides', 'Platelets', 'Prothrombin']\n",
    "    X = data[feature_cols].values\n",
    "    y = data['Stage'].values - 1  # 将类别转换为0-3\n",
    "\n",
    "    # 转换为numpy数组\n",
    "    X = X.astype(np.float32)\n",
    "    y = y.astype(np.int64)\n",
    "\n",
    "    # 运行训练和评估\n",
    "    train_test_split(X, y, splits=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5a3db4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据前5行:\n",
      "   N_Days  Status  Age  Ascites  Hepatomegaly  Spiders  Edema  Bilirubin  \\\n",
      "0     400       2   59      2.0           2.0      2.0      2       14.5   \n",
      "1    4500       0   56      0.0           2.0      2.0      0        1.1   \n",
      "2    1012       2   70      0.0           0.0      0.0      1        1.4   \n",
      "3    1925       2   55      0.0           2.0      2.0      1        1.8   \n",
      "4    1504       1   38      0.0           2.0      2.0      0        3.4   \n",
      "\n",
      "   Albumin  Copper    SGOT  Tryglicerides  Platelets  Prothrombin  Stage  \n",
      "0     2.60   156.0  137.95          172.0      190.0         12.2    4.0  \n",
      "1     4.14    54.0  113.52           88.0      221.0         10.6    3.0  \n",
      "2     3.48   210.0   96.10           55.0      151.0         12.0    4.0  \n",
      "3     2.54    64.0   60.63           92.0      183.0         10.3    4.0  \n",
      "4     3.53   143.0  113.15           72.0      136.0         10.9    3.0  \n",
      "\n",
      "类别分布:\n",
      "Stage\n",
      "3.0    161\n",
      "4.0    144\n",
      "2.0     92\n",
      "1.0     21\n",
      "Name: count, dtype: int64\n",
      "数据形状: X=(418, 9), y=(418,)\n",
      "类别分布: [ 21  92 161 144]\n",
      "\n",
      "==================================================\n",
      "Fold 1/10\n",
      "==================================================\n",
      "\n",
      "Fold 1 Test Metrics:\n",
      "Accuracy: 0.6429\n",
      "F1 Score: 0.6266\n",
      "Precision: 0.6134\n",
      "Recall: 0.6429\n",
      "AUPRC: 0.6099\n",
      "AUROC: 0.7688\n",
      "\n",
      "==================================================\n",
      "Fold 2/10\n",
      "==================================================\n",
      "\n",
      "Fold 2 Test Metrics:\n",
      "Accuracy: 0.4762\n",
      "F1 Score: 0.4506\n",
      "Precision: 0.4430\n",
      "Recall: 0.4762\n",
      "AUPRC: 0.4939\n",
      "AUROC: 0.6786\n",
      "\n",
      "==================================================\n",
      "Fold 3/10\n",
      "==================================================\n",
      "\n",
      "Fold 3 Test Metrics:\n",
      "Accuracy: 0.4762\n",
      "F1 Score: 0.4627\n",
      "Precision: 0.4521\n",
      "Recall: 0.4762\n",
      "AUPRC: 0.5098\n",
      "AUROC: 0.6941\n",
      "\n",
      "==================================================\n",
      "Fold 4/10\n",
      "==================================================\n",
      "\n",
      "Fold 4 Test Metrics:\n",
      "Accuracy: 0.5238\n",
      "F1 Score: 0.4760\n",
      "Precision: 0.4953\n",
      "Recall: 0.5238\n",
      "AUPRC: 0.5426\n",
      "AUROC: 0.6953\n",
      "\n",
      "==================================================\n",
      "Fold 5/10\n",
      "==================================================\n",
      "\n",
      "Fold 5 Test Metrics:\n",
      "Accuracy: 0.5000\n",
      "F1 Score: 0.4618\n",
      "Precision: 0.4530\n",
      "Recall: 0.5000\n",
      "AUPRC: 0.5263\n",
      "AUROC: 0.6890\n",
      "\n",
      "==================================================\n",
      "Fold 6/10\n",
      "==================================================\n",
      "\n",
      "Fold 6 Test Metrics:\n",
      "Accuracy: 0.5476\n",
      "F1 Score: 0.5257\n",
      "Precision: 0.5093\n",
      "Recall: 0.5476\n",
      "AUPRC: 0.6652\n",
      "AUROC: 0.7630\n",
      "\n",
      "==================================================\n",
      "Fold 7/10\n",
      "==================================================\n",
      "\n",
      "Fold 7 Test Metrics:\n",
      "Accuracy: 0.3571\n",
      "F1 Score: 0.3342\n",
      "Precision: 0.3260\n",
      "Recall: 0.3571\n",
      "AUPRC: 0.4474\n",
      "AUROC: 0.6135\n",
      "\n",
      "==================================================\n",
      "Fold 8/10\n",
      "==================================================\n",
      "\n",
      "Fold 8 Test Metrics:\n",
      "Accuracy: 0.5000\n",
      "F1 Score: 0.4640\n",
      "Precision: 0.4902\n",
      "Recall: 0.5000\n",
      "AUPRC: 0.4735\n",
      "AUROC: 0.6172\n",
      "\n",
      "==================================================\n",
      "Fold 9/10\n",
      "==================================================\n",
      "\n",
      "Fold 9 Test Metrics:\n",
      "Accuracy: 0.6098\n",
      "F1 Score: 0.5759\n",
      "Precision: 0.5545\n",
      "Recall: 0.6098\n",
      "AUPRC: 0.5677\n",
      "AUROC: 0.6974\n",
      "\n",
      "==================================================\n",
      "Fold 10/10\n",
      "==================================================\n",
      "\n",
      "Fold 10 Test Metrics:\n",
      "Accuracy: 0.4390\n",
      "F1 Score: 0.4245\n",
      "Precision: 0.4135\n",
      "Recall: 0.4390\n",
      "AUPRC: 0.4400\n",
      "AUROC: 0.5740\n",
      "\n",
      "Saved best model from fold 1 with AUROC 0.7688 as cirrhosis_rf.pkl\n",
      "Saved corresponding scaler as scaler.pkl\n",
      "\n",
      "==================================================\n",
      "Final Cross-Validation Results:\n",
      "==================================================\n",
      "Average Accuracy: 0.5073\n",
      "Average F1 Score: 0.4802\n",
      "Average Precision: 0.4750\n",
      "Average Recall: 0.5073\n",
      "Average AUPRC: 0.5276\n",
      "Average AUROC: 0.6791\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import (accuracy_score, f1_score, precision_score,\n",
    "                             recall_score, average_precision_score,\n",
    "                             precision_recall_curve, auc, roc_auc_score, confusion_matrix)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import interpolate\n",
    "import joblib\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "def calculate_metrics(y_true, y_pred, y_scores):\n",
    "    # 检查NaN值\n",
    "    if np.isnan(y_scores).any():\n",
    "        y_scores = np.nan_to_num(y_scores)\n",
    "\n",
    "    # 初始化存储每个类别的指标\n",
    "    class_metrics = []\n",
    "\n",
    "    for class_idx in range(4):  # 4个类别\n",
    "        # 确保y_true是numpy数组\n",
    "        y_true_np = np.array(y_true)\n",
    "        y_pred_np = np.array(y_pred)\n",
    "\n",
    "        # 二分类指标计算\n",
    "        y_true_class = (y_true_np == class_idx).astype(int)\n",
    "        y_pred_class = (y_pred_np == class_idx).astype(int)\n",
    "        y_scores_class = y_scores[:, class_idx]\n",
    "\n",
    "        try:\n",
    "            accuracy = accuracy_score(y_true_class, y_pred_class)\n",
    "            f1 = f1_score(y_true_class, y_pred_class, zero_division=0)\n",
    "            precision = precision_score(y_true_class, y_pred_class, zero_division=0)\n",
    "            recall = recall_score(y_true_class, y_pred_class, zero_division=0)\n",
    "\n",
    "            # 处理AUPRC计算\n",
    "            if len(np.unique(y_true_class)) > 1:\n",
    "                auprc = average_precision_score(y_true_class, y_scores_class)\n",
    "            else:\n",
    "                auprc = 0.0\n",
    "\n",
    "            # 处理AUROC计算\n",
    "            if len(np.unique(y_true_class)) > 1:\n",
    "                auroc = roc_auc_score(y_true_class, y_scores_class)\n",
    "            else:\n",
    "                auroc = 0.0\n",
    "\n",
    "            class_metrics.append({\n",
    "                'accuracy': accuracy,\n",
    "                'f1': f1,\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'auprc': auprc,\n",
    "                'auroc': auroc\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"计算类别{class_idx}指标时出错: {str(e)}\")\n",
    "            class_metrics.append({\n",
    "                'accuracy': 0,\n",
    "                'f1': 0,\n",
    "                'precision': 0,\n",
    "                'recall': 0,\n",
    "                'auprc': 0,\n",
    "                'auroc': 0\n",
    "            })\n",
    "\n",
    "    # 计算加权平均指标\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_true, y_pred),\n",
    "        'f1': f1_score(y_true, y_pred, average='weighted', zero_division=0),\n",
    "        'precision': precision_score(y_true, y_pred, average='weighted', zero_division=0),\n",
    "        'recall': recall_score(y_true, y_pred, average='weighted', zero_division=0),\n",
    "        'auprc': average_precision_score(y_true, y_scores, average='weighted'),\n",
    "        'auroc': roc_auc_score(y_true, y_scores, multi_class='ovr', average='weighted'),\n",
    "        'class_metrics': class_metrics\n",
    "    }\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def interpolate_pr_curve(precision, recall):\n",
    "    \"\"\"插值PR曲线到固定长度的点\"\"\"\n",
    "    f = interpolate.interp1d(recall, precision, bounds_error=False, fill_value=(1.0, 0.0))\n",
    "    new_recall = np.linspace(0, 1, 100)\n",
    "    new_precision = f(new_recall)\n",
    "    return new_precision, new_recall\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, fold, dpi=720):\n",
    "    \"\"\"绘制正方形混淆矩阵\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    cm_percentage = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100  # 百分比表示\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    ax = sns.heatmap(cm_percentage, annot=False, fmt='.2f', cmap='Blues', square=True, cbar=False,\n",
    "                     linewidths=2, linecolor='black')\n",
    "\n",
    "    # 在每个格子中显示个数和百分比\n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            # 判断字体颜色，深色背景用白色字体，浅色背景用黑色字体\n",
    "            text_color = 'white' if cm_percentage[i, j] > 50 else 'black'\n",
    "\n",
    "            # 将个数和百分比分行显示，个数在上，百分比在下\n",
    "            ax.text(j + 0.5, i + 0.5, f'{cm[i, j]}\\n({cm_percentage[i, j]:.2f}%)',\n",
    "                    color=text_color, ha='center', va='center', fontsize=14, fontweight='bold')\n",
    "\n",
    "    # 添加中文标签\n",
    "    plt.xlabel('预测类别', fontsize=16, fontweight='bold')\n",
    "    plt.ylabel('实际类别', fontsize=16, fontweight='bold')\n",
    "    plt.xticks(ticks=np.arange(4) + 0.5, labels=np.arange(1, 5), fontsize=14, fontweight='bold')\n",
    "    plt.yticks(ticks=np.arange(4) + 0.5, labels=np.arange(1, 5), fontsize=14, fontweight='bold')\n",
    "\n",
    "    # 调整布局，减少空白边缘\n",
    "    plt.subplots_adjust(left=0.1, right=0.9, top=0.9, bottom=0.1)\n",
    "\n",
    "    # 保存混淆矩阵图\n",
    "    plt.savefig(f'CI_RF_best_fold_confusion_matrix_fold{fold}.png', dpi=dpi)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def train_test_split(X, y, splits=10, batch_size=32):\n",
    "    # 检查数据\n",
    "    print(f\"数据形状: X={X.shape}, y={y.shape}\")\n",
    "    print(f\"类别分布: {np.bincount(y)}\")\n",
    "\n",
    "    # 处理可能的NaN值\n",
    "    X = np.nan_to_num(X)\n",
    "    y = np.nan_to_num(y).astype(int)\n",
    "\n",
    "    k_fold = StratifiedKFold(n_splits=splits, shuffle=True, random_state=2025)\n",
    "    results = []\n",
    "    best_model_info = {'val_score': -float('inf'), 'model': None, 'fold': -1}\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(k_fold.split(X, y)):\n",
    "        print(f'\\n{\"=\" * 50}')\n",
    "        print(f'Fold {fold + 1}/{splits}')\n",
    "        print(f'{\"=\" * 50}')\n",
    "\n",
    "        # 分割数据\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "        # 标准化\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "        # 使用随机森林替代LightGBM\n",
    "        rf_model = RandomForestClassifier(n_estimators=100, random_state=2025, n_jobs=-1)\n",
    "\n",
    "        # 训练模型\n",
    "        rf_model.fit(X_train, y_train)\n",
    "\n",
    "        # 预测\n",
    "        y_pred = rf_model.predict_proba(X_test)  # 预测类别概率\n",
    "        y_pred_class = np.argmax(y_pred, axis=1)\n",
    "\n",
    "        # 计算指标\n",
    "        metrics = calculate_metrics(y_test, y_pred_class, y_pred)\n",
    "\n",
    "        # 检查是否为最佳模型\n",
    "        current_score = metrics['auroc']  # 使用AUROC作为选择最佳模型的标准\n",
    "        if current_score > best_model_info['val_score']:\n",
    "            best_model_info['val_score'] = current_score\n",
    "            best_model_info['model'] = rf_model\n",
    "            best_model_info['fold'] = fold + 1\n",
    "            best_model_info['scaler'] = scaler\n",
    "\n",
    "        # 绘制并保存最佳折的混淆矩阵\n",
    "        plot_confusion_matrix(y_test, y_pred_class, best_model_info['fold'])\n",
    "\n",
    "        results.append(metrics)\n",
    "\n",
    "        # 打印当前折的结果\n",
    "        print(f'\\nFold {fold + 1} Test Metrics:')\n",
    "        print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
    "        print(f\"F1 Score: {metrics['f1']:.4f}\")\n",
    "        print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "        print(f\"Recall: {metrics['recall']:.4f}\")\n",
    "        print(f\"AUPRC: {metrics['auprc']:.4f}\")\n",
    "        print(f\"AUROC: {metrics['auroc']:.4f}\")\n",
    "\n",
    "    # 保存最佳模型\n",
    "    joblib.dump(best_model_info['model'], 'cirrhosis_rf.pkl')\n",
    "    joblib.dump(best_model_info['scaler'], 'scaler.pkl')\n",
    "\n",
    "    print(\n",
    "        f\"\\nSaved best model from fold {best_model_info['fold']} with AUROC {best_model_info['val_score']:.4f} as cirrhosis_rf.pkl\")\n",
    "    print(\"Saved corresponding scaler as scaler.pkl\")\n",
    "\n",
    "    # 计算并打印平均指标\n",
    "    avg_metrics = {\n",
    "        'accuracy': np.mean([r['accuracy'] for r in results]),\n",
    "        'f1': np.mean([r['f1'] for r in results]),\n",
    "        'precision': np.mean([r['precision'] for r in results]),\n",
    "        'recall': np.mean([r['recall'] for r in results]),\n",
    "        'auprc': np.mean([r['auprc'] for r in results]),\n",
    "        'auroc': np.mean([r['auroc'] for r in results])\n",
    "    }\n",
    "\n",
    "    print('\\n' + '=' * 50)\n",
    "    print('Final Cross-Validation Results:')\n",
    "    print('=' * 50)\n",
    "    print(f\"Average Accuracy: {avg_metrics['accuracy']:.4f}\")\n",
    "    print(f\"Average F1 Score: {avg_metrics['f1']:.4f}\")\n",
    "    print(f\"Average Precision: {avg_metrics['precision']:.4f}\")\n",
    "    print(f\"Average Recall: {avg_metrics['recall']:.4f}\")\n",
    "    print(f\"Average AUPRC: {avg_metrics['auprc']:.4f}\")\n",
    "    print(f\"Average AUROC: {avg_metrics['auroc']:.4f}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 加载数据\n",
    "    data = pd.read_csv('preparations/cirrhosis_output.csv')  # 请替换为您的实际文件路径\n",
    "\n",
    "    # 检查数据\n",
    "    print(\"数据前5行:\")\n",
    "    print(data.head())\n",
    "    print(\"\\n类别分布:\")\n",
    "    print(data['Stage'].value_counts())\n",
    "\n",
    "    # 分离特征和目标\n",
    "    feature_cols = ['N_Days', 'Age', 'Bilirubin', 'Albumin', 'Copper', 'SGOT',\n",
    "                    'Tryglicerides', 'Platelets', 'Prothrombin']\n",
    "    X = data[feature_cols].values\n",
    "    y = data['Stage'].values - 1  # 将类别转换为0-3\n",
    "\n",
    "    # 转换为numpy数组\n",
    "    X = X.astype(np.float32)\n",
    "    y = y.astype(np.int64)\n",
    "\n",
    "    # 运行训练和评估\n",
    "    train_test_split(X, y, splits=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6d17e077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据前5行:\n",
      "   N_Days  Status  Age  Ascites  Hepatomegaly  Spiders  Edema  Bilirubin  \\\n",
      "0     400       2   59      2.0           2.0      2.0      2       14.5   \n",
      "1    4500       0   56      0.0           2.0      2.0      0        1.1   \n",
      "2    1012       2   70      0.0           0.0      0.0      1        1.4   \n",
      "3    1925       2   55      0.0           2.0      2.0      1        1.8   \n",
      "4    1504       1   38      0.0           2.0      2.0      0        3.4   \n",
      "\n",
      "   Albumin  Copper    SGOT  Tryglicerides  Platelets  Prothrombin  Stage  \n",
      "0     2.60   156.0  137.95          172.0      190.0         12.2    4.0  \n",
      "1     4.14    54.0  113.52           88.0      221.0         10.6    3.0  \n",
      "2     3.48   210.0   96.10           55.0      151.0         12.0    4.0  \n",
      "3     2.54    64.0   60.63           92.0      183.0         10.3    4.0  \n",
      "4     3.53   143.0  113.15           72.0      136.0         10.9    3.0  \n",
      "\n",
      "类别分布:\n",
      "Stage\n",
      "3.0    161\n",
      "4.0    144\n",
      "2.0     92\n",
      "1.0     21\n",
      "Name: count, dtype: int64\n",
      "数据形状: X=(418, 9), y=(418,)\n",
      "类别分布: [ 21  92 161 144]\n",
      "\n",
      "==================================================\n",
      "Fold 1/10\n",
      "==================================================\n",
      "Epoch 10/500 - Train Loss: 0.1292, Val Loss: 0.1089\n",
      "Val Metrics - Acc: 0.5714, F1: 0.5743, Precision: 0.5893, Recall: 0.5714, AUPRC: 0.6785, AUROC: 0.8133\n",
      "Epoch 20/500 - Train Loss: 0.1169, Val Loss: 0.1052\n",
      "Val Metrics - Acc: 0.5952, F1: 0.6042, Precision: 0.6347, Recall: 0.5952, AUPRC: 0.6713, AUROC: 0.8120\n",
      "Epoch 30/500 - Train Loss: 0.1065, Val Loss: 0.1060\n",
      "Val Metrics - Acc: 0.5952, F1: 0.6002, Precision: 0.6381, Recall: 0.5952, AUPRC: 0.6620, AUROC: 0.8115\n",
      "Epoch 40/500 - Train Loss: 0.0981, Val Loss: 0.1091\n",
      "Val Metrics - Acc: 0.5952, F1: 0.5942, Precision: 0.6429, Recall: 0.5952, AUPRC: 0.6584, AUROC: 0.8030\n",
      "Early stopping at epoch 43\n",
      "\n",
      "Fold 1 Test Metrics:\n",
      "Accuracy: 0.6190\n",
      "F1 Score: 0.6233\n",
      "Precision: 0.6491\n",
      "Recall: 0.6190\n",
      "AUPRC: 0.6713\n",
      "AUROC: 0.8210\n",
      "\n",
      "==================================================\n",
      "Fold 2/10\n",
      "==================================================\n",
      "Epoch 10/500 - Train Loss: 0.1281, Val Loss: 0.1761\n",
      "Val Metrics - Acc: 0.3810, F1: 0.3738, Precision: 0.3866, Recall: 0.3810, AUPRC: 0.4601, AUROC: 0.5951\n",
      "Epoch 20/500 - Train Loss: 0.1124, Val Loss: 0.1875\n",
      "Val Metrics - Acc: 0.5238, F1: 0.4864, Precision: 0.4941, Recall: 0.5238, AUPRC: 0.4798, AUROC: 0.6174\n",
      "Early stopping at epoch 25\n",
      "\n",
      "Fold 2 Test Metrics:\n",
      "Accuracy: 0.4524\n",
      "F1 Score: 0.4085\n",
      "Precision: 0.3874\n",
      "Recall: 0.4524\n",
      "AUPRC: 0.4435\n",
      "AUROC: 0.5682\n",
      "\n",
      "==================================================\n",
      "Fold 3/10\n",
      "==================================================\n",
      "Epoch 10/500 - Train Loss: 0.1238, Val Loss: 0.1353\n",
      "Val Metrics - Acc: 0.5000, F1: 0.4940, Precision: 0.4934, Recall: 0.5000, AUPRC: 0.6093, AUROC: 0.7017\n",
      "Epoch 20/500 - Train Loss: 0.1086, Val Loss: 0.1344\n",
      "Val Metrics - Acc: 0.4524, F1: 0.4426, Precision: 0.4359, Recall: 0.4524, AUPRC: 0.5971, AUROC: 0.6977\n",
      "Epoch 30/500 - Train Loss: 0.0985, Val Loss: 0.1381\n",
      "Val Metrics - Acc: 0.4286, F1: 0.4189, Precision: 0.4178, Recall: 0.4286, AUPRC: 0.5825, AUROC: 0.6946\n",
      "Early stopping at epoch 34\n",
      "\n",
      "Fold 3 Test Metrics:\n",
      "Accuracy: 0.5000\n",
      "F1 Score: 0.4878\n",
      "Precision: 0.4814\n",
      "Recall: 0.5000\n",
      "AUPRC: 0.6269\n",
      "AUROC: 0.7060\n",
      "\n",
      "==================================================\n",
      "Fold 4/10\n",
      "==================================================\n",
      "Epoch 10/500 - Train Loss: 0.1268, Val Loss: 0.1271\n",
      "Val Metrics - Acc: 0.5952, F1: 0.5810, Precision: 0.5769, Recall: 0.5952, AUPRC: 0.6841, AUROC: 0.7993\n",
      "Epoch 20/500 - Train Loss: 0.1115, Val Loss: 0.1273\n",
      "Val Metrics - Acc: 0.5476, F1: 0.5366, Precision: 0.5291, Recall: 0.5476, AUPRC: 0.6531, AUROC: 0.8100\n",
      "Epoch 30/500 - Train Loss: 0.1016, Val Loss: 0.1272\n",
      "Val Metrics - Acc: 0.5952, F1: 0.5783, Precision: 0.5798, Recall: 0.5952, AUPRC: 0.6403, AUROC: 0.8001\n",
      "Epoch 40/500 - Train Loss: 0.0935, Val Loss: 0.1277\n",
      "Val Metrics - Acc: 0.5476, F1: 0.5348, Precision: 0.5293, Recall: 0.5476, AUPRC: 0.6462, AUROC: 0.7964\n",
      "Early stopping at epoch 44\n",
      "\n",
      "Fold 4 Test Metrics:\n",
      "Accuracy: 0.5952\n",
      "F1 Score: 0.5822\n",
      "Precision: 0.5807\n",
      "Recall: 0.5952\n",
      "AUPRC: 0.6575\n",
      "AUROC: 0.8117\n",
      "\n",
      "==================================================\n",
      "Fold 5/10\n",
      "==================================================\n",
      "Epoch 10/500 - Train Loss: 0.1289, Val Loss: 0.1540\n",
      "Val Metrics - Acc: 0.4286, F1: 0.3759, Precision: 0.3553, Recall: 0.4286, AUPRC: 0.4173, AUROC: 0.6101\n",
      "Epoch 20/500 - Train Loss: 0.1128, Val Loss: 0.1481\n",
      "Val Metrics - Acc: 0.4524, F1: 0.4074, Precision: 0.3920, Recall: 0.4524, AUPRC: 0.4410, AUROC: 0.6363\n",
      "Epoch 30/500 - Train Loss: 0.1011, Val Loss: 0.1512\n",
      "Val Metrics - Acc: 0.4762, F1: 0.4364, Precision: 0.4165, Recall: 0.4762, AUPRC: 0.4496, AUROC: 0.6362\n",
      "Early stopping at epoch 40\n",
      "\n",
      "Fold 5 Test Metrics:\n",
      "Accuracy: 0.4524\n",
      "F1 Score: 0.4074\n",
      "Precision: 0.3920\n",
      "Recall: 0.4524\n",
      "AUPRC: 0.4410\n",
      "AUROC: 0.6363\n",
      "\n",
      "==================================================\n",
      "Fold 6/10\n",
      "==================================================\n",
      "Epoch 10/500 - Train Loss: 0.1245, Val Loss: 0.1661\n",
      "Val Metrics - Acc: 0.5238, F1: 0.4873, Precision: 0.4841, Recall: 0.5238, AUPRC: 0.5562, AUROC: 0.6975\n",
      "Epoch 20/500 - Train Loss: 0.1107, Val Loss: 0.1763\n",
      "Val Metrics - Acc: 0.5476, F1: 0.4960, Precision: 0.4728, Recall: 0.5476, AUPRC: 0.5538, AUROC: 0.6938\n",
      "Early stopping at epoch 23\n",
      "\n",
      "Fold 6 Test Metrics:\n",
      "Accuracy: 0.4762\n",
      "F1 Score: 0.3996\n",
      "Precision: 0.3471\n",
      "Recall: 0.4762\n",
      "AUPRC: 0.5343\n",
      "AUROC: 0.6948\n",
      "\n",
      "==================================================\n",
      "Fold 7/10\n",
      "==================================================\n",
      "Epoch 10/500 - Train Loss: 0.1271, Val Loss: 0.1476\n",
      "Val Metrics - Acc: 0.4286, F1: 0.4062, Precision: 0.4242, Recall: 0.4286, AUPRC: 0.4822, AUROC: 0.6151\n",
      "Epoch 20/500 - Train Loss: 0.1119, Val Loss: 0.1466\n",
      "Val Metrics - Acc: 0.4048, F1: 0.3816, Precision: 0.3822, Recall: 0.4048, AUPRC: 0.4870, AUROC: 0.6109\n",
      "Epoch 30/500 - Train Loss: 0.1026, Val Loss: 0.1452\n",
      "Val Metrics - Acc: 0.4286, F1: 0.4080, Precision: 0.3918, Recall: 0.4286, AUPRC: 0.5131, AUROC: 0.6301\n",
      "Epoch 40/500 - Train Loss: 0.0941, Val Loss: 0.1471\n",
      "Val Metrics - Acc: 0.4048, F1: 0.3915, Precision: 0.3833, Recall: 0.4048, AUPRC: 0.4962, AUROC: 0.6275\n",
      "Early stopping at epoch 44\n",
      "\n",
      "Fold 7 Test Metrics:\n",
      "Accuracy: 0.4286\n",
      "F1 Score: 0.4052\n",
      "Precision: 0.4050\n",
      "Recall: 0.4286\n",
      "AUPRC: 0.5065\n",
      "AUROC: 0.6228\n",
      "\n",
      "==================================================\n",
      "Fold 8/10\n",
      "==================================================\n",
      "Epoch 10/500 - Train Loss: 0.1254, Val Loss: 0.1913\n",
      "Val Metrics - Acc: 0.4524, F1: 0.4235, Precision: 0.4544, Recall: 0.4524, AUPRC: 0.4142, AUROC: 0.5776\n",
      "Epoch 20/500 - Train Loss: 0.1102, Val Loss: 0.1902\n",
      "Val Metrics - Acc: 0.5000, F1: 0.4751, Precision: 0.4651, Recall: 0.5000, AUPRC: 0.4608, AUROC: 0.6083\n",
      "Early stopping at epoch 22\n",
      "\n",
      "Fold 8 Test Metrics:\n",
      "Accuracy: 0.4286\n",
      "F1 Score: 0.3906\n",
      "Precision: 0.4059\n",
      "Recall: 0.4286\n",
      "AUPRC: 0.3994\n",
      "AUROC: 0.5290\n",
      "\n",
      "==================================================\n",
      "Fold 9/10\n",
      "==================================================\n",
      "Epoch 10/500 - Train Loss: 0.1290, Val Loss: 0.1405\n",
      "Val Metrics - Acc: 0.5854, F1: 0.5600, Precision: 0.5695, Recall: 0.5854, AUPRC: 0.5799, AUROC: 0.6910\n",
      "Epoch 20/500 - Train Loss: 0.1130, Val Loss: 0.1444\n",
      "Val Metrics - Acc: 0.5610, F1: 0.5436, Precision: 0.5333, Recall: 0.5610, AUPRC: 0.5776, AUROC: 0.6903\n",
      "Early stopping at epoch 29\n",
      "\n",
      "Fold 9 Test Metrics:\n",
      "Accuracy: 0.6098\n",
      "F1 Score: 0.5844\n",
      "Precision: 0.5864\n",
      "Recall: 0.6098\n",
      "AUPRC: 0.5785\n",
      "AUROC: 0.6953\n",
      "\n",
      "==================================================\n",
      "Fold 10/10\n",
      "==================================================\n",
      "Epoch 10/500 - Train Loss: 0.1251, Val Loss: 0.1531\n",
      "Val Metrics - Acc: 0.3902, F1: 0.3746, Precision: 0.3727, Recall: 0.3902, AUPRC: 0.4609, AUROC: 0.6156\n",
      "Epoch 20/500 - Train Loss: 0.1086, Val Loss: 0.1589\n",
      "Val Metrics - Acc: 0.4146, F1: 0.4021, Precision: 0.3965, Recall: 0.4146, AUPRC: 0.4677, AUROC: 0.5979\n",
      "Early stopping at epoch 30\n",
      "\n",
      "Fold 10 Test Metrics:\n",
      "Accuracy: 0.3902\n",
      "F1 Score: 0.3746\n",
      "Precision: 0.3727\n",
      "Recall: 0.3902\n",
      "AUPRC: 0.4609\n",
      "AUROC: 0.6156\n",
      "\n",
      "Saved best model with val loss 0.1033 as cirrhosis_mlp.pth\n",
      "Saved corresponding confusion matrix as best_fold_confusion_matrix.png\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import (accuracy_score, f1_score, precision_score,\n",
    "                             recall_score, average_precision_score,\n",
    "                             precision_recall_curve, auc, roc_auc_score, confusion_matrix)\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import interpolate\n",
    "import joblib  # 用于保存scaler\n",
    "\n",
    "\n",
    "# TeLU激活函数\n",
    "class TeLU(nn.Module):\n",
    "    def __init__(self, alpha=0.15):\n",
    "        super(TeLU, self).__init__()\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.where(x >= 0, x, self.alpha * (torch.exp(x) - 1))\n",
    "\n",
    "\n",
    "# 前馈神经网络\n",
    "class FFNN(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(FFNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 32)\n",
    "        self.telu1 = TeLU(alpha=0.15)\n",
    "        self.fc2 = nn.Linear(32, 64)\n",
    "        self.telu2 = TeLU(alpha=0.1)\n",
    "        self.fc3 = nn.Linear(64, 4)  # 4个输出类别\n",
    "\n",
    "        # 初始化权重\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.telu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.telu2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# 焦点损失函数\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        BCE_loss = nn.CrossEntropyLoss(reduction='none')(inputs, targets)\n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        F_loss = self.alpha * (1 - pt) ** self.gamma * BCE_loss\n",
    "        return F_loss.mean()\n",
    "\n",
    "\n",
    "def calculate_metrics(y_true, y_pred, y_scores):\n",
    "    # 检查NaN值\n",
    "    if np.isnan(y_scores).any():\n",
    "        y_scores = np.nan_to_num(y_scores)\n",
    "\n",
    "    # 初始化存储每个类别的指标\n",
    "    class_metrics = []\n",
    "\n",
    "    for class_idx in range(4):  # 4个类别\n",
    "        # 确保y_true是numpy数组\n",
    "        y_true_np = np.array(y_true)\n",
    "        y_pred_np = np.array(y_pred)\n",
    "\n",
    "        # 二分类指标计算\n",
    "        y_true_class = (y_true_np == class_idx).astype(int)\n",
    "        y_pred_class = (y_pred_np == class_idx).astype(int)\n",
    "        y_scores_class = y_scores[:, class_idx]\n",
    "\n",
    "        try:\n",
    "            accuracy = accuracy_score(y_true_class, y_pred_class)\n",
    "            f1 = f1_score(y_true_class, y_pred_class, zero_division=0)\n",
    "            precision = precision_score(y_true_class, y_pred_class, zero_division=0)\n",
    "            recall = recall_score(y_true_class, y_pred_class, zero_division=0)\n",
    "\n",
    "            # 处理AUPRC计算\n",
    "            if len(np.unique(y_true_class)) > 1:\n",
    "                auprc = average_precision_score(y_true_class, y_scores_class)\n",
    "            else:\n",
    "                auprc = 0.0\n",
    "\n",
    "            # 处理AUROC计算\n",
    "            if len(np.unique(y_true_class)) > 1:\n",
    "                auroc = roc_auc_score(y_true_class, y_scores_class)\n",
    "            else:\n",
    "                auroc = 0.0\n",
    "\n",
    "            class_metrics.append({\n",
    "                'accuracy': accuracy,\n",
    "                'f1': f1,\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'auprc': auprc,\n",
    "                'auroc': auroc\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"计算类别{class_idx}指标时出错: {str(e)}\")\n",
    "            class_metrics.append({\n",
    "                'accuracy': 0,\n",
    "                'f1': 0,\n",
    "                'precision': 0,\n",
    "                'recall': 0,\n",
    "                'auprc': 0,\n",
    "                'auroc': 0\n",
    "            })\n",
    "\n",
    "    # 计算加权平均指标\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_true, y_pred),\n",
    "        'f1': f1_score(y_true, y_pred, average='weighted', zero_division=0),\n",
    "        'precision': precision_score(y_true, y_pred, average='weighted', zero_division=0),\n",
    "        'recall': recall_score(y_true, y_pred, average='weighted', zero_division=0),\n",
    "        'auprc': average_precision_score(y_true, y_scores, average='weighted'),\n",
    "        'auroc': roc_auc_score(y_true, y_scores, multi_class='ovr', average='weighted'),\n",
    "        'class_metrics': class_metrics\n",
    "    }\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, dpi=720):\n",
    "    \"\"\"绘制正方形混淆矩阵\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    cm_percentage = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100  # 百分比表示\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    ax = sns.heatmap(cm_percentage, annot=False, fmt='.2f', cmap='Blues', square=True, cbar=False,\n",
    "                     linewidths=2, linecolor='black')\n",
    "\n",
    "    # 在每个格子中显示个数和百分比\n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            # 判断字体颜色，深色背景用白色字体，浅色背景用黑色字体\n",
    "            text_color = 'white' if cm_percentage[i, j] > 50 else 'black'\n",
    "\n",
    "            # 将个数和百分百分比显示\n",
    "            ax.text(j + 0.5, i + 0.5, f'{cm[i, j]}\\n({cm_percentage[i, j]:.2f}%)',\n",
    "                    color=text_color, ha='center', va='center', fontsize=14, fontweight='bold')\n",
    "\n",
    "    # 添加中文标签\n",
    "    plt.xlabel('预测类别', fontsize=16, fontweight='bold')\n",
    "    plt.ylabel('实际类别', fontsize=16, fontweight='bold')\n",
    "    plt.xticks(ticks=np.arange(4) + 0.5, labels=np.arange(1, 5), fontsize=14, fontweight='bold')\n",
    "    plt.yticks(ticks=np.arange(4) + 0.5, labels=np.arange(1, 5), fontsize=14, fontweight='bold')\n",
    "\n",
    "    # 调整布局\n",
    "    plt.subplots_adjust(left=0.1, right=0.9, top=0.9, bottom=0.1)\n",
    "\n",
    "    # 保存最佳折叠混淆矩阵图\n",
    "    plt.savefig(f'best_fold_confusion_matrix.png', dpi=dpi)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs=500, patience=20):\n",
    "    best_val_loss = float('inf')\n",
    "    best_model = None\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "\n",
    "            # 梯度裁剪防止爆炸\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        # 验证\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        all_scores = []\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "                # 使用softmax获取概率\n",
    "                scores = torch.softmax(outputs, dim=1)\n",
    "                _, predicted = torch.max(scores.data, 1)\n",
    "\n",
    "                all_preds.extend(predicted.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                all_scores.extend(scores.cpu().numpy())\n",
    "\n",
    "        # 检查NaN值\n",
    "        if np.isnan(np.array(all_scores)).any():\n",
    "            all_scores = np.nan_to_num(all_scores)\n",
    "\n",
    "        # 计算指标\n",
    "        train_loss = train_loss / len(train_loader.dataset)\n",
    "        val_loss = val_loss / len(val_loader.dataset)\n",
    "        metrics = calculate_metrics(all_labels, all_preds, np.array(all_scores))\n",
    "\n",
    "        # 早停\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model = copy.deepcopy(model.state_dict())\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f'Early stopping at epoch {epoch + 1}')\n",
    "                break\n",
    "\n",
    "        # 打印进度\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch {epoch + 1}/{epochs} - Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "            print(f\"Val Metrics - Acc: {metrics['accuracy']:.4f}, F1: {metrics['f1']:.4f}, \"\n",
    "                  f\"Precision: {metrics['precision']:.4f}, Recall: {metrics['recall']:.4f}, \"\n",
    "                  f\"AUPRC: {metrics['auprc']:.4f}, AUROC: {metrics['auroc']:.4f}\")\n",
    "\n",
    "    # 加载最佳模型\n",
    "    model.load_state_dict(best_model)\n",
    "    return model, best_val_loss  # 返回最佳模型和对应的验证损失\n",
    "\n",
    "\n",
    "def train_test_split(X, y, splits=10, epochs=500, batch_size=32, lr=0.001):\n",
    "    # 检查数据\n",
    "    print(f\"数据形状: X={X.shape}, y={y.shape}\")\n",
    "    print(f\"类别分布: {np.bincount(y)}\")\n",
    "\n",
    "    # 处理可能的NaN值\n",
    "    X = np.nan_to_num(X)\n",
    "    y = np.nan_to_num(y).astype(int)\n",
    "\n",
    "    k_fold = StratifiedKFold(n_splits=splits, shuffle=True, random_state=2025)\n",
    "    results = []\n",
    "    best_model_info = {'val_loss': float('inf'), 'model_state': None}\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(k_fold.split(X, y)):\n",
    "        print(f'\\n{\"=\" * 50}')\n",
    "        print(f'Fold {fold + 1}/{splits}')\n",
    "        print(f'{\"=\" * 50}')\n",
    "\n",
    "        # 分割数据\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "        # 标准化\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "        # 转换为张量\n",
    "        X_train_tensor = torch.FloatTensor(X_train)\n",
    "        y_train_tensor = torch.LongTensor(y_train)\n",
    "        X_test_tensor = torch.FloatTensor(X_test)\n",
    "        y_test_tensor = torch.LongTensor(y_test)\n",
    "\n",
    "        # 创建数据集和数据加载器\n",
    "        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "        test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "        # 初始化模型、损失函数和优化器\n",
    "        model = FFNN(input_size=X.shape[1])\n",
    "        criterion = FocalLoss(alpha=0.25, gamma=2)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "        # 训练模型\n",
    "        model, val_loss = train_model(model, train_loader, test_loader, criterion, optimizer,\n",
    "                                      epochs=epochs, patience=20)\n",
    "\n",
    "        # 检查是否为最佳模型\n",
    "        if val_loss < best_model_info['val_loss']:\n",
    "            best_model_info['val_loss'] = val_loss\n",
    "            best_model_info['model_state'] = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        # 评估测试集\n",
    "        y_true, y_pred, y_scores = evaluate_model(model, test_loader)\n",
    "        metrics = calculate_metrics(y_true, y_pred, y_scores)\n",
    "\n",
    "        # 保存结果\n",
    "        results.append(metrics)\n",
    "\n",
    "        # 打印当前折的结果\n",
    "        print(f'\\nFold {fold + 1} Test Metrics:')\n",
    "        print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
    "        print(f\"F1 Score: {metrics['f1']:.4f}\")\n",
    "        print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "        print(f\"Recall: {metrics['recall']:.4f}\")\n",
    "        print(f\"AUPRC: {metrics['auprc']:.4f}\")\n",
    "        print(f\"AUROC: {metrics['auroc']:.4f}\")\n",
    "\n",
    "    # 保存最佳模型\n",
    "    best_model = FFNN(input_size=X.shape[1])\n",
    "    best_model.load_state_dict(best_model_info['model_state'])\n",
    "    torch.save(best_model.state_dict(), 'cirrhosis_mlp.pth')\n",
    "\n",
    "    # 保存最佳模型的混淆矩阵\n",
    "    y_true, y_pred, y_scores = evaluate_model(best_model, test_loader)\n",
    "    plot_confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    print(f\"\\nSaved best model with val loss {best_model_info['val_loss']:.4f} as cirrhosis_mlp.pth\")\n",
    "    print(\"Saved corresponding confusion matrix as best_fold_confusion_matrix.png\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 加载数据\n",
    "    data = pd.read_csv('preparations/cirrhosis_output.csv')  # 请替换为您的实际文件路径\n",
    "\n",
    "    # 检查数据\n",
    "    print(\"数据前5行:\")\n",
    "    print(data.head())\n",
    "    print(\"\\n类别分布:\")\n",
    "    print(data['Stage'].value_counts())\n",
    "\n",
    "    # 分离特征和目标\n",
    "    feature_cols = ['N_Days', 'Age', 'Bilirubin', 'Albumin', 'Copper', 'SGOT',\n",
    "                    'Tryglicerides', 'Platelets', 'Prothrombin']\n",
    "    X = data[feature_cols].values\n",
    "    y = data['Stage'].values - 1  # 将类别转换为0-3\n",
    "\n",
    "    # 转换为numpy数组\n",
    "    X = X.astype(np.float32)\n",
    "    y = y.astype(np.int64)\n",
    "\n",
    "    # 运行训练和评估\n",
    "    train_test_split(X, y, splits=10, epochs=500, batch_size=32, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cb99ed5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据前5行:\n",
      "   N_Days  Status  Age  Ascites  Hepatomegaly  Spiders  Edema  Bilirubin  \\\n",
      "0     400       2   59      2.0           2.0      2.0      2       14.5   \n",
      "1    4500       0   56      0.0           2.0      2.0      0        1.1   \n",
      "2    1012       2   70      0.0           0.0      0.0      1        1.4   \n",
      "3    1925       2   55      0.0           2.0      2.0      1        1.8   \n",
      "4    1504       1   38      0.0           2.0      2.0      0        3.4   \n",
      "\n",
      "   Albumin  Copper    SGOT  Tryglicerides  Platelets  Prothrombin  Stage  \n",
      "0     2.60   156.0  137.95          172.0      190.0         12.2    4.0  \n",
      "1     4.14    54.0  113.52           88.0      221.0         10.6    3.0  \n",
      "2     3.48   210.0   96.10           55.0      151.0         12.0    4.0  \n",
      "3     2.54    64.0   60.63           92.0      183.0         10.3    4.0  \n",
      "4     3.53   143.0  113.15           72.0      136.0         10.9    3.0  \n",
      "\n",
      "类别分布:\n",
      "Stage\n",
      "3.0    161\n",
      "4.0    144\n",
      "2.0     92\n",
      "1.0     21\n",
      "Name: count, dtype: int64\n",
      "数据形状: X=(418, 9), y=(418,)\n",
      "类别分布: [ 21  92 161 144]\n",
      "\n",
      "==================================================\n",
      "Fold 1/10\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "Fold 2/10\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "Fold 3/10\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "Fold 4/10\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "Fold 5/10\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "Fold 6/10\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "Fold 7/10\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "Fold 8/10\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "Fold 9/10\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "Fold 10/10\n",
      "==================================================\n",
      "\n",
      "Saved best model from fold 1 with AUROC 0.7549 as best_cirrhosis_logreg.pkl\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import (accuracy_score, f1_score, precision_score,\n",
    "                             recall_score, average_precision_score,\n",
    "                             precision_recall_curve, auc, roc_auc_score, confusion_matrix)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "def calculate_metrics(y_true, y_pred, y_scores):\n",
    "    # 检查NaN值\n",
    "    if np.isnan(y_scores).any():\n",
    "        y_scores = np.nan_to_num(y_scores)\n",
    "\n",
    "    # 计算加权平均指标\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_true, y_pred),\n",
    "        'f1': f1_score(y_true, y_pred, average='weighted', zero_division=0),\n",
    "        'precision': precision_score(y_true, y_pred, average='weighted', zero_division=0),\n",
    "        'recall': recall_score(y_true, y_pred, average='weighted', zero_division=0),\n",
    "        'auprc': average_precision_score(y_true, y_scores, average='weighted'),\n",
    "        'auroc': roc_auc_score(y_true, y_scores, multi_class='ovr', average='weighted'),\n",
    "    }\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, fold, dpi=720):\n",
    "    \"\"\"绘制正方形混淆矩阵\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    cm_percentage = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100  # 百分比表示\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    ax = sns.heatmap(cm_percentage, annot=False, fmt='.2f', cmap='Blues', square=True, cbar=False,\n",
    "                     linewidths=2, linecolor='black')\n",
    "\n",
    "    # 在每个格子中显示个数和百分比\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            # 计算背景颜色的亮度（灰度）\n",
    "            background_brightness = cm_percentage[i, j] / 100\n",
    "\n",
    "            # 根据亮度选择字体颜色：浅色背景使用黑色字体，深色背景使用白色字体\n",
    "            text_color = 'black' if background_brightness < 0.5 else 'white'\n",
    "\n",
    "            # 将个数和百分百分比分行显示，个数在上，百分比在下\n",
    "            ax.text(j + 0.5, i + 0.5, f'{cm[i, j]}\\n({cm_percentage[i, j]:.2f}%)',\n",
    "                    color=text_color, ha='center', va='center', fontsize=14, fontweight='bold')\n",
    "\n",
    "    # 添加中文标签\n",
    "    plt.xlabel('预测类别', fontsize=16, fontweight='bold')\n",
    "    plt.ylabel('实际类别', fontsize=16, fontweight='bold')\n",
    "    plt.xticks(ticks=np.arange(cm.shape[1]) + 0.5, labels=np.arange(1, cm.shape[1] + 1), fontsize=14, fontweight='bold')\n",
    "    plt.yticks(ticks=np.arange(cm.shape[0]) + 0.5, labels=np.arange(1, cm.shape[0] + 1), fontsize=14, fontweight='bold')\n",
    "\n",
    "    # 调整布局，减少空白边缘\n",
    "    plt.subplots_adjust(left=0.1, right=0.9, top=0.9, bottom=0.1)\n",
    "\n",
    "    # 保存混淆矩阵图\n",
    "    plt.savefig(f'CI_LR_best_fold_confusion_matrix_fold{fold}.png', dpi=dpi)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def train_test_split(X, y, splits=10):\n",
    "    # 检查数据\n",
    "    print(f\"数据形状: X={X.shape}, y={y.shape}\")\n",
    "    print(f\"类别分布: {np.bincount(y)}\")\n",
    "\n",
    "    # 处理可能的NaN值\n",
    "    X = np.nan_to_num(X)\n",
    "    y = np.nan_to_num(y).astype(int)\n",
    "\n",
    "    best_model_info = {'val_score': -float('inf'), 'model': None, 'fold': -1}\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(StratifiedKFold(n_splits=splits, shuffle=True, random_state=2025).split(X, y)):\n",
    "        print(f'\\n{\"=\" * 50}')\n",
    "        print(f'Fold {fold + 1}/{splits}')\n",
    "        print(f'{\"=\" * 50}')\n",
    "\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "        # 创建并训练逻辑回归模型\n",
    "        model = LogisticRegression(\n",
    "            multi_class='multinomial',\n",
    "            solver='lbfgs',\n",
    "            max_iter=1000,\n",
    "            class_weight='balanced',  # 处理类别不平衡\n",
    "            random_state=2025\n",
    "        )\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # 预测\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_scores = model.predict_proba(X_test)\n",
    "\n",
    "        # 计算指标\n",
    "        metrics = calculate_metrics(y_test, y_pred, y_scores)\n",
    "\n",
    "        current_score = metrics['auroc']\n",
    "        if current_score > best_model_info['val_score']:\n",
    "            best_model_info['val_score'] = current_score\n",
    "            best_model_info['model'] = model\n",
    "            best_model_info['fold'] = fold + 1\n",
    "            best_model_info['scaler'] = scaler\n",
    "\n",
    "    # 保存最佳模型\n",
    "    joblib.dump({\n",
    "        'model': best_model_info['model'],\n",
    "        'scaler': best_model_info['scaler']\n",
    "    }, 'best_cirrhosis_logreg.pkl')\n",
    "\n",
    "    print(f\"\\nSaved best model from fold {best_model_info['fold']} with AUROC {best_model_info['val_score']:.4f} as best_cirrhosis_logreg.pkl\")\n",
    "\n",
    "    # 绘制并保存最佳模型的混淆矩阵\n",
    "    plot_confusion_matrix(y_test, best_model_info['model'].predict(X_test), best_model_info['fold'])\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data = pd.read_csv('preparations/cirrhosis_output.csv')\n",
    "    print(\"数据前5行:\")\n",
    "    print(data.head())\n",
    "    print(\"\\n类别分布:\")\n",
    "    print(data['Stage'].value_counts())\n",
    "\n",
    "    feature_cols = ['N_Days', 'Age', 'Bilirubin', 'Albumin', 'Copper', 'SGOT',\n",
    "                    'Tryglicerides', 'Platelets', 'Prothrombin']\n",
    "    X = data[feature_cols].values\n",
    "    y = data['Stage'].values - 1\n",
    "\n",
    "    X = X.astype(np.float32)\n",
    "    y = y.astype(np.int64)\n",
    "\n",
    "    train_test_split(X, y, splits=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3553bdf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据前5行:\n",
      "   Age  Sex  ChestPainType  RestingBP  Cholesterol  FastingBS  RestingECG  \\\n",
      "0   40    0              2        140          289          0           0   \n",
      "1   49    1              1        160          180          0           0   \n",
      "2   37    0              2        130          283          0           1   \n",
      "3   48    1              0        138          214          0           0   \n",
      "4   54    0              1        150          195          0           0   \n",
      "\n",
      "   MaxHR  ExerciseAngina  Oldpeak  ST_Slope  HeartDisease  \n",
      "0    172               0      0.0         0             0  \n",
      "1    156               0      1.0         1             1  \n",
      "2     98               0      0.0         0             0  \n",
      "3    108               1      1.5         1             1  \n",
      "4    122               0      0.0         0             0  \n",
      "\n",
      "类别分布:\n",
      "HeartDisease\n",
      "1    508\n",
      "0    410\n",
      "Name: count, dtype: int64\n",
      "数据形状: X=(918, 11), y=(918,)\n",
      "类别分布: [410 508]\n",
      "\n",
      "==================================================\n",
      "Fold 1/10\n",
      "==================================================\n",
      "[50]\tvalid_0's binary_logloss: 0.343711\n",
      "[100]\tvalid_0's binary_logloss: 0.340025\n",
      "\n",
      "==================================================\n",
      "Fold 2/10\n",
      "==================================================\n",
      "[50]\tvalid_0's binary_logloss: 0.319431\n",
      "[100]\tvalid_0's binary_logloss: 0.312929\n",
      "\n",
      "==================================================\n",
      "Fold 3/10\n",
      "==================================================\n",
      "[50]\tvalid_0's binary_logloss: 0.361466\n",
      "[100]\tvalid_0's binary_logloss: 0.34217\n",
      "\n",
      "==================================================\n",
      "Fold 4/10\n",
      "==================================================\n",
      "[50]\tvalid_0's binary_logloss: 0.301252\n",
      "[100]\tvalid_0's binary_logloss: 0.272622\n",
      "[150]\tvalid_0's binary_logloss: 0.257668\n",
      "\n",
      "==================================================\n",
      "Fold 5/10\n",
      "==================================================\n",
      "[50]\tvalid_0's binary_logloss: 0.316931\n",
      "[100]\tvalid_0's binary_logloss: 0.317576\n",
      "\n",
      "==================================================\n",
      "Fold 6/10\n",
      "==================================================\n",
      "[50]\tvalid_0's binary_logloss: 0.44734\n",
      "[100]\tvalid_0's binary_logloss: 0.445608\n",
      "\n",
      "==================================================\n",
      "Fold 7/10\n",
      "==================================================\n",
      "[50]\tvalid_0's binary_logloss: 0.320867\n",
      "[100]\tvalid_0's binary_logloss: 0.331426\n",
      "\n",
      "==================================================\n",
      "Fold 8/10\n",
      "==================================================\n",
      "[50]\tvalid_0's binary_logloss: 0.342652\n",
      "[100]\tvalid_0's binary_logloss: 0.340716\n",
      "\n",
      "==================================================\n",
      "Fold 9/10\n",
      "==================================================\n",
      "[50]\tvalid_0's binary_logloss: 0.365949\n",
      "[100]\tvalid_0's binary_logloss: 0.3728\n",
      "\n",
      "==================================================\n",
      "Fold 10/10\n",
      "==================================================\n",
      "[50]\tvalid_0's binary_logloss: 0.276007\n",
      "[100]\tvalid_0's binary_logloss: 0.260434\n",
      "\n",
      "Saved best model from fold 4 with AUPRC 0.9723 as best_heart_disease_model.pkl\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import (accuracy_score, f1_score, precision_score,\n",
    "                             recall_score, average_precision_score,\n",
    "                             precision_recall_curve, auc, confusion_matrix)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def calculate_metrics(y_true, y_pred, y_scores):\n",
    "    # Check for NaN values\n",
    "    if np.isnan(y_scores).any():\n",
    "        y_scores = np.nan_to_num(y_scores)\n",
    "\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_true, y_pred),\n",
    "        'f1': f1_score(y_true, y_pred, zero_division=0),\n",
    "        'precision': precision_score(y_true, y_pred, zero_division=0),\n",
    "        'recall': recall_score(y_true, y_pred, zero_division=0),\n",
    "    }\n",
    "\n",
    "    # Calculate AUPRC\n",
    "    try:\n",
    "        metrics['auprc'] = average_precision_score(y_true, y_scores)\n",
    "    except:\n",
    "        print(\"无法计算AUPRC，使用默认值0\")\n",
    "        metrics['auprc'] = 0\n",
    "\n",
    "    return metrics\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred):\n",
    "    \"\"\"绘制并保存混淆矩阵\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    cm_percentage = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100  # 百分比表示\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    ax = sns.heatmap(cm_percentage, annot=False, fmt='.2f', cmap='Blues', square=True, cbar=False,\n",
    "                     linewidths=2, linecolor='black')\n",
    "\n",
    "    # 在每个格子中显示个数和百分比\n",
    "    for i in range(2):  # Assuming binary classification\n",
    "        for j in range(2):\n",
    "            text_color = 'white' if cm_percentage[i, j] > 50 else 'black'\n",
    "            ax.text(j + 0.5, i + 0.5, f'{cm[i, j]}\\n({cm_percentage[i, j]:.2f}%)',\n",
    "                    color=text_color, ha='center', va='center', fontsize=14, fontweight='bold')\n",
    "\n",
    "    # 添加中文标签\n",
    "    plt.xlabel('预测类别', fontsize=16, fontweight='bold')\n",
    "    plt.ylabel('实际类别', fontsize=16, fontweight='bold')\n",
    "    plt.xticks(ticks=np.arange(2) + 0.5, labels=np.arange(1, 3), fontsize=14, fontweight='bold')\n",
    "    plt.yticks(ticks=np.arange(2) + 0.5, labels=np.arange(1, 3), fontsize=14, fontweight='bold')\n",
    "\n",
    "    # 调整布局\n",
    "    plt.subplots_adjust(left=0.1, right=0.9, top=0.9, bottom=0.1)\n",
    "\n",
    "    # 保存混淆矩阵\n",
    "    plt.savefig(f'best_confusion_matrix.png', dpi=720)\n",
    "    plt.close()\n",
    "\n",
    "def train_test_split(X, y, splits=10, batch_size=32):\n",
    "    # Check data\n",
    "    print(f\"数据形状: X={X.shape}, y={y.shape}\")\n",
    "    print(f\"类别分布: {np.bincount(y)}\")\n",
    "\n",
    "    # Handle possible NaN values\n",
    "    X = np.nan_to_num(X)\n",
    "    y = np.nan_to_num(y).astype(int)\n",
    "\n",
    "    k_fold = StratifiedKFold(n_splits=splits, shuffle=True, random_state=2025)\n",
    "    results = []\n",
    "    \n",
    "    best_model_info = {'val_score': -float('inf'), 'model': None, 'fold': -1}\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(k_fold.split(X, y)):\n",
    "        print(f'\\n{\"=\" * 50}')\n",
    "        print(f'Fold {fold + 1}/{splits}')\n",
    "        print(f'{\"=\" * 50}')\n",
    "\n",
    "        # Split data\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "        # Standardization\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "        # Create LightGBM datasets\n",
    "        train_data = lgb.Dataset(X_train, label=y_train)\n",
    "        test_data = lgb.Dataset(X_test, label=y_test, reference=train_data)\n",
    "\n",
    "        # LightGBM parameters\n",
    "        params = {\n",
    "            'objective': 'binary',\n",
    "            'metric': 'binary_logloss',\n",
    "            'boosting_type': 'gbdt',\n",
    "            'learning_rate': 0.05,\n",
    "            'num_leaves': 31,\n",
    "            'max_depth': -1,\n",
    "            'min_child_samples': 20,\n",
    "            'feature_fraction': 0.8,\n",
    "            'bagging_fraction': 0.8,\n",
    "            'bagging_freq': 5,\n",
    "            'lambda_l1': 0.1,\n",
    "            'lambda_l2': 0.1,\n",
    "            'verbose': -1,\n",
    "            'random_state': 42\n",
    "        }\n",
    "\n",
    "        # Train model\n",
    "        model = lgb.train(\n",
    "            params,\n",
    "            train_data,\n",
    "            num_boost_round=1000,\n",
    "            valid_sets=[test_data],\n",
    "            callbacks=[lgb.early_stopping(stopping_rounds=50, verbose=False),\n",
    "                       lgb.log_evaluation(period=50)]\n",
    "        )\n",
    "\n",
    "        # Evaluate on test set\n",
    "        y_scores = model.predict(X_test)\n",
    "        y_pred = (y_scores > 0.5).astype(int)\n",
    "        metrics = calculate_metrics(y_test, y_pred, y_scores)\n",
    "\n",
    "        current_score = metrics['auprc']\n",
    "        if current_score > best_model_info['val_score']:\n",
    "            best_model_info['val_score'] = current_score\n",
    "            best_model_info['model'] = model\n",
    "            best_model_info['fold'] = fold + 1\n",
    "            best_model_info['scaler'] = scaler\n",
    "\n",
    "    # Save best model\n",
    "    joblib.dump({\n",
    "        'model': best_model_info['model'],\n",
    "        'scaler': best_model_info['scaler']\n",
    "    }, 'best_heart_disease_model.pkl')\n",
    "\n",
    "    print(f\"\\nSaved best model from fold {best_model_info['fold']} with AUPRC {best_model_info['val_score']:.4f} as best_heart_disease_model.pkl\")\n",
    "\n",
    "    # Plot and save the confusion matrix for the best model\n",
    "    best_y_pred = (best_model_info['model'].predict(X_test) > 0.5).astype(int)\n",
    "    plot_confusion_matrix(y_test, best_y_pred)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data = pd.read_csv('preparations/heart_output.csv')  # Replace with your actual file path\n",
    "\n",
    "    # Check data\n",
    "    print(\"数据前5行:\")\n",
    "    print(data.head())\n",
    "    print(\"\\n类别分布:\")\n",
    "    print(data['HeartDisease'].value_counts())\n",
    "\n",
    "    # Separate features and target\n",
    "    X = data.drop('HeartDisease', axis=1).values\n",
    "    y = data['HeartDisease'].values\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    X = X.astype(np.float32)\n",
    "    y = y.astype(np.int64)\n",
    "\n",
    "    # Run training and evaluation\n",
    "    train_test_split(X, y, splits=10, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bdcd7029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据前5行:\n",
      "   Age  Sex  ChestPainType  RestingBP  Cholesterol  FastingBS  RestingECG  \\\n",
      "0   40    0              2        140          289          0           0   \n",
      "1   49    1              1        160          180          0           0   \n",
      "2   37    0              2        130          283          0           1   \n",
      "3   48    1              0        138          214          0           0   \n",
      "4   54    0              1        150          195          0           0   \n",
      "\n",
      "   MaxHR  ExerciseAngina  Oldpeak  ST_Slope  HeartDisease  \n",
      "0    172               0      0.0         0             0  \n",
      "1    156               0      1.0         1             1  \n",
      "2     98               0      0.0         0             0  \n",
      "3    108               1      1.5         1             1  \n",
      "4    122               0      0.0         0             0  \n",
      "\n",
      "类别分布:\n",
      "HeartDisease\n",
      "1    508\n",
      "0    410\n",
      "Name: count, dtype: int64\n",
      "数据形状: X=(918, 11), y=(918,)\n",
      "类别分布: [410 508]\n",
      "\n",
      "==================================================\n",
      "Fold 1/10\n",
      "==================================================\n",
      "Epoch 10/500 - Train Loss: 0.0193, Val Loss: 0.0258\n",
      "Val Metrics - Acc: 0.8152, F1: 0.8411, Precision: 0.8036, Recall: 0.8824, AUPRC: 0.9076\n",
      "Epoch 20/500 - Train Loss: 0.0154, Val Loss: 0.0260\n",
      "Val Metrics - Acc: 0.8478, F1: 0.8679, Precision: 0.8364, Recall: 0.9020, AUPRC: 0.8862\n",
      "Epoch 30/500 - Train Loss: 0.0130, Val Loss: 0.0283\n",
      "Val Metrics - Acc: 0.8478, F1: 0.8627, Precision: 0.8627, Recall: 0.8627, AUPRC: 0.8828\n",
      "Early stopping at epoch 39\n",
      "\n",
      "==================================================\n",
      "Fold 2/10\n",
      "==================================================\n",
      "Epoch 10/500 - Train Loss: 0.0197, Val Loss: 0.0210\n",
      "Val Metrics - Acc: 0.8478, F1: 0.8600, Precision: 0.8776, Recall: 0.8431, AUPRC: 0.9468\n",
      "Epoch 20/500 - Train Loss: 0.0153, Val Loss: 0.0226\n",
      "Val Metrics - Acc: 0.8696, F1: 0.8800, Precision: 0.8980, Recall: 0.8627, AUPRC: 0.9419\n",
      "Early stopping at epoch 26\n",
      "\n",
      "==================================================\n",
      "Fold 3/10\n",
      "==================================================\n",
      "Epoch 10/500 - Train Loss: 0.0184, Val Loss: 0.0230\n",
      "Val Metrics - Acc: 0.8587, F1: 0.8738, Precision: 0.8654, Recall: 0.8824, AUPRC: 0.9234\n",
      "Epoch 20/500 - Train Loss: 0.0151, Val Loss: 0.0233\n",
      "Val Metrics - Acc: 0.8696, F1: 0.8846, Precision: 0.8679, Recall: 0.9020, AUPRC: 0.9166\n",
      "Epoch 30/500 - Train Loss: 0.0124, Val Loss: 0.0257\n",
      "Val Metrics - Acc: 0.8804, F1: 0.8952, Precision: 0.8704, Recall: 0.9216, AUPRC: 0.9124\n",
      "Early stopping at epoch 33\n",
      "\n",
      "==================================================\n",
      "Fold 4/10\n",
      "==================================================\n",
      "Epoch 10/500 - Train Loss: 0.0201, Val Loss: 0.0238\n",
      "Val Metrics - Acc: 0.8587, F1: 0.8713, Precision: 0.8800, Recall: 0.8627, AUPRC: 0.9399\n",
      "Epoch 20/500 - Train Loss: 0.0165, Val Loss: 0.0203\n",
      "Val Metrics - Acc: 0.8478, F1: 0.8627, Precision: 0.8627, Recall: 0.8627, AUPRC: 0.9561\n",
      "Epoch 30/500 - Train Loss: 0.0137, Val Loss: 0.0196\n",
      "Val Metrics - Acc: 0.8913, F1: 0.9000, Precision: 0.9184, Recall: 0.8824, AUPRC: 0.9579\n",
      "Epoch 40/500 - Train Loss: 0.0117, Val Loss: 0.0224\n",
      "Val Metrics - Acc: 0.8804, F1: 0.8889, Precision: 0.9167, Recall: 0.8627, AUPRC: 0.9582\n",
      "Early stopping at epoch 43\n",
      "\n",
      "==================================================\n",
      "Fold 5/10\n",
      "==================================================\n",
      "Epoch 10/500 - Train Loss: 0.0187, Val Loss: 0.0220\n",
      "Val Metrics - Acc: 0.8152, F1: 0.8317, Precision: 0.8400, Recall: 0.8235, AUPRC: 0.9389\n",
      "Epoch 20/500 - Train Loss: 0.0158, Val Loss: 0.0205\n",
      "Val Metrics - Acc: 0.8478, F1: 0.8679, Precision: 0.8364, Recall: 0.9020, AUPRC: 0.9511\n",
      "Epoch 30/500 - Train Loss: 0.0133, Val Loss: 0.0206\n",
      "Val Metrics - Acc: 0.8478, F1: 0.8679, Precision: 0.8364, Recall: 0.9020, AUPRC: 0.9516\n",
      "Early stopping at epoch 40\n",
      "\n",
      "==================================================\n",
      "Fold 6/10\n",
      "==================================================\n",
      "Epoch 10/500 - Train Loss: 0.0175, Val Loss: 0.0259\n",
      "Val Metrics - Acc: 0.8261, F1: 0.8400, Precision: 0.8571, Recall: 0.8235, AUPRC: 0.9271\n",
      "Epoch 20/500 - Train Loss: 0.0140, Val Loss: 0.0280\n",
      "Val Metrics - Acc: 0.8152, F1: 0.8317, Precision: 0.8400, Recall: 0.8235, AUPRC: 0.9289\n",
      "Early stopping at epoch 21\n",
      "\n",
      "==================================================\n",
      "Fold 7/10\n",
      "==================================================\n",
      "Epoch 10/500 - Train Loss: 0.0190, Val Loss: 0.0243\n",
      "Val Metrics - Acc: 0.8696, F1: 0.8868, Precision: 0.8545, Recall: 0.9216, AUPRC: 0.9059\n",
      "Epoch 20/500 - Train Loss: 0.0152, Val Loss: 0.0281\n",
      "Val Metrics - Acc: 0.8804, F1: 0.8932, Precision: 0.8846, Recall: 0.9020, AUPRC: 0.9036\n",
      "Early stopping at epoch 25\n",
      "\n",
      "==================================================\n",
      "Fold 8/10\n",
      "==================================================\n",
      "Epoch 10/500 - Train Loss: 0.0182, Val Loss: 0.0254\n",
      "Val Metrics - Acc: 0.8478, F1: 0.8704, Precision: 0.8246, Recall: 0.9216, AUPRC: 0.9173\n",
      "Epoch 20/500 - Train Loss: 0.0146, Val Loss: 0.0300\n",
      "Val Metrics - Acc: 0.8587, F1: 0.8807, Precision: 0.8276, Recall: 0.9412, AUPRC: 0.9017\n",
      "Early stopping at epoch 24\n",
      "\n",
      "==================================================\n",
      "Fold 9/10\n",
      "==================================================\n",
      "Epoch 10/500 - Train Loss: 0.0185, Val Loss: 0.0249\n",
      "Val Metrics - Acc: 0.8681, F1: 0.8824, Precision: 0.8654, Recall: 0.9000, AUPRC: 0.8835\n",
      "Epoch 20/500 - Train Loss: 0.0151, Val Loss: 0.0230\n",
      "Val Metrics - Acc: 0.8901, F1: 0.9057, Precision: 0.8571, Recall: 0.9600, AUPRC: 0.8906\n",
      "Epoch 30/500 - Train Loss: 0.0127, Val Loss: 0.0257\n",
      "Val Metrics - Acc: 0.8901, F1: 0.9057, Precision: 0.8571, Recall: 0.9600, AUPRC: 0.9026\n",
      "Early stopping at epoch 40\n",
      "\n",
      "==================================================\n",
      "Fold 10/10\n",
      "==================================================\n",
      "Epoch 10/500 - Train Loss: 0.0188, Val Loss: 0.0211\n",
      "Val Metrics - Acc: 0.9011, F1: 0.9091, Precision: 0.9184, Recall: 0.9000, AUPRC: 0.9424\n",
      "Epoch 20/500 - Train Loss: 0.0157, Val Loss: 0.0201\n",
      "Val Metrics - Acc: 0.9121, F1: 0.9200, Precision: 0.9200, Recall: 0.9200, AUPRC: 0.9472\n",
      "Epoch 30/500 - Train Loss: 0.0129, Val Loss: 0.0214\n",
      "Val Metrics - Acc: 0.9011, F1: 0.9091, Precision: 0.9184, Recall: 0.9000, AUPRC: 0.9478\n",
      "Epoch 40/500 - Train Loss: 0.0108, Val Loss: 0.0233\n",
      "Val Metrics - Acc: 0.8901, F1: 0.8980, Precision: 0.9167, Recall: 0.8800, AUPRC: 0.9552\n",
      "Early stopping at epoch 43\n",
      "\n",
      "==================================================\n",
      "Final Cross-Validation Results:\n",
      "==================================================\n",
      "Average Accuracy: 0.8606 ± 0.0244\n",
      "Average F1 Score: 0.8773 ± 0.0211\n",
      "Average Precision: 0.8569 ± 0.0290\n",
      "Average Recall: 0.8998 ± 0.0319\n",
      "Average AUPRC: 0.9294 ± 0.0222\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import (accuracy_score, f1_score, precision_score,\n",
    "                             recall_score, average_precision_score,\n",
    "                             precision_recall_curve, auc, confusion_matrix)\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import interpolate\n",
    "import warnings\n",
    "\n",
    "# 忽略警告\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "# 设置字体为黑体，确保中文可见\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# TeLU激活函数\n",
    "class TeLU(nn.Module):\n",
    "    def __init__(self, alpha=0.15):\n",
    "        super(TeLU, self).__init__()\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.where(x >= 0, x, self.alpha * (torch.exp(x) - 1))\n",
    "\n",
    "# 前馈神经网络\n",
    "class FFNN(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(FFNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 32)\n",
    "        self.telu1 = TeLU(alpha=0.15)\n",
    "        self.fc2 = nn.Linear(32, 64)\n",
    "        self.telu2 = TeLU(alpha=0.1)\n",
    "        self.fc3 = nn.Linear(64, 2)\n",
    "\n",
    "        # 初始化权重\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.telu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.telu2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# 焦点损失函数\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        BCE_loss = nn.CrossEntropyLoss(reduction='none')(inputs, targets)\n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        F_loss = self.alpha * (1 - pt) ** self.gamma * BCE_loss\n",
    "        return F_loss.mean()\n",
    "\n",
    "def calculate_metrics(y_true, y_pred, y_scores):\n",
    "    # 检查NaN值\n",
    "    if np.isnan(y_scores).any():\n",
    "        y_scores = np.nan_to_num(y_scores)\n",
    "\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_true, y_pred),\n",
    "        'f1': f1_score(y_true, y_pred, zero_division=0),\n",
    "        'precision': precision_score(y_true, y_pred, zero_division=0),\n",
    "        'recall': recall_score(y_true, y_pred, zero_division=0),\n",
    "    }\n",
    "\n",
    "    # 只有在y_scores有效时才计算AUPRC\n",
    "    try:\n",
    "        metrics['auprc'] = average_precision_score(y_true, y_scores[:, 1])\n",
    "    except:\n",
    "        print(\"无法计算AUPRC，使用默认值0\")\n",
    "        metrics['auprc'] = 0\n",
    "\n",
    "    return metrics\n",
    "\n",
    "def interpolate_pr_curve(precision, recall):\n",
    "    \"\"\"插值PR曲线到固定长度的点\"\"\"\n",
    "    f = interpolate.interp1d(recall, precision, bounds_error=False, fill_value=(1.0, 0.0))\n",
    "    new_recall = np.linspace(0, 1, 100)\n",
    "    new_precision = f(new_recall)\n",
    "    return new_precision, new_recall\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, dpi=720):\n",
    "    \"\"\"绘制最佳模型的混淆矩阵\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    cm_percentage = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100  # 百分比表示\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    ax = sns.heatmap(cm_percentage, annot=False, fmt='.2f', cmap='Blues', square=True, cbar=False,\n",
    "                     linewidths=2, linecolor='black')\n",
    "\n",
    "    # 在每个格子中显示个数和百分比\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            # 判断字体颜色，深色背景用白色字体，浅色背景用黑色字体\n",
    "            text_color = 'white' if cm_percentage[i, j] > 50 else 'black'\n",
    "\n",
    "            # 将个数和百分比分行显示，个数在上，百分比在下\n",
    "            ax.text(j + 0.5, i + 0.5, f'{cm[i, j]}\\n({cm_percentage[i, j]:.2f}%)',\n",
    "                    color=text_color, ha='center', va='center', fontsize=14, fontweight='bold')\n",
    "\n",
    "    # 添加中文标签\n",
    "    plt.xlabel('预测类别', fontsize=16, fontweight='bold')\n",
    "    plt.ylabel('实际类别', fontsize=16, fontweight='bold')\n",
    "    plt.xticks(ticks=np.arange(2) + 0.5, labels=np.arange(1, 3), fontsize=14, fontweight='bold')\n",
    "    plt.yticks(ticks=np.arange(2) + 0.5, labels=np.arange(1, 3), fontsize=14, fontweight='bold')\n",
    "\n",
    "    # 调整布局，减少空白边缘\n",
    "    plt.subplots_adjust(left=0.1, right=0.9, top=0.9, bottom=0.1)\n",
    "\n",
    "    # 保存最佳模型的混淆矩阵图\n",
    "    plt.savefig('best_model_confusion_matrix.png', dpi=dpi)\n",
    "    plt.close()\n",
    "\n",
    "def plot_pr_curve(y_true, y_scores, fold):\n",
    "    try:\n",
    "        precision, recall, _ = precision_recall_curve(y_true, y_scores[:, 1])\n",
    "        auprc = auc(recall, precision)\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(recall, precision, label=f'Fold {fold} (AUPRC = {auprc:.2f})')\n",
    "        plt.xlabel('Recall')\n",
    "        plt.ylabel('Precision')\n",
    "        plt.title('Precision-Recall Curve')\n",
    "        plt.legend()\n",
    "        plt.savefig(f'pr_curve_fold{fold}.png')\n",
    "        plt.close()\n",
    "        return precision, recall\n",
    "    except Exception as e:\n",
    "        print(f\"无法绘制Fold {fold}的PR曲线: {str(e)}\")\n",
    "        return None, None\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs=500, patience=20):\n",
    "    best_val_loss = float('inf')\n",
    "    best_model = None\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "\n",
    "            # 梯度裁剪防止爆炸\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        # 验证\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        all_scores = []\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "                # 使用softmax获取概率\n",
    "                scores = torch.softmax(outputs, dim=1)\n",
    "                _, predicted = torch.max(scores.data, 1)\n",
    "\n",
    "                all_preds.extend(predicted.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                all_scores.extend(scores.cpu().numpy())\n",
    "\n",
    "        # 检查NaN值\n",
    "        if np.isnan(np.array(all_scores)).any():\n",
    "            all_scores = np.nan_to_num(all_scores)\n",
    "\n",
    "        # 计算指标\n",
    "        train_loss = train_loss / len(train_loader.dataset)\n",
    "        val_loss = val_loss / len(val_loader.dataset)\n",
    "        metrics = calculate_metrics(all_labels, all_preds, np.array(all_scores))\n",
    "\n",
    "        # 早停\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model = copy.deepcopy(model.state_dict())\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f'Early stopping at epoch {epoch + 1}')\n",
    "                break\n",
    "\n",
    "        # 打印进度\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch {epoch + 1}/{epochs} - Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "            print(f\"Val Metrics - Acc: {metrics['accuracy']:.4f}, F1: {metrics['f1']:.4f}, \"\n",
    "                  f\"Precision: {metrics['precision']:.4f}, Recall: {metrics['recall']:.4f}, \"\n",
    "                  f\"AUPRC: {metrics['auprc']:.4f}\")\n",
    "\n",
    "    # 加载最佳模型\n",
    "    model.load_state_dict(best_model)\n",
    "    return model\n",
    "\n",
    "def train_test_split(X, y, splits=10, epochs=500, batch_size=32, lr=0.001):\n",
    "    # 检查数据\n",
    "    print(f\"数据形状: X={X.shape}, y={y.shape}\")\n",
    "    print(f\"类别分布: {np.bincount(y)}\")\n",
    "\n",
    "    # 处理可能的NaN值\n",
    "    X = np.nan_to_num(X)\n",
    "    y = np.nan_to_num(y).astype(int)\n",
    "\n",
    "    k_fold = StratifiedKFold(n_splits=splits, shuffle=True, random_state=2025)\n",
    "    results = []\n",
    "\n",
    "    # 存储所有折的PR曲线数据（插值后的）\n",
    "    interp_precisions = []\n",
    "    interp_recalls = np.linspace(0, 1, 100)  # 固定100个recall点\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(k_fold.split(X, y)):\n",
    "        print(f'\\n{\"=\" * 50}')\n",
    "        print(f'Fold {fold + 1}/{splits}')\n",
    "        print(f'{\"=\" * 50}')\n",
    "\n",
    "        # 分割数据\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "        # 标准化\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "        # 转换为张量\n",
    "        X_train_tensor = torch.FloatTensor(X_train)\n",
    "        y_train_tensor = torch.LongTensor(y_train)\n",
    "        X_test_tensor = torch.FloatTensor(X_test)\n",
    "        y_test_tensor = torch.LongTensor(y_test)\n",
    "\n",
    "        # 创建数据集和数据加载器\n",
    "        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "        test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "        # 初始化模型、损失函数和优化器\n",
    "        model = FFNN(input_size=X.shape[1])\n",
    "        criterion = FocalLoss(alpha=0.25, gamma=2)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "        # 训练模型\n",
    "        model = train_model(model, train_loader, test_loader, criterion, optimizer,\n",
    "                            epochs=epochs, patience=20)\n",
    "\n",
    "        # 评估测试集\n",
    "        y_true, y_pred, y_scores = evaluate_model(model, test_loader)\n",
    "        metrics = calculate_metrics(y_true, y_pred, y_scores)\n",
    "\n",
    "        # 只保存最佳模型的混淆矩阵\n",
    "        if fold == splits - 1:  # 最后一折的最佳模型\n",
    "            plot_confusion_matrix(y_true, y_pred)\n",
    "\n",
    "        # 绘制并保存当前折的PR曲线\n",
    "        precision, recall = plot_pr_curve(y_true, y_scores, fold + 1)\n",
    "\n",
    "        # 插值PR曲线到固定长度\n",
    "        if precision is not None and recall is not None:\n",
    "            interp_precision, _ = interpolate_pr_curve(precision, recall)\n",
    "            interp_precisions.append(interp_precision)\n",
    "\n",
    "        # 保存结果\n",
    "        results.append(metrics)\n",
    "\n",
    "    # 计算并打印平均指标\n",
    "    avg_metrics = {\n",
    "        'accuracy': np.mean([r['accuracy'] for r in results]),\n",
    "        'f1': np.mean([r['f1'] for r in results]),\n",
    "        'precision': np.mean([r['precision'] for r in results]),\n",
    "        'recall': np.mean([r['recall'] for r in results]),\n",
    "        'auprc': np.mean([r['auprc'] for r in results])\n",
    "    }\n",
    "\n",
    "    std_metrics = {\n",
    "        'accuracy': np.std([r['accuracy'] for r in results]),\n",
    "        'f1': np.std([r['f1'] for r in results]),\n",
    "        'precision': np.std([r['precision'] for r in results]),\n",
    "        'recall': np.std([r['recall'] for r in results]),\n",
    "        'auprc': np.std([r['auprc'] for r in results])\n",
    "    }\n",
    "\n",
    "    print('\\n' + '=' * 50)\n",
    "    print('Final Cross-Validation Results:')\n",
    "    print('=' * 50)\n",
    "    print(f\"Average Accuracy: {avg_metrics['accuracy']:.4f} ± {std_metrics['accuracy']:.4f}\")\n",
    "    print(f\"Average F1 Score: {avg_metrics['f1']:.4f} ± {std_metrics['f1']:.4f}\")\n",
    "    print(f\"Average Precision: {avg_metrics['precision']:.4f} ± {std_metrics['precision']:.4f}\")\n",
    "    print(f\"Average Recall: {avg_metrics['recall']:.4f} ± {std_metrics['recall']:.4f}\")\n",
    "    print(f\"Average AUPRC: {avg_metrics['auprc']:.4f} ± {std_metrics['auprc']:.4f}\")\n",
    "\n",
    "\n",
    "# 加载数据\n",
    "data = pd.read_csv('preparations/heart_output.csv')  # 请替换为您的实际文件路径\n",
    "\n",
    "# 检查数据\n",
    "print(\"数据前5行:\")\n",
    "print(data.head())\n",
    "print(\"\\n类别分布:\")\n",
    "print(data['HeartDisease'].value_counts())\n",
    "\n",
    "# 分离特征和目标\n",
    "X = data.drop('HeartDisease', axis=1).values\n",
    "y = data['HeartDisease'].values\n",
    "\n",
    "# 转换为numpy数组\n",
    "X = X.astype(np.float32)\n",
    "y = y.astype(np.int64)\n",
    "\n",
    "# 运行训练和评估\n",
    "train_test_split(X, y, splits=10, epochs=500, batch_size=32, lr=0.001)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a66b2678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据前5行:\n",
      "   Age  Sex  ChestPainType  RestingBP  Cholesterol  FastingBS  RestingECG  \\\n",
      "0   40    0              2        140          289          0           0   \n",
      "1   49    1              1        160          180          0           0   \n",
      "2   37    0              2        130          283          0           1   \n",
      "3   48    1              0        138          214          0           0   \n",
      "4   54    0              1        150          195          0           0   \n",
      "\n",
      "   MaxHR  ExerciseAngina  Oldpeak  ST_Slope  HeartDisease  \n",
      "0    172               0      0.0         0             0  \n",
      "1    156               0      1.0         1             1  \n",
      "2     98               0      0.0         0             0  \n",
      "3    108               1      1.5         1             1  \n",
      "4    122               0      0.0         0             0  \n",
      "\n",
      "类别分布:\n",
      "HeartDisease\n",
      "1    508\n",
      "0    410\n",
      "Name: count, dtype: int64\n",
      "\n",
      "训练模型（不带SMOTE）:\n",
      "数据形状: X=(918, 11), y=(918,)\n",
      "类别分布: [410 508]\n",
      "使用SMOTE: 否\n",
      "\n",
      "==================================================\n",
      "Fold 1/10\n",
      "==================================================\n",
      "Epoch 10/500 - Train Loss: 0.0198, Val Loss: 0.0221\n",
      "Val Metrics - Acc: 0.8370, F1: 0.8571, Precision: 0.8333, Recall: 0.8824, AUPRC: 0.9440\n",
      "Epoch 20/500 - Train Loss: 0.0160, Val Loss: 0.0210\n",
      "Val Metrics - Acc: 0.8478, F1: 0.8679, Precision: 0.8364, Recall: 0.9020, AUPRC: 0.9453\n",
      "Epoch 30/500 - Train Loss: 0.0136, Val Loss: 0.0212\n",
      "Val Metrics - Acc: 0.8478, F1: 0.8679, Precision: 0.8364, Recall: 0.9020, AUPRC: 0.9444\n",
      "Early stopping at epoch 39\n",
      "\n",
      "==================================================\n",
      "Fold 2/10\n",
      "==================================================\n",
      "Epoch 10/500 - Train Loss: 0.0199, Val Loss: 0.0209\n",
      "Val Metrics - Acc: 0.8913, F1: 0.9020, Precision: 0.9020, Recall: 0.9020, AUPRC: 0.9352\n",
      "Epoch 20/500 - Train Loss: 0.0157, Val Loss: 0.0228\n",
      "Val Metrics - Acc: 0.8913, F1: 0.9020, Precision: 0.9020, Recall: 0.9020, AUPRC: 0.9274\n",
      "Early stopping at epoch 28\n",
      "\n",
      "==================================================\n",
      "Fold 3/10\n",
      "==================================================\n",
      "Epoch 10/500 - Train Loss: 0.0190, Val Loss: 0.0248\n",
      "Val Metrics - Acc: 0.8478, F1: 0.8654, Precision: 0.8491, Recall: 0.8824, AUPRC: 0.8971\n",
      "Epoch 20/500 - Train Loss: 0.0150, Val Loss: 0.0243\n",
      "Val Metrics - Acc: 0.8370, F1: 0.8544, Precision: 0.8462, Recall: 0.8627, AUPRC: 0.8956\n",
      "Epoch 30/500 - Train Loss: 0.0117, Val Loss: 0.0262\n",
      "Val Metrics - Acc: 0.8478, F1: 0.8654, Precision: 0.8491, Recall: 0.8824, AUPRC: 0.8935\n",
      "Early stopping at epoch 38\n",
      "\n",
      "==================================================\n",
      "Fold 4/10\n",
      "==================================================\n",
      "Epoch 10/500 - Train Loss: 0.0196, Val Loss: 0.0247\n",
      "Val Metrics - Acc: 0.8478, F1: 0.8542, Precision: 0.9111, Recall: 0.8039, AUPRC: 0.9409\n",
      "Epoch 20/500 - Train Loss: 0.0157, Val Loss: 0.0214\n",
      "Val Metrics - Acc: 0.8913, F1: 0.9000, Precision: 0.9184, Recall: 0.8824, AUPRC: 0.9527\n",
      "Epoch 30/500 - Train Loss: 0.0131, Val Loss: 0.0225\n",
      "Val Metrics - Acc: 0.8478, F1: 0.8600, Precision: 0.8776, Recall: 0.8431, AUPRC: 0.9507\n",
      "Early stopping at epoch 40\n",
      "\n",
      "==================================================\n",
      "Fold 5/10\n",
      "==================================================\n",
      "Epoch 10/500 - Train Loss: 0.0203, Val Loss: 0.0215\n",
      "Val Metrics - Acc: 0.8696, F1: 0.8800, Precision: 0.8980, Recall: 0.8627, AUPRC: 0.9398\n",
      "Epoch 20/500 - Train Loss: 0.0165, Val Loss: 0.0197\n",
      "Val Metrics - Acc: 0.8696, F1: 0.8846, Precision: 0.8679, Recall: 0.9020, AUPRC: 0.9525\n",
      "Epoch 30/500 - Train Loss: 0.0138, Val Loss: 0.0195\n",
      "Val Metrics - Acc: 0.8587, F1: 0.8738, Precision: 0.8654, Recall: 0.8824, AUPRC: 0.9573\n",
      "Epoch 40/500 - Train Loss: 0.0116, Val Loss: 0.0198\n",
      "Val Metrics - Acc: 0.8696, F1: 0.8846, Precision: 0.8679, Recall: 0.9020, AUPRC: 0.9550\n",
      "Epoch 50/500 - Train Loss: 0.0102, Val Loss: 0.0211\n",
      "Val Metrics - Acc: 0.8696, F1: 0.8800, Precision: 0.8980, Recall: 0.8627, AUPRC: 0.9498\n",
      "Early stopping at epoch 51\n",
      "\n",
      "==================================================\n",
      "Fold 6/10\n",
      "==================================================\n",
      "Epoch 10/500 - Train Loss: 0.0191, Val Loss: 0.0295\n",
      "Val Metrics - Acc: 0.7500, F1: 0.7850, Precision: 0.7500, Recall: 0.8235, AUPRC: 0.9044\n",
      "Epoch 20/500 - Train Loss: 0.0148, Val Loss: 0.0297\n",
      "Val Metrics - Acc: 0.7609, F1: 0.7885, Precision: 0.7736, Recall: 0.8039, AUPRC: 0.9223\n",
      "Early stopping at epoch 22\n",
      "\n",
      "==================================================\n",
      "Fold 7/10\n",
      "==================================================\n",
      "Epoch 10/500 - Train Loss: 0.0184, Val Loss: 0.0239\n",
      "Val Metrics - Acc: 0.8913, F1: 0.9038, Precision: 0.8868, Recall: 0.9216, AUPRC: 0.9313\n",
      "Epoch 20/500 - Train Loss: 0.0147, Val Loss: 0.0284\n",
      "Val Metrics - Acc: 0.8587, F1: 0.8738, Precision: 0.8654, Recall: 0.8824, AUPRC: 0.9114\n",
      "Early stopping at epoch 26\n",
      "\n",
      "==================================================\n",
      "Fold 8/10\n",
      "==================================================\n",
      "Epoch 10/500 - Train Loss: 0.0194, Val Loss: 0.0245\n",
      "Val Metrics - Acc: 0.8478, F1: 0.8727, Precision: 0.8136, Recall: 0.9412, AUPRC: 0.9367\n",
      "Epoch 20/500 - Train Loss: 0.0160, Val Loss: 0.0265\n",
      "Val Metrics - Acc: 0.8478, F1: 0.8727, Precision: 0.8136, Recall: 0.9412, AUPRC: 0.9320\n",
      "Early stopping at epoch 22\n",
      "\n",
      "==================================================\n",
      "Fold 9/10\n",
      "==================================================\n",
      "Epoch 10/500 - Train Loss: 0.0187, Val Loss: 0.0292\n",
      "Val Metrics - Acc: 0.8352, F1: 0.8571, Precision: 0.8182, Recall: 0.9000, AUPRC: 0.8413\n",
      "Epoch 20/500 - Train Loss: 0.0152, Val Loss: 0.0280\n",
      "Val Metrics - Acc: 0.8571, F1: 0.8762, Precision: 0.8364, Recall: 0.9200, AUPRC: 0.8847\n",
      "Epoch 30/500 - Train Loss: 0.0124, Val Loss: 0.0288\n",
      "Val Metrics - Acc: 0.8571, F1: 0.8762, Precision: 0.8364, Recall: 0.9200, AUPRC: 0.9051\n",
      "Epoch 40/500 - Train Loss: 0.0104, Val Loss: 0.0309\n",
      "Val Metrics - Acc: 0.8571, F1: 0.8762, Precision: 0.8364, Recall: 0.9200, AUPRC: 0.9076\n",
      "Early stopping at epoch 42\n",
      "\n",
      "==================================================\n",
      "Fold 10/10\n",
      "==================================================\n",
      "Epoch 10/500 - Train Loss: 0.0192, Val Loss: 0.0208\n",
      "Val Metrics - Acc: 0.9341, F1: 0.9412, Precision: 0.9231, Recall: 0.9600, AUPRC: 0.9587\n",
      "Epoch 20/500 - Train Loss: 0.0156, Val Loss: 0.0209\n",
      "Val Metrics - Acc: 0.9231, F1: 0.9320, Precision: 0.9057, Recall: 0.9600, AUPRC: 0.9546\n",
      "Epoch 30/500 - Train Loss: 0.0133, Val Loss: 0.0218\n",
      "Val Metrics - Acc: 0.9011, F1: 0.9109, Precision: 0.9020, Recall: 0.9200, AUPRC: 0.9507\n",
      "Epoch 40/500 - Train Loss: 0.0112, Val Loss: 0.0243\n",
      "Val Metrics - Acc: 0.8901, F1: 0.8980, Precision: 0.9167, Recall: 0.8800, AUPRC: 0.9429\n",
      "Early stopping at epoch 41\n",
      "\n",
      "Average Metrics:\n",
      "Average Accuracy: 0.8650\n",
      "Average F1: 0.8804\n",
      "Average Precision: 0.8644\n",
      "Average Recall: 0.8977\n",
      "Average AUPRC: 0.9293\n",
      "\n",
      "训练模型（带SMOTE）:\n",
      "数据形状: X=(918, 11), y=(918,)\n",
      "类别分布: [410 508]\n",
      "使用SMOTE: 是\n",
      "\n",
      "==================================================\n",
      "Fold 1/10\n",
      "==================================================\n",
      "SMOTE应用后训练数据形状: X=(914, 11), y=(914,)\n",
      "Epoch 10/500 - Train Loss: 0.0176, Val Loss: 0.0230\n",
      "Val Metrics - Acc: 0.8478, F1: 0.8600, Precision: 0.8776, Recall: 0.8431, AUPRC: 0.9341\n",
      "Epoch 20/500 - Train Loss: 0.0136, Val Loss: 0.0235\n",
      "Val Metrics - Acc: 0.8478, F1: 0.8571, Precision: 0.8936, Recall: 0.8235, AUPRC: 0.9129\n",
      "Epoch 30/500 - Train Loss: 0.0109, Val Loss: 0.0248\n",
      "Val Metrics - Acc: 0.8913, F1: 0.9020, Precision: 0.9020, Recall: 0.9020, AUPRC: 0.9222\n",
      "Early stopping at epoch 33\n",
      "\n",
      "==================================================\n",
      "Fold 2/10\n",
      "==================================================\n",
      "SMOTE应用后训练数据形状: X=(914, 11), y=(914,)\n",
      "Epoch 10/500 - Train Loss: 0.0186, Val Loss: 0.0240\n",
      "Val Metrics - Acc: 0.8587, F1: 0.8632, Precision: 0.9318, Recall: 0.8039, AUPRC: 0.9276\n",
      "Epoch 20/500 - Train Loss: 0.0153, Val Loss: 0.0249\n",
      "Val Metrics - Acc: 0.8587, F1: 0.8687, Precision: 0.8958, Recall: 0.8431, AUPRC: 0.9244\n",
      "Early stopping at epoch 23\n",
      "\n",
      "==================================================\n",
      "Fold 3/10\n",
      "==================================================\n",
      "SMOTE应用后训练数据形状: X=(914, 11), y=(914,)\n",
      "Epoch 10/500 - Train Loss: 0.0185, Val Loss: 0.0237\n",
      "Val Metrics - Acc: 0.8696, F1: 0.8868, Precision: 0.8545, Recall: 0.9216, AUPRC: 0.9170\n",
      "Epoch 20/500 - Train Loss: 0.0146, Val Loss: 0.0261\n",
      "Val Metrics - Acc: 0.8478, F1: 0.8654, Precision: 0.8491, Recall: 0.8824, AUPRC: 0.8924\n",
      "Epoch 30/500 - Train Loss: 0.0121, Val Loss: 0.0311\n",
      "Val Metrics - Acc: 0.7935, F1: 0.8190, Precision: 0.7963, Recall: 0.8431, AUPRC: 0.8759\n",
      "Early stopping at epoch 32\n",
      "\n",
      "==================================================\n",
      "Fold 4/10\n",
      "==================================================\n",
      "SMOTE应用后训练数据形状: X=(914, 11), y=(914,)\n",
      "Epoch 10/500 - Train Loss: 0.0186, Val Loss: 0.0240\n",
      "Val Metrics - Acc: 0.8587, F1: 0.8738, Precision: 0.8654, Recall: 0.8824, AUPRC: 0.9433\n",
      "Epoch 20/500 - Train Loss: 0.0153, Val Loss: 0.0240\n",
      "Val Metrics - Acc: 0.8696, F1: 0.8824, Precision: 0.8824, Recall: 0.8824, AUPRC: 0.9447\n",
      "Epoch 30/500 - Train Loss: 0.0126, Val Loss: 0.0251\n",
      "Val Metrics - Acc: 0.8804, F1: 0.8889, Precision: 0.9167, Recall: 0.8627, AUPRC: 0.9448\n",
      "Epoch 40/500 - Train Loss: 0.0104, Val Loss: 0.0255\n",
      "Val Metrics - Acc: 0.8804, F1: 0.8911, Precision: 0.9000, Recall: 0.8824, AUPRC: 0.9434\n",
      "Early stopping at epoch 44\n",
      "\n",
      "==================================================\n",
      "Fold 5/10\n",
      "==================================================\n",
      "SMOTE应用后训练数据形状: X=(914, 11), y=(914,)\n",
      "Epoch 10/500 - Train Loss: 0.0178, Val Loss: 0.0199\n",
      "Val Metrics - Acc: 0.8261, F1: 0.8400, Precision: 0.8571, Recall: 0.8235, AUPRC: 0.9505\n",
      "Epoch 20/500 - Train Loss: 0.0146, Val Loss: 0.0177\n",
      "Val Metrics - Acc: 0.8587, F1: 0.8713, Precision: 0.8800, Recall: 0.8627, AUPRC: 0.9587\n",
      "Epoch 30/500 - Train Loss: 0.0119, Val Loss: 0.0181\n",
      "Val Metrics - Acc: 0.9022, F1: 0.9143, Precision: 0.8889, Recall: 0.9412, AUPRC: 0.9598\n",
      "Epoch 40/500 - Train Loss: 0.0099, Val Loss: 0.0209\n",
      "Val Metrics - Acc: 0.8913, F1: 0.9020, Precision: 0.9020, Recall: 0.9020, AUPRC: 0.9522\n",
      "Early stopping at epoch 47\n",
      "\n",
      "==================================================\n",
      "Fold 6/10\n",
      "==================================================\n",
      "SMOTE应用后训练数据形状: X=(914, 11), y=(914,)\n",
      "Epoch 10/500 - Train Loss: 0.0178, Val Loss: 0.0302\n",
      "Val Metrics - Acc: 0.8043, F1: 0.8235, Precision: 0.8235, Recall: 0.8235, AUPRC: 0.8997\n",
      "Epoch 20/500 - Train Loss: 0.0141, Val Loss: 0.0318\n",
      "Val Metrics - Acc: 0.7717, F1: 0.8000, Precision: 0.7778, Recall: 0.8235, AUPRC: 0.9114\n",
      "Early stopping at epoch 22\n",
      "\n",
      "==================================================\n",
      "Fold 7/10\n",
      "==================================================\n",
      "SMOTE应用后训练数据形状: X=(914, 11), y=(914,)\n",
      "Epoch 10/500 - Train Loss: 0.0183, Val Loss: 0.0260\n",
      "Val Metrics - Acc: 0.8913, F1: 0.9038, Precision: 0.8868, Recall: 0.9216, AUPRC: 0.9058\n",
      "Epoch 20/500 - Train Loss: 0.0140, Val Loss: 0.0317\n",
      "Val Metrics - Acc: 0.8913, F1: 0.9038, Precision: 0.8868, Recall: 0.9216, AUPRC: 0.8942\n",
      "Early stopping at epoch 28\n",
      "\n",
      "==================================================\n",
      "Fold 8/10\n",
      "==================================================\n",
      "SMOTE应用后训练数据形状: X=(914, 11), y=(914,)\n",
      "Epoch 10/500 - Train Loss: 0.0177, Val Loss: 0.0230\n",
      "Val Metrics - Acc: 0.8696, F1: 0.8868, Precision: 0.8545, Recall: 0.9216, AUPRC: 0.9298\n",
      "Epoch 20/500 - Train Loss: 0.0136, Val Loss: 0.0276\n",
      "Val Metrics - Acc: 0.8587, F1: 0.8829, Precision: 0.8167, Recall: 0.9608, AUPRC: 0.9147\n",
      "Early stopping at epoch 23\n",
      "\n",
      "==================================================\n",
      "Fold 9/10\n",
      "==================================================\n",
      "SMOTE应用后训练数据形状: X=(916, 11), y=(916,)\n",
      "Epoch 10/500 - Train Loss: 0.0180, Val Loss: 0.0284\n",
      "Val Metrics - Acc: 0.8462, F1: 0.8654, Precision: 0.8333, Recall: 0.9000, AUPRC: 0.8911\n",
      "Epoch 20/500 - Train Loss: 0.0144, Val Loss: 0.0293\n",
      "Val Metrics - Acc: 0.8571, F1: 0.8738, Precision: 0.8491, Recall: 0.9000, AUPRC: 0.8992\n",
      "Epoch 30/500 - Train Loss: 0.0122, Val Loss: 0.0315\n",
      "Val Metrics - Acc: 0.8462, F1: 0.8654, Precision: 0.8333, Recall: 0.9000, AUPRC: 0.8992\n",
      "Early stopping at epoch 31\n",
      "\n",
      "==================================================\n",
      "Fold 10/10\n",
      "==================================================\n",
      "SMOTE应用后训练数据形状: X=(916, 11), y=(916,)\n",
      "Epoch 10/500 - Train Loss: 0.0189, Val Loss: 0.0180\n",
      "Val Metrics - Acc: 0.9231, F1: 0.9293, Precision: 0.9388, Recall: 0.9200, AUPRC: 0.9695\n",
      "Epoch 20/500 - Train Loss: 0.0148, Val Loss: 0.0172\n",
      "Val Metrics - Acc: 0.9121, F1: 0.9200, Precision: 0.9200, Recall: 0.9200, AUPRC: 0.9747\n",
      "Epoch 30/500 - Train Loss: 0.0121, Val Loss: 0.0179\n",
      "Val Metrics - Acc: 0.9121, F1: 0.9200, Precision: 0.9200, Recall: 0.9200, AUPRC: 0.9721\n",
      "Epoch 40/500 - Train Loss: 0.0100, Val Loss: 0.0201\n",
      "Val Metrics - Acc: 0.9011, F1: 0.9072, Precision: 0.9362, Recall: 0.8800, AUPRC: 0.9712\n",
      "Early stopping at epoch 44\n",
      "\n",
      "Average Metrics:\n",
      "Average Accuracy: 0.8584\n",
      "Average F1: 0.8743\n",
      "Average Precision: 0.8599\n",
      "Average Recall: 0.8898\n",
      "Average AUPRC: 0.9266\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import (accuracy_score, f1_score, precision_score,\n",
    "                             recall_score, average_precision_score,\n",
    "                             precision_recall_curve, auc)\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# TeLU激活函数\n",
    "class TeLU(nn.Module):\n",
    "    def __init__(self, alpha=0.15):\n",
    "        super(TeLU, self).__init__()\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.where(x >= 0, x, self.alpha * (torch.exp(x) - 1))\n",
    "\n",
    "\n",
    "# 前馈神经网络\n",
    "class FFNN(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(FFNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 32)\n",
    "        self.telu1 = TeLU(alpha=0.15)\n",
    "        self.fc2 = nn.Linear(32, 64)\n",
    "        self.telu2 = TeLU(alpha=0.1)\n",
    "        self.fc3 = nn.Linear(64, 2)\n",
    "\n",
    "        # 初始化权重\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.telu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.telu2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# 焦点损失函数\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        BCE_loss = nn.CrossEntropyLoss(reduction='none')(inputs, targets)\n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        F_loss = self.alpha * (1 - pt) ** self.gamma * BCE_loss\n",
    "        return F_loss.mean()\n",
    "\n",
    "\n",
    "def calculate_metrics(y_true, y_pred, y_scores):\n",
    "    # 检查NaN值\n",
    "    if np.isnan(y_scores).any():\n",
    "        y_scores = np.nan_to_num(y_scores)\n",
    "\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_true, y_pred),\n",
    "        'f1': f1_score(y_true, y_pred, zero_division=0),\n",
    "        'precision': precision_score(y_true, y_pred, zero_division=0),\n",
    "        'recall': recall_score(y_true, y_pred, zero_division=0),\n",
    "    }\n",
    "\n",
    "    # 只有在y_scores有效时才计算AUPRC\n",
    "    try:\n",
    "        metrics['auprc'] = average_precision_score(y_true, y_scores[:, 1])\n",
    "    except:\n",
    "        print(\"无法计算AUPRC，使用默认值0\")\n",
    "        metrics['auprc'] = 0\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, dpi=720):\n",
    "    \"\"\"绘制正方形混淆矩阵\"\"\"\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    cm_percentage = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100  # 百分比表示\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    ax = sns.heatmap(cm_percentage, annot=False, fmt='.2f', cmap='Blues', square=True, cbar=False,\n",
    "                     linewidths=2, linecolor='black')\n",
    "\n",
    "    # 在每个格子中显示个数和百分比\n",
    "    for i in range(2):  # 因为是二分类\n",
    "        for j in range(2):\n",
    "            # 判断字体颜色，深色背景用白色字体，浅色背景用黑色字体\n",
    "            text_color = 'white' if cm_percentage[i, j] > 50 else 'black'\n",
    "\n",
    "            # 将个数和百分百分比行显示，个数在上，百分比在下\n",
    "            ax.text(j + 0.5, i + 0.5, f'{cm[i, j]}\\n({cm_percentage[i, j]:.2f}%)',\n",
    "                    color=text_color, ha='center', va='center', fontsize=14, fontweight='bold')\n",
    "\n",
    "    # 添加中文标签\n",
    "    plt.xlabel('预测类别', fontsize=16, fontweight='bold')\n",
    "    plt.ylabel('实际类别', fontsize=16, fontweight='bold')\n",
    "    plt.xticks(ticks=np.arange(2) + 0.5, labels=np.arange(1, 3), fontsize=14, fontweight='bold')\n",
    "    plt.yticks(ticks=np.arange(2) + 0.5, labels=np.arange(1, 3), fontsize=14, fontweight='bold')\n",
    "\n",
    "    # 调整布局，减少空白边缘\n",
    "    plt.subplots_adjust(left=0.1, right=0.9, top=0.9, bottom=0.1)\n",
    "\n",
    "    # 保存混淆矩阵图\n",
    "    plt.savefig(f'best_fold_confusion_matrix.png', dpi=dpi)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs=500, patience=20):\n",
    "    best_val_loss = float('inf')\n",
    "    best_model = None\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "\n",
    "            # 梯度裁剪防止爆炸\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        # 验证\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        all_scores = []\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "                # 使用softmax获取概率\n",
    "                scores = torch.softmax(outputs, dim=1)\n",
    "                _, predicted = torch.max(scores.data, 1)\n",
    "\n",
    "                all_preds.extend(predicted.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                all_scores.extend(scores.cpu().numpy())\n",
    "\n",
    "        # 检查NaN值\n",
    "        if np.isnan(np.array(all_scores)).any():\n",
    "            all_scores = np.nan_to_num(all_scores)\n",
    "\n",
    "        # 计算指标\n",
    "        train_loss = train_loss / len(train_loader.dataset)\n",
    "        val_loss = val_loss / len(val_loader.dataset)\n",
    "        metrics = calculate_metrics(all_labels, all_preds, np.array(all_scores))\n",
    "\n",
    "        # 早停\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model = copy.deepcopy(model.state_dict())\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f'Early stopping at epoch {epoch + 1}')\n",
    "                break\n",
    "\n",
    "        # 打印进度\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch {epoch + 1}/{epochs} - Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "            print(f\"Val Metrics - Acc: {metrics['accuracy']:.4f}, F1: {metrics['f1']:.4f}, \"\n",
    "                  f\"Precision: {metrics['precision']:.4f}, Recall: {metrics['recall']:.4f}, \"\n",
    "                  f\"AUPRC: {metrics['auprc']:.4f}\")\n",
    "\n",
    "    # 加载最佳模型\n",
    "    model.load_state_dict(best_model)\n",
    "    return model\n",
    "\n",
    "\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_scores = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            scores = torch.softmax(outputs, dim=1)\n",
    "            _, predicted = torch.max(scores.data, 1)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_scores.extend(scores.cpu().numpy())\n",
    "\n",
    "    # 检查NaN值\n",
    "    if np.isnan(np.array(all_scores)).any():\n",
    "        all_scores = np.nan_to_num(all_scores)\n",
    "\n",
    "    return all_labels, all_preds, np.array(all_scores)\n",
    "\n",
    "\n",
    "def train_test_split(X, y, splits=10, epochs=500, batch_size=32, lr=0.001, use_smote=False):\n",
    "    # 检查数据\n",
    "    print(f\"数据形状: X={X.shape}, y={y.shape}\")\n",
    "    print(f\"类别分布: {np.bincount(y)}\")\n",
    "    print(f\"使用SMOTE: {'是' if use_smote else '否'}\")\n",
    "\n",
    "    # 处理可能的NaN值\n",
    "    X = np.nan_to_num(X)\n",
    "    y = np.nan_to_num(y).astype(int)\n",
    "\n",
    "    k_fold = StratifiedKFold(n_splits=splits, shuffle=True, random_state=2025)\n",
    "    results = []\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(k_fold.split(X, y)):\n",
    "        print(f'\\n{\"=\" * 50}')\n",
    "        print(f'Fold {fold + 1}/{splits}')\n",
    "        print(f'{\"=\" * 50}')\n",
    "\n",
    "        # 分割数据\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "        # 标准化\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "        # 应用SMOTE过采样（仅对训练数据）\n",
    "        if use_smote:\n",
    "            smote = SMOTE(random_state=42)\n",
    "            X_train_res, y_train_res = smote.fit_resample(X_train_scaled, y_train)\n",
    "            print(f\"SMOTE应用后训练数据形状: X={X_train_res.shape}, y={y_train_res.shape}\")\n",
    "\n",
    "            # 使用过采样后的数据\n",
    "            X_train_final = X_train_res\n",
    "            y_train_final = y_train_res\n",
    "        else:\n",
    "            X_train_final = X_train_scaled\n",
    "            y_train_final = y_train\n",
    "\n",
    "        # 转换为张量\n",
    "        X_train_tensor = torch.FloatTensor(X_train_final)\n",
    "        y_train_tensor = torch.LongTensor(y_train_final)\n",
    "        X_test_tensor = torch.FloatTensor(X_test_scaled)\n",
    "        y_test_tensor = torch.LongTensor(y_test)\n",
    "\n",
    "        # 创建数据集和数据加载器\n",
    "        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "        test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "        # 初始化模型、损失函数和优化器\n",
    "        model = FFNN(input_size=X.shape[1])\n",
    "        criterion = FocalLoss(alpha=0.25, gamma=2)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "        # 训练模型\n",
    "        model = train_model(model, train_loader, test_loader, criterion, optimizer,\n",
    "                            epochs=epochs, patience=20)\n",
    "\n",
    "        # 评估测试集\n",
    "        y_true, y_pred, y_scores = evaluate_model(model, test_loader)\n",
    "\n",
    "        # 绘制并保存最佳折的混淆矩阵\n",
    "        if fold == 0:\n",
    "            plot_confusion_matrix(y_true, y_pred)\n",
    "\n",
    "        # 计算并保存结果\n",
    "        metrics = calculate_metrics(y_true, y_pred, y_scores)\n",
    "        results.append(metrics)\n",
    "\n",
    "    # 计算并打印平均指标\n",
    "    avg_metrics = {\n",
    "        'accuracy': np.mean([r['accuracy'] for r in results]),\n",
    "        'f1': np.mean([r['f1'] for r in results]),\n",
    "        'precision': np.mean([r['precision'] for r in results]),\n",
    "        'recall': np.mean([r['recall'] for r in results]),\n",
    "        'auprc': np.mean([r['auprc'] for r in results])\n",
    "    }\n",
    "\n",
    "    print(f'\\nAverage Metrics:')\n",
    "    print(f\"Average Accuracy: {avg_metrics['accuracy']:.4f}\")\n",
    "    print(f\"Average F1: {avg_metrics['f1']:.4f}\")\n",
    "    print(f\"Average Precision: {avg_metrics['precision']:.4f}\")\n",
    "    print(f\"Average Recall: {avg_metrics['recall']:.4f}\")\n",
    "    print(f\"Average AUPRC: {avg_metrics['auprc']:.4f}\")\n",
    "\n",
    "\n",
    "# 加载数据\n",
    "data = pd.read_csv('preparations/heart_output.csv')  # 请替换为您的实际文件路径\n",
    "\n",
    "# 检查数据\n",
    "print(\"数据前5行:\")\n",
    "print(data.head())\n",
    "print(\"\\n类别分布:\")\n",
    "print(data['HeartDisease'].value_counts())\n",
    "\n",
    "# 分离特征和目标\n",
    "X = data.drop('HeartDisease', axis=1).values\n",
    "y = data['HeartDisease'].values\n",
    "\n",
    "# 转换为numpy数组\n",
    "X = X.astype(np.float32)\n",
    "y = y.astype(np.int64)\n",
    "\n",
    "# 运行训练和评估（不带SMOTE）\n",
    "print(\"\\n训练模型（不带SMOTE）:\")\n",
    "train_test_split(X, y, splits=10, epochs=500, batch_size=32, lr=0.001, use_smote=False)\n",
    "\n",
    "# 运行训练和评估（带SMOTE）\n",
    "print(\"\\n训练模型（带SMOTE）:\")\n",
    "train_test_split(X, y, splits=10, epochs=500, batch_size=32, lr=0.001, use_smote=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0b10019a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据前5行:\n",
      "   Age  Sex  ChestPainType  RestingBP  Cholesterol  FastingBS  RestingECG  \\\n",
      "0   40    0              2        140          289          0           0   \n",
      "1   49    1              1        160          180          0           0   \n",
      "2   37    0              2        130          283          0           1   \n",
      "3   48    1              0        138          214          0           0   \n",
      "4   54    0              1        150          195          0           0   \n",
      "\n",
      "   MaxHR  ExerciseAngina  Oldpeak  ST_Slope  HeartDisease  \n",
      "0    172               0      0.0         0             0  \n",
      "1    156               0      1.0         1             1  \n",
      "2     98               0      0.0         0             0  \n",
      "3    108               1      1.5         1             1  \n",
      "4    122               0      0.0         0             0  \n",
      "\n",
      "==================================================\n",
      "Fold 1/10\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "Fold 2/10\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "Fold 3/10\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "Fold 4/10\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "Fold 5/10\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "Fold 6/10\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "Fold 7/10\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "Fold 8/10\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "Fold 9/10\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "Fold 10/10\n",
      "==================================================\n",
      "绘制最佳折叠的混淆矩阵: 2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 设置中文字体\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']  # 设置中文字体为黑体\n",
    "plt.rcParams['axes.unicode_minus'] = False  # 解决负号显示问题\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, dpi=720):\n",
    "    \"\"\"绘制最佳模型混淆矩阵\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    cm_percentage = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100  # 百分比表示\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    ax = sns.heatmap(cm_percentage, annot=False, fmt='.2f', cmap='Blues', square=True, cbar=False,\n",
    "                     linewidths=2, linecolor='black')\n",
    "\n",
    "    # 在每个格子中显示个数和百分比\n",
    "    for i in range(2):  # 假设是二分类，可以根据需要调整\n",
    "        for j in range(2):\n",
    "            text_color = 'white' if cm_percentage[i, j] > 50 else 'black'\n",
    "            ax.text(j + 0.5, i + 0.5, f'{cm[i, j]}\\n({cm_percentage[i, j]:.2f}%)',\n",
    "                    color=text_color, ha='center', va='center', fontsize=14, fontweight='bold')\n",
    "\n",
    "    # 添加中文标签\n",
    "    plt.xlabel('预测类别', fontsize=16, fontweight='bold')\n",
    "    plt.ylabel('实际类别', fontsize=16, fontweight='bold')\n",
    "    plt.xticks(ticks=np.arange(2) + 0.5, labels=np.arange(1, 3), fontsize=14, fontweight='bold')\n",
    "    plt.yticks(ticks=np.arange(2) + 0.5, labels=np.arange(1, 3), fontsize=14, fontweight='bold')\n",
    "\n",
    "    # 调整布局，减少空白边缘\n",
    "    plt.subplots_adjust(left=0.1, right=0.9, top=0.9, bottom=0.1)\n",
    "\n",
    "    # 保存混淆矩阵图\n",
    "    plt.savefig(f'best_model_confusion_matrix.png', dpi=dpi)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def train_test_split(X, y, splits=10):\n",
    "    X = np.nan_to_num(X)\n",
    "    y = np.nan_to_num(y).astype(int)\n",
    "\n",
    "    k_fold = StratifiedKFold(n_splits=splits, shuffle=True, random_state=2025)\n",
    "    \n",
    "    best_fold_metrics = None\n",
    "    best_fold = -1\n",
    "    best_y_true = None\n",
    "    best_y_pred = None\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(k_fold.split(X, y)):\n",
    "        print(f'\\n{\"=\" * 50}')\n",
    "        print(f'Fold {fold + 1}/{splits}')\n",
    "        print(f'{\"=\" * 50}')\n",
    "\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "        model = LogisticRegression(penalty='l2', C=1.0, solver='liblinear', random_state=42)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        metrics = {\n",
    "            'accuracy': accuracy_score(y_test, y_pred),\n",
    "            'f1': f1_score(y_test, y_pred),\n",
    "            'precision': precision_score(y_test, y_pred),\n",
    "            'recall': recall_score(y_test, y_pred),\n",
    "            'auroc': roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])\n",
    "        }\n",
    "\n",
    "        # 跟踪最佳折叠（根据 AUROC）\n",
    "        if best_fold_metrics is None or metrics['auroc'] > best_fold_metrics['auroc']:\n",
    "            best_fold_metrics = metrics\n",
    "            best_fold = fold\n",
    "            best_y_true = y_test\n",
    "            best_y_pred = y_pred\n",
    "\n",
    "    # 绘制最佳折叠的混淆矩阵并保存\n",
    "    print(f\"绘制最佳折叠的混淆矩阵: {best_fold + 1}\")\n",
    "    plot_confusion_matrix(best_y_true, best_y_pred)\n",
    "\n",
    "\n",
    "# 加载数据\n",
    "data = pd.read_csv('preparations/heart_output.csv')\n",
    "\n",
    "# 检查数据\n",
    "print(\"数据前5行:\")\n",
    "print(data.head())\n",
    "\n",
    "# 分离特征和目标\n",
    "X = data.drop('HeartDisease', axis=1).values\n",
    "y = data['HeartDisease'].values\n",
    "\n",
    "# 运行训练和评估\n",
    "train_test_split(X, y, splits=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "11a1fa2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据前5行:\n",
      "   Age  Sex  ChestPainType  RestingBP  Cholesterol  FastingBS  RestingECG  \\\n",
      "0   40    0              2        140          289          0           0   \n",
      "1   49    1              1        160          180          0           0   \n",
      "2   37    0              2        130          283          0           1   \n",
      "3   48    1              0        138          214          0           0   \n",
      "4   54    0              1        150          195          0           0   \n",
      "\n",
      "   MaxHR  ExerciseAngina  Oldpeak  ST_Slope  HeartDisease  \n",
      "0    172               0      0.0         0             0  \n",
      "1    156               0      1.0         1             1  \n",
      "2     98               0      0.0         0             0  \n",
      "3    108               1      1.5         1             1  \n",
      "4    122               0      0.0         0             0  \n",
      "数据形状: X=(918, 11), y=(918,)\n",
      "类别分布: [410 508]\n",
      "\n",
      "==================================================\n",
      "Fold 1/10\n",
      "==================================================\n",
      "\n",
      "Fold 1 测试集指标:\n",
      "准确度: 0.9022\n",
      "F1分数: 0.9109\n",
      "精确率: 0.9200\n",
      "召回率: 0.9020\n",
      "AUPRC: 0.9299\n",
      "\n",
      "==================================================\n",
      "Fold 2/10\n",
      "==================================================\n",
      "\n",
      "Fold 2 测试集指标:\n",
      "准确度: 0.8913\n",
      "F1分数: 0.9020\n",
      "精确率: 0.9020\n",
      "召回率: 0.9020\n",
      "AUPRC: 0.9520\n",
      "\n",
      "==================================================\n",
      "Fold 3/10\n",
      "==================================================\n",
      "\n",
      "Fold 3 测试集指标:\n",
      "准确度: 0.8696\n",
      "F1分数: 0.8868\n",
      "精确率: 0.8545\n",
      "召回率: 0.9216\n",
      "AUPRC: 0.8606\n",
      "\n",
      "==================================================\n",
      "Fold 4/10\n",
      "==================================================\n",
      "\n",
      "Fold 4 测试集指标:\n",
      "准确度: 0.8696\n",
      "F1分数: 0.8800\n",
      "精确率: 0.8980\n",
      "召回率: 0.8627\n",
      "AUPRC: 0.9650\n",
      "\n",
      "==================================================\n",
      "Fold 5/10\n",
      "==================================================\n",
      "\n",
      "Fold 5 测试集指标:\n",
      "准确度: 0.8478\n",
      "F1分数: 0.8571\n",
      "精确率: 0.8936\n",
      "召回率: 0.8235\n",
      "AUPRC: 0.9217\n",
      "\n",
      "==================================================\n",
      "Fold 6/10\n",
      "==================================================\n",
      "\n",
      "Fold 6 测试集指标:\n",
      "准确度: 0.7826\n",
      "F1分数: 0.8148\n",
      "精确率: 0.7719\n",
      "召回率: 0.8627\n",
      "AUPRC: 0.9068\n",
      "\n",
      "==================================================\n",
      "Fold 7/10\n",
      "==================================================\n",
      "\n",
      "Fold 7 测试集指标:\n",
      "准确度: 0.8913\n",
      "F1分数: 0.9057\n",
      "精确率: 0.8727\n",
      "召回率: 0.9412\n",
      "AUPRC: 0.8905\n",
      "\n",
      "==================================================\n",
      "Fold 8/10\n",
      "==================================================\n",
      "\n",
      "Fold 8 测试集指标:\n",
      "准确度: 0.8696\n",
      "F1分数: 0.8909\n",
      "精确率: 0.8305\n",
      "召回率: 0.9608\n",
      "AUPRC: 0.9310\n",
      "\n",
      "==================================================\n",
      "Fold 9/10\n",
      "==================================================\n",
      "\n",
      "Fold 9 测试集指标:\n",
      "准确度: 0.8462\n",
      "F1分数: 0.8654\n",
      "精确率: 0.8333\n",
      "召回率: 0.9000\n",
      "AUPRC: 0.8843\n",
      "\n",
      "==================================================\n",
      "Fold 10/10\n",
      "==================================================\n",
      "\n",
      "Fold 10 测试集指标:\n",
      "准确度: 0.9011\n",
      "F1分数: 0.9072\n",
      "精确率: 0.9362\n",
      "召回率: 0.8800\n",
      "AUPRC: 0.9618\n",
      "绘制最佳折叠的混淆矩阵: 4\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score, auc\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, dpi=720):\n",
    "    \"\"\"绘制最佳模型混淆矩阵\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    cm_percentage = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100  # 百分比表示\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    ax = sns.heatmap(cm_percentage, annot=False, fmt='.2f', cmap='Blues', square=True, cbar=False,\n",
    "                     linewidths=2, linecolor='black')\n",
    "\n",
    "    # 在每个格子中显示个数和百分比\n",
    "    for i in range(2):  # 假设是二分类，可以根据需要调整\n",
    "        for j in range(2):\n",
    "            text_color = 'white' if cm_percentage[i, j] > 50 else 'black'\n",
    "            ax.text(j + 0.5, i + 0.5, f'{cm[i, j]}\\n({cm_percentage[i, j]:.2f}%)',\n",
    "                    color=text_color, ha='center', va='center', fontsize=14, fontweight='bold')\n",
    "\n",
    "    # 添加中文标签\n",
    "    plt.xlabel('预测类别', fontsize=16, fontweight='bold')\n",
    "    plt.ylabel('实际类别', fontsize=16, fontweight='bold')\n",
    "    plt.xticks(ticks=np.arange(2) + 0.5, labels=np.arange(1, 3), fontsize=14, fontweight='bold')\n",
    "    plt.yticks(ticks=np.arange(2) + 0.5, labels=np.arange(1, 3), fontsize=14, fontweight='bold')\n",
    "\n",
    "    # 调整布局，减少空白边缘\n",
    "    plt.subplots_adjust(left=0.1, right=0.9, top=0.9, bottom=0.1)\n",
    "\n",
    "    # 保存混淆矩阵图\n",
    "    plt.savefig(f'best_model_confusion_matrix.png', dpi=dpi)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def calculate_metrics(y_true, y_pred, y_scores):\n",
    "    \"\"\"计算评估指标\"\"\"\n",
    "    # 检查NaN值\n",
    "    if np.isnan(y_scores).any():\n",
    "        y_scores = np.nan_to_num(y_scores)\n",
    "\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_true, y_pred),\n",
    "        'f1': f1_score(y_true, y_pred, zero_division=0),\n",
    "        'precision': precision_score(y_true, y_pred, zero_division=0),\n",
    "        'recall': recall_score(y_true, y_pred, zero_division=0),\n",
    "    }\n",
    "\n",
    "    # 计算AUPRC\n",
    "    try:\n",
    "        metrics['auprc'] = average_precision_score(y_true, y_scores)\n",
    "    except:\n",
    "        print(\"无法计算AUPRC，使用默认值0\")\n",
    "        metrics['auprc'] = 0\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def train_test_split(X, y, splits=10, batch_size=32):\n",
    "    # 检查数据\n",
    "    print(f\"数据形状: X={X.shape}, y={y.shape}\")\n",
    "    print(f\"类别分布: {np.bincount(y)}\")\n",
    "\n",
    "    # 处理可能的NaN值\n",
    "    X = np.nan_to_num(X)\n",
    "    y = np.nan_to_num(y).astype(int)\n",
    "\n",
    "    k_fold = StratifiedKFold(n_splits=splits, shuffle=True, random_state=2025)\n",
    "    results = []\n",
    "\n",
    "    best_fold_metrics = None\n",
    "    best_fold = -1\n",
    "    best_y_true = None\n",
    "    best_y_pred = None\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(k_fold.split(X, y)):\n",
    "        print(f'\\n{\"=\" * 50}')\n",
    "        print(f'Fold {fold + 1}/{splits}')\n",
    "        print(f'{\"=\" * 50}')\n",
    "\n",
    "        # 切分数据\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "        # 标准化\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "        # 创建RandomForest模型\n",
    "        rf_model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "\n",
    "        # 训练模型\n",
    "        rf_model.fit(X_train, y_train)\n",
    "\n",
    "        # 测试集评估\n",
    "        y_scores = rf_model.predict_proba(X_test)[:, 1]  # 获取类1（正类）的概率\n",
    "        y_pred = (y_scores > 0.5).astype(int)\n",
    "        metrics = calculate_metrics(y_test, y_pred, y_scores)\n",
    "\n",
    "        # 跟踪最佳折叠（根据AUPRC）\n",
    "        if best_fold_metrics is None or metrics['auprc'] > best_fold_metrics['auprc']:\n",
    "            best_fold_metrics = metrics\n",
    "            best_fold = fold\n",
    "            best_y_true = y_test\n",
    "            best_y_pred = y_pred\n",
    "\n",
    "        # 保存每折的结果\n",
    "        results.append(metrics)\n",
    "\n",
    "        # 打印当前折的结果\n",
    "        print(f'\\nFold {fold + 1} 测试集指标:')\n",
    "        print(f\"准确度: {metrics['accuracy']:.4f}\")\n",
    "        print(f\"F1分数: {metrics['f1']:.4f}\")\n",
    "        print(f\"精确率: {metrics['precision']:.4f}\")\n",
    "        print(f\"召回率: {metrics['recall']:.4f}\")\n",
    "        print(f\"AUPRC: {metrics['auprc']:.4f}\")\n",
    "\n",
    "    # 绘制最佳折叠的混淆矩阵并保存\n",
    "    print(f\"绘制最佳折叠的混淆矩阵: {best_fold + 1}\")\n",
    "    plot_confusion_matrix(best_y_true, best_y_pred)\n",
    "\n",
    "\n",
    "# 加载数据\n",
    "data = pd.read_csv('preparations/heart_output.csv')  # 替换为实际文件路径\n",
    "\n",
    "# 检查数据\n",
    "print(\"数据前5行:\")\n",
    "print(data.head())\n",
    "\n",
    "# 分离特征和目标\n",
    "X = data.drop('HeartDisease', axis=1).values\n",
    "y = data['HeartDisease'].values\n",
    "\n",
    "# 转换为numpy数组\n",
    "X = X.astype(np.float32)\n",
    "y = y.astype(np.int64)\n",
    "\n",
    "# 运行训练和评估\n",
    "train_test_split(X, y, splits=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a16c16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据前5行:\n",
      "   Age  Sex  ChestPainType  RestingBP  Cholesterol  FastingBS  RestingECG  \\\n",
      "0   40    0              2        140          289          0           0   \n",
      "1   49    1              1        160          180          0           0   \n",
      "2   37    0              2        130          283          0           1   \n",
      "3   48    1              0        138          214          0           0   \n",
      "4   54    0              1        150          195          0           0   \n",
      "\n",
      "   MaxHR  ExerciseAngina  Oldpeak  ST_Slope  HeartDisease  \n",
      "0    172               0      0.0         0             0  \n",
      "1    156               0      1.0         1             1  \n",
      "2     98               0      0.0         0             0  \n",
      "3    108               1      1.5         1             1  \n",
      "4    122               0      0.0         0             0  \n",
      "数据形状: X=(918, 11), y=(918,)\n",
      "类别分布: [410 508]\n",
      "\n",
      "==================================================\n",
      "Fold 1/10\n",
      "==================================================\n",
      "\n",
      "Fold 1 测试集指标:\n",
      "准确度: 0.8478\n",
      "F1分数: 0.8679\n",
      "精确率: 0.8364\n",
      "召回率: 0.9020\n",
      "AUPRC: 0.9291\n",
      "\n",
      "==================================================\n",
      "Fold 2/10\n",
      "==================================================\n",
      "\n",
      "Fold 2 测试集指标:\n",
      "准确度: 0.9022\n",
      "F1分数: 0.9126\n",
      "精确率: 0.9038\n",
      "召回率: 0.9216\n",
      "AUPRC: 0.9376\n",
      "\n",
      "==================================================\n",
      "Fold 3/10\n",
      "==================================================\n",
      "\n",
      "Fold 3 测试集指标:\n",
      "准确度: 0.8478\n",
      "F1分数: 0.8679\n",
      "精确率: 0.8364\n",
      "召回率: 0.9020\n",
      "AUPRC: 0.8843\n",
      "\n",
      "==================================================\n",
      "Fold 4/10\n",
      "==================================================\n",
      "\n",
      "Fold 4 测试集指标:\n",
      "准确度: 0.8696\n",
      "F1分数: 0.8824\n",
      "精确率: 0.8824\n",
      "召回率: 0.8824\n",
      "AUPRC: 0.9561\n",
      "\n",
      "==================================================\n",
      "Fold 5/10\n",
      "==================================================\n",
      "\n",
      "Fold 5 测试集指标:\n",
      "准确度: 0.9130\n",
      "F1分数: 0.9216\n",
      "精确率: 0.9216\n",
      "召回率: 0.9216\n",
      "AUPRC: 0.9566\n",
      "\n",
      "==================================================\n",
      "Fold 6/10\n",
      "==================================================\n",
      "\n",
      "Fold 6 测试集指标:\n",
      "准确度: 0.7826\n",
      "F1分数: 0.8148\n",
      "精确率: 0.7719\n",
      "召回率: 0.8627\n",
      "AUPRC: 0.8946\n",
      "\n",
      "==================================================\n",
      "Fold 7/10\n",
      "==================================================\n",
      "\n",
      "Fold 7 测试集指标:\n",
      "准确度: 0.8913\n",
      "F1分数: 0.9038\n",
      "精确率: 0.8868\n",
      "召回率: 0.9216\n",
      "AUPRC: 0.8994\n",
      "\n",
      "==================================================\n",
      "Fold 8/10\n",
      "==================================================\n",
      "\n",
      "Fold 8 测试集指标:\n",
      "准确度: 0.8696\n",
      "F1分数: 0.8909\n",
      "精确率: 0.8305\n",
      "召回率: 0.9608\n",
      "AUPRC: 0.9167\n",
      "\n",
      "==================================================\n",
      "Fold 9/10\n",
      "==================================================\n",
      "\n",
      "Fold 9 测试集指标:\n",
      "准确度: 0.8242\n",
      "F1分数: 0.8462\n",
      "精确率: 0.8148\n",
      "召回率: 0.8800\n",
      "AUPRC: 0.9219\n",
      "\n",
      "==================================================\n",
      "Fold 10/10\n",
      "==================================================\n",
      "\n",
      "Fold 10 测试集指标:\n",
      "准确度: 0.9121\n",
      "F1分数: 0.9216\n",
      "精确率: 0.9038\n",
      "召回率: 0.9400\n",
      "AUPRC: 0.9601\n",
      "绘制最佳折叠的混淆矩阵: 10\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score, auc\n",
    "from sklearn.svm import SVC\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, dpi=720):\n",
    "    \"\"\"绘制最佳模型混淆矩阵\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    cm_percentage = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100  # 百分比表示\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    ax = sns.heatmap(cm_percentage, annot=False, fmt='.2f', cmap='Blues', square=True, cbar=False,\n",
    "                     linewidths=2, linecolor='black')\n",
    "\n",
    "    # 在每个格子中显示个数和百分比\n",
    "    for i in range(2):  # 假设是二分类，可以根据需要调整\n",
    "        for j in range(2):\n",
    "            text_color = 'white' if cm_percentage[i, j] > 50 else 'black'\n",
    "            ax.text(j + 0.5, i + 0.5, f'{cm[i, j]}\\n({cm_percentage[i, j]:.2f}%)',\n",
    "                    color=text_color, ha='center', va='center', fontsize=14, fontweight='bold')\n",
    "\n",
    "    # 添加中文标签\n",
    "    plt.xlabel('预测类别', fontsize=16, fontweight='bold')\n",
    "    plt.ylabel('实际类别', fontsize=16, fontweight='bold')\n",
    "    plt.xticks(ticks=np.arange(2) + 0.5, labels=np.arange(1, 3), fontsize=14, fontweight='bold')\n",
    "    plt.yticks(ticks=np.arange(2) + 0.5, labels=np.arange(1, 3), fontsize=14, fontweight='bold')\n",
    "\n",
    "    # 调整布局，减少空白边缘\n",
    "    plt.subplots_adjust(left=0.1, right=0.9, top=0.9, bottom=0.1)\n",
    "\n",
    "    # 保存混淆矩阵图\n",
    "    plt.savefig(f'best_model_confusion_matrix.png', dpi=dpi)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def calculate_metrics(y_true, y_pred, y_scores):\n",
    "    \"\"\"计算评估指标\"\"\"\n",
    "    # 检查NaN值\n",
    "    if np.isnan(y_scores).any():\n",
    "        y_scores = np.nan_to_num(y_scores)\n",
    "\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_true, y_pred),\n",
    "        'f1': f1_score(y_true, y_pred, zero_division=0),\n",
    "        'precision': precision_score(y_true, y_pred, zero_division=0),\n",
    "        'recall': recall_score(y_true, y_pred, zero_division=0),\n",
    "    }\n",
    "\n",
    "    # 计算AUPRC\n",
    "    try:\n",
    "        metrics['auprc'] = average_precision_score(y_true, y_scores)\n",
    "    except:\n",
    "        print(\"无法计算AUPRC，使用默认值0\")\n",
    "        metrics['auprc'] = 0\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def train_test_split(X, y, splits=10, batch_size=32):\n",
    "    # 检查数据\n",
    "    print(f\"数据形状: X={X.shape}, y={y.shape}\")\n",
    "    print(f\"类别分布: {np.bincount(y)}\")\n",
    "\n",
    "    # 处理可能的NaN值\n",
    "    X = np.nan_to_num(X)\n",
    "    y = np.nan_to_num(y).astype(int)\n",
    "\n",
    "    k_fold = StratifiedKFold(n_splits=splits, shuffle=True, random_state=2025)\n",
    "    results = []\n",
    "\n",
    "    # 跟踪最佳折叠（根据AUPRC）\n",
    "    best_fold_metrics = None\n",
    "    best_fold = -1\n",
    "    best_y_true = None\n",
    "    best_y_pred = None\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(k_fold.split(X, y)):\n",
    "        print(f'\\n{\"=\" * 50}')\n",
    "        print(f'Fold {fold + 1}/{splits}')\n",
    "        print(f'{\"=\" * 50}')\n",
    "\n",
    "        # 切分数据\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "        # 标准化\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "        # 创建SVM模型\n",
    "        model = SVC(kernel='rbf', C=1.0, gamma='scale', probability=True, random_state=42)\n",
    "\n",
    "        # 训练模型\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # 测试集评估\n",
    "        y_scores = model.predict_proba(X_test)[:, 1]  # 获取类1（正类）的概率\n",
    "        y_pred = (y_scores > 0.5).astype(int)\n",
    "        metrics = calculate_metrics(y_test, y_pred, y_scores)\n",
    "\n",
    "        # 跟踪最佳折叠\n",
    "        if best_fold_metrics is None or metrics['auprc'] > best_fold_metrics['auprc']:\n",
    "            best_fold_metrics = metrics\n",
    "            best_fold = fold\n",
    "            best_y_true = y_test\n",
    "            best_y_pred = y_pred\n",
    "\n",
    "        # 保存每折的结果\n",
    "        results.append(metrics)\n",
    "\n",
    "        # 打印当前折的结果\n",
    "        print(f'\\nFold {fold + 1} 测试集指标:')\n",
    "        print(f\"准确度: {metrics['accuracy']:.4f}\")\n",
    "        print(f\"F1分数: {metrics['f1']:.4f}\")\n",
    "        print(f\"精确率: {metrics['precision']:.4f}\")\n",
    "        print(f\"召回率: {metrics['recall']:.4f}\")\n",
    "        print(f\"AUPRC: {metrics['auprc']:.4f}\")\n",
    "\n",
    "    # 绘制最佳折叠的混淆矩阵并保存\n",
    "    print(f\"绘制最佳折叠的混淆矩阵: {best_fold + 1}\")\n",
    "    plot_confusion_matrix(best_y_true, best_y_pred)\n",
    "\n",
    "\n",
    "# 加载数据\n",
    "data = pd.read_csv('preparations/heart_output.csv')  # 替换为实际文件路径\n",
    "\n",
    "# 检查数据\n",
    "print(\"数据前5行:\")\n",
    "print(data.head())\n",
    "\n",
    "# 分离特征和目标\n",
    "X = data.drop('HeartDisease', axis=1).values\n",
    "y = data['HeartDisease'].values\n",
    "\n",
    "# 转换为numpy数组\n",
    "X = X.astype(np.float32)\n",
    "y = y.astype(np.int64)\n",
    "\n",
    "# 运行训练和评估\n",
    "train_test_split(X, y, splits=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "78f23723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据形状: X=(918, 11), y=(918,)\n",
      "类别分布: [410 508]\n",
      "\n",
      "==================================================\n",
      "Fold 1/10\n",
      "==================================================\n",
      "epoch 0  | loss: 0.5596  | val_0_auc: 0.92779 |  0:00:00s\n",
      "epoch 1  | loss: 0.42643 | val_0_auc: 0.88953 |  0:00:01s\n",
      "epoch 2  | loss: 0.41216 | val_0_auc: 0.901   |  0:00:02s\n",
      "epoch 3  | loss: 0.42895 | val_0_auc: 0.8582  |  0:00:02s\n",
      "epoch 4  | loss: 0.39687 | val_0_auc: 0.90866 |  0:00:03s\n",
      "epoch 5  | loss: 0.38664 | val_0_auc: 0.8637  |  0:00:04s\n",
      "epoch 6  | loss: 0.35683 | val_0_auc: 0.89455 |  0:00:04s\n",
      "epoch 7  | loss: 0.37754 | val_0_auc: 0.91248 |  0:00:05s\n",
      "epoch 8  | loss: 0.3558  | val_0_auc: 0.89144 |  0:00:06s\n",
      "epoch 9  | loss: 0.35001 | val_0_auc: 0.89383 |  0:00:06s\n",
      "epoch 10 | loss: 0.32909 | val_0_auc: 0.86705 |  0:00:07s\n",
      "epoch 11 | loss: 0.37618 | val_0_auc: 0.88809 |  0:00:07s\n",
      "epoch 12 | loss: 0.37751 | val_0_auc: 0.89766 |  0:00:08s\n",
      "epoch 13 | loss: 0.35447 | val_0_auc: 0.90483 |  0:00:09s\n",
      "epoch 14 | loss: 0.37833 | val_0_auc: 0.91296 |  0:00:10s\n",
      "epoch 15 | loss: 0.33428 | val_0_auc: 0.94165 |  0:00:11s\n",
      "epoch 16 | loss: 0.35022 | val_0_auc: 0.91774 |  0:00:11s\n",
      "epoch 17 | loss: 0.35482 | val_0_auc: 0.90292 |  0:00:12s\n",
      "epoch 18 | loss: 0.33724 | val_0_auc: 0.90626 |  0:00:13s\n",
      "epoch 19 | loss: 0.33468 | val_0_auc: 0.91057 |  0:00:14s\n",
      "epoch 20 | loss: 0.3307  | val_0_auc: 0.90387 |  0:00:15s\n",
      "epoch 21 | loss: 0.31841 | val_0_auc: 0.912   |  0:00:16s\n",
      "epoch 22 | loss: 0.3391  | val_0_auc: 0.93735 |  0:00:17s\n",
      "epoch 23 | loss: 0.31098 | val_0_auc: 0.94261 |  0:00:18s\n",
      "epoch 24 | loss: 0.31757 | val_0_auc: 0.90818 |  0:00:19s\n",
      "epoch 25 | loss: 0.32153 | val_0_auc: 0.9297  |  0:00:21s\n",
      "epoch 26 | loss: 0.31362 | val_0_auc: 0.91487 |  0:00:22s\n",
      "epoch 27 | loss: 0.2995  | val_0_auc: 0.91918 |  0:00:23s\n",
      "epoch 28 | loss: 0.32641 | val_0_auc: 0.92253 |  0:00:24s\n",
      "epoch 29 | loss: 0.31762 | val_0_auc: 0.89909 |  0:00:25s\n",
      "epoch 30 | loss: 0.32557 | val_0_auc: 0.86035 |  0:00:26s\n",
      "epoch 31 | loss: 0.29413 | val_0_auc: 0.91583 |  0:00:28s\n",
      "epoch 32 | loss: 0.29719 | val_0_auc: 0.91726 |  0:00:29s\n",
      "epoch 33 | loss: 0.28226 | val_0_auc: 0.93066 |  0:00:30s\n",
      "epoch 34 | loss: 0.30835 | val_0_auc: 0.93926 |  0:00:31s\n",
      "epoch 35 | loss: 0.29245 | val_0_auc: 0.92731 |  0:00:32s\n",
      "epoch 36 | loss: 0.29317 | val_0_auc: 0.934   |  0:00:33s\n",
      "epoch 37 | loss: 0.30872 | val_0_auc: 0.92157 |  0:00:34s\n",
      "epoch 38 | loss: 0.3006  | val_0_auc: 0.923   |  0:00:34s\n",
      "epoch 39 | loss: 0.31043 | val_0_auc: 0.90579 |  0:00:35s\n",
      "epoch 40 | loss: 0.30536 | val_0_auc: 0.91822 |  0:00:36s\n",
      "epoch 41 | loss: 0.27958 | val_0_auc: 0.92253 |  0:00:37s\n",
      "epoch 42 | loss: 0.31302 | val_0_auc: 0.93687 |  0:00:38s\n",
      "epoch 43 | loss: 0.28683 | val_0_auc: 0.92444 |  0:00:39s\n",
      "epoch 44 | loss: 0.28848 | val_0_auc: 0.90292 |  0:00:40s\n",
      "epoch 45 | loss: 0.29558 | val_0_auc: 0.91009 |  0:00:41s\n",
      "epoch 46 | loss: 0.30501 | val_0_auc: 0.92348 |  0:00:42s\n",
      "epoch 47 | loss: 0.29657 | val_0_auc: 0.90196 |  0:00:44s\n",
      "epoch 48 | loss: 0.28858 | val_0_auc: 0.91822 |  0:00:45s\n",
      "epoch 49 | loss: 0.31765 | val_0_auc: 0.89479 |  0:00:46s\n",
      "epoch 50 | loss: 0.28977 | val_0_auc: 0.92205 |  0:00:47s\n",
      "epoch 51 | loss: 0.30202 | val_0_auc: 0.91726 |  0:00:48s\n",
      "epoch 52 | loss: 0.30657 | val_0_auc: 0.92205 |  0:00:50s\n",
      "epoch 53 | loss: 0.29919 | val_0_auc: 0.93448 |  0:00:51s\n",
      "epoch 54 | loss: 0.29262 | val_0_auc: 0.93066 |  0:00:53s\n",
      "epoch 55 | loss: 0.29452 | val_0_auc: 0.94692 |  0:00:54s\n",
      "epoch 56 | loss: 0.29095 | val_0_auc: 0.93879 |  0:00:56s\n",
      "epoch 57 | loss: 0.29935 | val_0_auc: 0.93783 |  0:00:57s\n",
      "epoch 58 | loss: 0.30328 | val_0_auc: 0.93735 |  0:00:58s\n",
      "epoch 59 | loss: 0.2927  | val_0_auc: 0.94261 |  0:00:59s\n",
      "epoch 60 | loss: 0.30538 | val_0_auc: 0.94118 |  0:01:01s\n",
      "epoch 61 | loss: 0.29683 | val_0_auc: 0.90961 |  0:01:02s\n",
      "epoch 62 | loss: 0.30894 | val_0_auc: 0.91631 |  0:01:03s\n",
      "epoch 63 | loss: 0.30403 | val_0_auc: 0.93783 |  0:01:04s\n",
      "epoch 64 | loss: 0.30281 | val_0_auc: 0.9144  |  0:01:06s\n",
      "epoch 65 | loss: 0.28363 | val_0_auc: 0.93831 |  0:01:07s\n",
      "epoch 66 | loss: 0.28541 | val_0_auc: 0.94165 |  0:01:08s\n",
      "epoch 67 | loss: 0.28347 | val_0_auc: 0.93161 |  0:01:09s\n",
      "epoch 68 | loss: 0.29659 | val_0_auc: 0.92253 |  0:01:11s\n",
      "epoch 69 | loss: 0.29062 | val_0_auc: 0.91631 |  0:01:12s\n",
      "epoch 70 | loss: 0.28816 | val_0_auc: 0.89718 |  0:01:13s\n",
      "epoch 71 | loss: 0.30207 | val_0_auc: 0.92253 |  0:01:14s\n",
      "epoch 72 | loss: 0.29086 | val_0_auc: 0.92444 |  0:01:16s\n",
      "epoch 73 | loss: 0.28134 | val_0_auc: 0.88881 |  0:01:17s\n",
      "epoch 74 | loss: 0.2926  | val_0_auc: 0.92109 |  0:01:18s\n",
      "epoch 75 | loss: 0.27757 | val_0_auc: 0.92683 |  0:01:19s\n",
      "epoch 76 | loss: 0.27286 | val_0_auc: 0.89813 |  0:01:21s\n",
      "epoch 77 | loss: 0.29155 | val_0_auc: 0.9077  |  0:01:22s\n",
      "epoch 78 | loss: 0.27709 | val_0_auc: 0.93496 |  0:01:23s\n",
      "epoch 79 | loss: 0.28485 | val_0_auc: 0.92874 |  0:01:24s\n",
      "epoch 80 | loss: 0.27251 | val_0_auc: 0.93592 |  0:01:25s\n",
      "epoch 81 | loss: 0.27565 | val_0_auc: 0.90387 |  0:01:27s\n",
      "epoch 82 | loss: 0.26063 | val_0_auc: 0.91009 |  0:01:28s\n",
      "epoch 83 | loss: 0.27106 | val_0_auc: 0.90674 |  0:01:30s\n",
      "epoch 84 | loss: 0.28407 | val_0_auc: 0.91009 |  0:01:31s\n",
      "epoch 85 | loss: 0.26886 | val_0_auc: 0.9089  |  0:01:33s\n",
      "epoch 86 | loss: 0.28453 | val_0_auc: 0.94022 |  0:01:35s\n",
      "epoch 87 | loss: 0.26404 | val_0_auc: 0.93879 |  0:01:37s\n",
      "epoch 88 | loss: 0.27935 | val_0_auc: 0.95361 |  0:01:39s\n",
      "epoch 89 | loss: 0.2774  | val_0_auc: 0.93687 |  0:01:41s\n",
      "epoch 90 | loss: 0.27427 | val_0_auc: 0.92013 |  0:01:42s\n",
      "epoch 91 | loss: 0.28353 | val_0_auc: 0.94644 |  0:01:43s\n",
      "epoch 92 | loss: 0.27442 | val_0_auc: 0.91966 |  0:01:46s\n",
      "epoch 93 | loss: 0.27407 | val_0_auc: 0.93687 |  0:01:47s\n",
      "epoch 94 | loss: 0.26363 | val_0_auc: 0.93448 |  0:01:49s\n",
      "epoch 95 | loss: 0.25939 | val_0_auc: 0.91344 |  0:01:51s\n",
      "epoch 96 | loss: 0.25281 | val_0_auc: 0.93352 |  0:01:53s\n",
      "epoch 97 | loss: 0.26457 | val_0_auc: 0.95791 |  0:01:55s\n",
      "epoch 98 | loss: 0.25669 | val_0_auc: 0.92444 |  0:01:56s\n",
      "epoch 99 | loss: 0.2644  | val_0_auc: 0.91966 |  0:01:57s\n",
      "Stop training because you reached max_epochs = 100 with best_epoch = 97 and best_val_0_auc = 0.95791\n",
      "\n",
      "Fold 1 Test Metrics:\n",
      "Accuracy: 0.9022\n",
      "F1 Score: 0.9109\n",
      "Precision: 0.9200\n",
      "Recall: 0.9020\n",
      "AUPRC: 0.9622\n",
      "\n",
      "==================================================\n",
      "Fold 2/10\n",
      "==================================================\n",
      "epoch 0  | loss: 0.56871 | val_0_auc: 0.88905 |  0:00:01s\n",
      "epoch 1  | loss: 0.46229 | val_0_auc: 0.90913 |  0:00:02s\n",
      "epoch 2  | loss: 0.44342 | val_0_auc: 0.93639 |  0:00:03s\n",
      "epoch 3  | loss: 0.44289 | val_0_auc: 0.91487 |  0:00:05s\n",
      "epoch 4  | loss: 0.43109 | val_0_auc: 0.91559 |  0:00:06s\n",
      "epoch 5  | loss: 0.38349 | val_0_auc: 0.90842 |  0:00:07s\n",
      "epoch 6  | loss: 0.40014 | val_0_auc: 0.9187  |  0:00:09s\n",
      "epoch 7  | loss: 0.40951 | val_0_auc: 0.92922 |  0:00:10s\n",
      "epoch 8  | loss: 0.37565 | val_0_auc: 0.91057 |  0:00:11s\n",
      "epoch 9  | loss: 0.38422 | val_0_auc: 0.95385 |  0:00:13s\n",
      "epoch 10 | loss: 0.35937 | val_0_auc: 0.94333 |  0:00:14s\n",
      "epoch 11 | loss: 0.37012 | val_0_auc: 0.934   |  0:00:17s\n",
      "epoch 12 | loss: 0.37224 | val_0_auc: 0.93831 |  0:00:18s\n",
      "epoch 13 | loss: 0.35242 | val_0_auc: 0.93879 |  0:00:19s\n",
      "epoch 14 | loss: 0.32573 | val_0_auc: 0.93926 |  0:00:21s\n",
      "epoch 15 | loss: 0.30901 | val_0_auc: 0.93113 |  0:00:22s\n",
      "epoch 16 | loss: 0.33168 | val_0_auc: 0.93592 |  0:00:23s\n",
      "epoch 17 | loss: 0.31681 | val_0_auc: 0.945   |  0:00:25s\n",
      "epoch 18 | loss: 0.2985  | val_0_auc: 0.923   |  0:00:26s\n",
      "epoch 19 | loss: 0.31423 | val_0_auc: 0.92635 |  0:00:27s\n",
      "epoch 20 | loss: 0.30518 | val_0_auc: 0.93879 |  0:00:29s\n",
      "epoch 21 | loss: 0.30825 | val_0_auc: 0.93544 |  0:00:30s\n",
      "epoch 22 | loss: 0.30423 | val_0_auc: 0.93042 |  0:00:32s\n",
      "epoch 23 | loss: 0.30216 | val_0_auc: 0.93615 |  0:00:33s\n",
      "epoch 24 | loss: 0.30155 | val_0_auc: 0.91798 |  0:00:35s\n",
      "epoch 25 | loss: 0.29658 | val_0_auc: 0.93113 |  0:00:37s\n",
      "epoch 26 | loss: 0.27861 | val_0_auc: 0.93783 |  0:00:38s\n",
      "epoch 27 | loss: 0.28368 | val_0_auc: 0.94357 |  0:00:40s\n",
      "epoch 28 | loss: 0.31418 | val_0_auc: 0.93257 |  0:00:41s\n",
      "epoch 29 | loss: 0.31562 | val_0_auc: 0.93305 |  0:00:42s\n",
      "epoch 30 | loss: 0.31269 | val_0_auc: 0.93233 |  0:00:44s\n",
      "epoch 31 | loss: 0.33566 | val_0_auc: 0.94165 |  0:00:45s\n",
      "epoch 32 | loss: 0.3045  | val_0_auc: 0.95146 |  0:00:47s\n",
      "epoch 33 | loss: 0.31368 | val_0_auc: 0.95313 |  0:00:49s\n",
      "epoch 34 | loss: 0.28175 | val_0_auc: 0.93615 |  0:00:50s\n",
      "epoch 35 | loss: 0.31322 | val_0_auc: 0.94811 |  0:00:52s\n",
      "epoch 36 | loss: 0.28846 | val_0_auc: 0.94094 |  0:00:53s\n",
      "epoch 37 | loss: 0.29321 | val_0_auc: 0.94333 |  0:00:55s\n",
      "epoch 38 | loss: 0.29081 | val_0_auc: 0.94978 |  0:00:56s\n",
      "epoch 39 | loss: 0.29397 | val_0_auc: 0.94644 |  0:00:58s\n",
      "epoch 40 | loss: 0.30699 | val_0_auc: 0.9505  |  0:00:59s\n",
      "epoch 41 | loss: 0.294   | val_0_auc: 0.94285 |  0:01:00s\n",
      "epoch 42 | loss: 0.27801 | val_0_auc: 0.93974 |  0:01:02s\n",
      "epoch 43 | loss: 0.28002 | val_0_auc: 0.93639 |  0:01:03s\n",
      "epoch 44 | loss: 0.26072 | val_0_auc: 0.91368 |  0:01:04s\n",
      "epoch 45 | loss: 0.28315 | val_0_auc: 0.91463 |  0:01:06s\n",
      "epoch 46 | loss: 0.30386 | val_0_auc: 0.93998 |  0:01:07s\n",
      "epoch 47 | loss: 0.27572 | val_0_auc: 0.92205 |  0:01:08s\n",
      "epoch 48 | loss: 0.27211 | val_0_auc: 0.94142 |  0:01:10s\n",
      "epoch 49 | loss: 0.31326 | val_0_auc: 0.94668 |  0:01:11s\n",
      "epoch 50 | loss: 0.28558 | val_0_auc: 0.93783 |  0:01:12s\n",
      "epoch 51 | loss: 0.27017 | val_0_auc: 0.93783 |  0:01:14s\n",
      "epoch 52 | loss: 0.27242 | val_0_auc: 0.94046 |  0:01:15s\n",
      "epoch 53 | loss: 0.26948 | val_0_auc: 0.92109 |  0:01:17s\n",
      "epoch 54 | loss: 0.28166 | val_0_auc: 0.92133 |  0:01:19s\n",
      "epoch 55 | loss: 0.28604 | val_0_auc: 0.93544 |  0:01:21s\n",
      "epoch 56 | loss: 0.27663 | val_0_auc: 0.94022 |  0:01:22s\n",
      "epoch 57 | loss: 0.28568 | val_0_auc: 0.9187  |  0:01:24s\n",
      "epoch 58 | loss: 0.28085 | val_0_auc: 0.92587 |  0:01:25s\n",
      "epoch 59 | loss: 0.27094 | val_0_auc: 0.92085 |  0:01:26s\n",
      "\n",
      "Early stopping occurred at epoch 59 with best_epoch = 9 and best_val_0_auc = 0.95385\n",
      "\n",
      "Fold 2 Test Metrics:\n",
      "Accuracy: 0.9239\n",
      "F1 Score: 0.9320\n",
      "Precision: 0.9231\n",
      "Recall: 0.9412\n",
      "AUPRC: 0.9598\n",
      "\n",
      "==================================================\n",
      "Fold 3/10\n",
      "==================================================\n",
      "epoch 0  | loss: 0.55817 | val_0_auc: 0.8319  |  0:00:01s\n",
      "epoch 1  | loss: 0.45776 | val_0_auc: 0.85748 |  0:00:02s\n",
      "epoch 2  | loss: 0.41386 | val_0_auc: 0.86442 |  0:00:03s\n",
      "epoch 3  | loss: 0.41314 | val_0_auc: 0.88666 |  0:00:05s\n",
      "epoch 4  | loss: 0.40298 | val_0_auc: 0.85222 |  0:00:06s\n",
      "epoch 5  | loss: 0.38246 | val_0_auc: 0.88642 |  0:00:07s\n",
      "epoch 6  | loss: 0.37312 | val_0_auc: 0.89718 |  0:00:08s\n",
      "epoch 7  | loss: 0.36817 | val_0_auc: 0.89264 |  0:00:10s\n",
      "epoch 8  | loss: 0.36253 | val_0_auc: 0.91511 |  0:00:11s\n",
      "epoch 9  | loss: 0.34206 | val_0_auc: 0.90913 |  0:00:12s\n",
      "epoch 10 | loss: 0.3548  | val_0_auc: 0.90818 |  0:00:13s\n",
      "epoch 11 | loss: 0.35521 | val_0_auc: 0.90531 |  0:00:15s\n",
      "epoch 12 | loss: 0.34496 | val_0_auc: 0.89383 |  0:00:16s\n",
      "epoch 13 | loss: 0.34442 | val_0_auc: 0.91248 |  0:00:18s\n",
      "epoch 14 | loss: 0.33888 | val_0_auc: 0.90555 |  0:00:19s\n",
      "epoch 15 | loss: 0.33968 | val_0_auc: 0.86944 |  0:00:20s\n",
      "epoch 16 | loss: 0.32575 | val_0_auc: 0.90913 |  0:00:22s\n",
      "epoch 17 | loss: 0.35053 | val_0_auc: 0.92253 |  0:00:23s\n",
      "epoch 18 | loss: 0.32973 | val_0_auc: 0.9034  |  0:00:24s\n",
      "epoch 19 | loss: 0.31528 | val_0_auc: 0.90244 |  0:00:26s\n",
      "epoch 20 | loss: 0.32869 | val_0_auc: 0.91631 |  0:00:28s\n",
      "epoch 21 | loss: 0.32823 | val_0_auc: 0.90866 |  0:00:29s\n",
      "epoch 22 | loss: 0.30535 | val_0_auc: 0.90961 |  0:00:31s\n",
      "epoch 23 | loss: 0.31208 | val_0_auc: 0.90053 |  0:00:32s\n",
      "epoch 24 | loss: 0.3328  | val_0_auc: 0.88187 |  0:00:33s\n",
      "epoch 25 | loss: 0.32761 | val_0_auc: 0.90005 |  0:00:34s\n",
      "epoch 26 | loss: 0.33562 | val_0_auc: 0.90961 |  0:00:36s\n",
      "epoch 27 | loss: 0.32777 | val_0_auc: 0.90531 |  0:00:37s\n",
      "epoch 28 | loss: 0.33373 | val_0_auc: 0.90531 |  0:00:38s\n",
      "epoch 29 | loss: 0.32197 | val_0_auc: 0.91942 |  0:00:40s\n",
      "epoch 30 | loss: 0.33483 | val_0_auc: 0.89813 |  0:00:41s\n",
      "epoch 31 | loss: 0.3269  | val_0_auc: 0.8967  |  0:00:42s\n",
      "epoch 32 | loss: 0.30312 | val_0_auc: 0.90292 |  0:00:43s\n",
      "epoch 33 | loss: 0.28844 | val_0_auc: 0.89646 |  0:00:45s\n",
      "epoch 34 | loss: 0.31561 | val_0_auc: 0.88642 |  0:00:46s\n",
      "epoch 35 | loss: 0.31435 | val_0_auc: 0.85175 |  0:00:47s\n",
      "epoch 36 | loss: 0.28684 | val_0_auc: 0.88522 |  0:00:49s\n",
      "epoch 37 | loss: 0.31903 | val_0_auc: 0.90244 |  0:00:50s\n",
      "epoch 38 | loss: 0.29576 | val_0_auc: 0.90961 |  0:00:51s\n",
      "epoch 39 | loss: 0.32156 | val_0_auc: 0.86944 |  0:00:53s\n",
      "epoch 40 | loss: 0.32018 | val_0_auc: 0.86561 |  0:00:54s\n",
      "epoch 41 | loss: 0.29632 | val_0_auc: 0.8759  |  0:00:55s\n",
      "epoch 42 | loss: 0.30652 | val_0_auc: 0.91009 |  0:00:57s\n",
      "epoch 43 | loss: 0.30136 | val_0_auc: 0.8814  |  0:00:58s\n",
      "epoch 44 | loss: 0.29039 | val_0_auc: 0.92229 |  0:01:00s\n",
      "epoch 45 | loss: 0.29949 | val_0_auc: 0.94261 |  0:01:02s\n",
      "epoch 46 | loss: 0.29255 | val_0_auc: 0.9407  |  0:01:03s\n",
      "epoch 47 | loss: 0.28989 | val_0_auc: 0.92396 |  0:01:05s\n",
      "epoch 48 | loss: 0.27534 | val_0_auc: 0.9077  |  0:01:06s\n",
      "epoch 49 | loss: 0.29754 | val_0_auc: 0.91487 |  0:01:08s\n",
      "epoch 50 | loss: 0.28912 | val_0_auc: 0.92994 |  0:01:09s\n",
      "epoch 51 | loss: 0.29786 | val_0_auc: 0.88379 |  0:01:10s\n",
      "epoch 52 | loss: 0.29652 | val_0_auc: 0.90674 |  0:01:12s\n",
      "epoch 53 | loss: 0.27058 | val_0_auc: 0.90961 |  0:01:14s\n",
      "epoch 54 | loss: 0.29232 | val_0_auc: 0.92037 |  0:01:15s\n",
      "epoch 55 | loss: 0.26674 | val_0_auc: 0.90722 |  0:01:17s\n",
      "epoch 56 | loss: 0.27594 | val_0_auc: 0.91057 |  0:01:18s\n",
      "epoch 57 | loss: 0.27303 | val_0_auc: 0.90483 |  0:01:20s\n",
      "epoch 58 | loss: 0.28273 | val_0_auc: 0.90674 |  0:01:21s\n",
      "epoch 59 | loss: 0.28642 | val_0_auc: 0.92061 |  0:01:22s\n",
      "epoch 60 | loss: 0.27727 | val_0_auc: 0.92157 |  0:01:24s\n",
      "epoch 61 | loss: 0.31806 | val_0_auc: 0.87901 |  0:01:25s\n",
      "epoch 62 | loss: 0.28997 | val_0_auc: 0.91344 |  0:01:26s\n",
      "epoch 63 | loss: 0.26245 | val_0_auc: 0.91989 |  0:01:29s\n",
      "epoch 64 | loss: 0.27759 | val_0_auc: 0.91511 |  0:01:30s\n",
      "epoch 65 | loss: 0.30379 | val_0_auc: 0.90005 |  0:01:33s\n",
      "epoch 66 | loss: 0.28457 | val_0_auc: 0.91535 |  0:01:34s\n",
      "epoch 67 | loss: 0.27157 | val_0_auc: 0.91081 |  0:01:35s\n",
      "epoch 68 | loss: 0.29907 | val_0_auc: 0.90124 |  0:01:37s\n",
      "epoch 69 | loss: 0.31522 | val_0_auc: 0.92922 |  0:01:38s\n",
      "epoch 70 | loss: 0.30086 | val_0_auc: 0.93592 |  0:01:39s\n",
      "epoch 71 | loss: 0.3048  | val_0_auc: 0.91822 |  0:01:42s\n",
      "epoch 72 | loss: 0.30083 | val_0_auc: 0.91631 |  0:01:44s\n",
      "epoch 73 | loss: 0.26798 | val_0_auc: 0.93448 |  0:01:45s\n",
      "epoch 74 | loss: 0.26532 | val_0_auc: 0.91344 |  0:01:47s\n",
      "epoch 75 | loss: 0.27751 | val_0_auc: 0.90387 |  0:01:49s\n",
      "epoch 76 | loss: 0.29244 | val_0_auc: 0.92348 |  0:01:51s\n",
      "epoch 77 | loss: 0.28581 | val_0_auc: 0.93352 |  0:01:52s\n",
      "epoch 78 | loss: 0.29288 | val_0_auc: 0.93687 |  0:01:54s\n",
      "epoch 79 | loss: 0.28231 | val_0_auc: 0.92994 |  0:01:55s\n",
      "epoch 80 | loss: 0.30929 | val_0_auc: 0.94165 |  0:01:57s\n",
      "epoch 81 | loss: 0.32958 | val_0_auc: 0.91822 |  0:01:59s\n",
      "epoch 82 | loss: 0.2893  | val_0_auc: 0.923   |  0:02:00s\n",
      "epoch 83 | loss: 0.30406 | val_0_auc: 0.90746 |  0:02:02s\n",
      "epoch 84 | loss: 0.31189 | val_0_auc: 0.93329 |  0:02:04s\n",
      "epoch 85 | loss: 0.26304 | val_0_auc: 0.92826 |  0:02:05s\n",
      "epoch 86 | loss: 0.28138 | val_0_auc: 0.93161 |  0:02:07s\n",
      "epoch 87 | loss: 0.29056 | val_0_auc: 0.93544 |  0:02:09s\n",
      "epoch 88 | loss: 0.26517 | val_0_auc: 0.93209 |  0:02:10s\n",
      "epoch 89 | loss: 0.26482 | val_0_auc: 0.91583 |  0:02:12s\n",
      "epoch 90 | loss: 0.25378 | val_0_auc: 0.90483 |  0:02:14s\n",
      "epoch 91 | loss: 0.25892 | val_0_auc: 0.90626 |  0:02:16s\n",
      "epoch 92 | loss: 0.25673 | val_0_auc: 0.92874 |  0:02:17s\n",
      "epoch 93 | loss: 0.26832 | val_0_auc: 0.92444 |  0:02:19s\n",
      "epoch 94 | loss: 0.27813 | val_0_auc: 0.91631 |  0:02:20s\n",
      "epoch 95 | loss: 0.27321 | val_0_auc: 0.93831 |  0:02:22s\n",
      "\n",
      "Early stopping occurred at epoch 95 with best_epoch = 45 and best_val_0_auc = 0.94261\n",
      "\n",
      "Fold 3 Test Metrics:\n",
      "Accuracy: 0.8587\n",
      "F1 Score: 0.8829\n",
      "Precision: 0.8167\n",
      "Recall: 0.9608\n",
      "AUPRC: 0.9451\n",
      "\n",
      "==================================================\n",
      "Fold 4/10\n",
      "==================================================\n",
      "epoch 0  | loss: 0.58006 | val_0_auc: 0.84027 |  0:00:01s\n",
      "epoch 1  | loss: 0.45028 | val_0_auc: 0.85509 |  0:00:03s\n",
      "epoch 2  | loss: 0.41627 | val_0_auc: 0.86729 |  0:00:05s\n",
      "epoch 3  | loss: 0.37213 | val_0_auc: 0.94763 |  0:00:07s\n",
      "epoch 4  | loss: 0.36096 | val_0_auc: 0.95696 |  0:00:09s\n",
      "epoch 5  | loss: 0.35571 | val_0_auc: 0.93592 |  0:00:11s\n",
      "epoch 6  | loss: 0.38281 | val_0_auc: 0.90937 |  0:00:13s\n",
      "epoch 7  | loss: 0.34639 | val_0_auc: 0.93185 |  0:00:15s\n",
      "epoch 8  | loss: 0.35603 | val_0_auc: 0.92253 |  0:00:17s\n",
      "epoch 9  | loss: 0.34849 | val_0_auc: 0.91033 |  0:00:19s\n",
      "epoch 10 | loss: 0.34708 | val_0_auc: 0.90124 |  0:00:20s\n",
      "epoch 11 | loss: 0.347   | val_0_auc: 0.91416 |  0:00:22s\n",
      "epoch 12 | loss: 0.3718  | val_0_auc: 0.90794 |  0:00:23s\n",
      "epoch 13 | loss: 0.36923 | val_0_auc: 0.90507 |  0:00:25s\n",
      "epoch 14 | loss: 0.346   | val_0_auc: 0.89574 |  0:00:27s\n",
      "epoch 15 | loss: 0.353   | val_0_auc: 0.93089 |  0:00:29s\n",
      "epoch 16 | loss: 0.32867 | val_0_auc: 0.945   |  0:00:31s\n",
      "epoch 17 | loss: 0.34637 | val_0_auc: 0.9242  |  0:00:32s\n",
      "epoch 18 | loss: 0.34879 | val_0_auc: 0.92372 |  0:00:34s\n",
      "epoch 19 | loss: 0.32368 | val_0_auc: 0.93639 |  0:00:36s\n",
      "epoch 20 | loss: 0.32107 | val_0_auc: 0.92492 |  0:00:38s\n",
      "epoch 21 | loss: 0.33393 | val_0_auc: 0.92444 |  0:00:40s\n",
      "epoch 22 | loss: 0.31705 | val_0_auc: 0.92492 |  0:00:42s\n",
      "epoch 23 | loss: 0.32601 | val_0_auc: 0.934   |  0:00:43s\n",
      "epoch 24 | loss: 0.30958 | val_0_auc: 0.93879 |  0:00:45s\n",
      "epoch 25 | loss: 0.32152 | val_0_auc: 0.93879 |  0:00:47s\n",
      "epoch 26 | loss: 0.3427  | val_0_auc: 0.93137 |  0:00:48s\n",
      "epoch 27 | loss: 0.35361 | val_0_auc: 0.93161 |  0:00:50s\n",
      "epoch 28 | loss: 0.35009 | val_0_auc: 0.93066 |  0:00:51s\n",
      "epoch 29 | loss: 0.32792 | val_0_auc: 0.92635 |  0:00:53s\n",
      "epoch 30 | loss: 0.34655 | val_0_auc: 0.92109 |  0:00:55s\n",
      "epoch 31 | loss: 0.30811 | val_0_auc: 0.94309 |  0:00:57s\n",
      "epoch 32 | loss: 0.28055 | val_0_auc: 0.94883 |  0:00:58s\n",
      "epoch 33 | loss: 0.32765 | val_0_auc: 0.94883 |  0:01:00s\n",
      "epoch 34 | loss: 0.3084  | val_0_auc: 0.94931 |  0:01:01s\n",
      "epoch 35 | loss: 0.30312 | val_0_auc: 0.93018 |  0:01:03s\n",
      "epoch 36 | loss: 0.3014  | val_0_auc: 0.93018 |  0:01:04s\n",
      "epoch 37 | loss: 0.317   | val_0_auc: 0.92205 |  0:01:05s\n",
      "epoch 38 | loss: 0.29984 | val_0_auc: 0.934   |  0:01:07s\n",
      "epoch 39 | loss: 0.30536 | val_0_auc: 0.93305 |  0:01:08s\n",
      "epoch 40 | loss: 0.28849 | val_0_auc: 0.92587 |  0:01:09s\n",
      "epoch 41 | loss: 0.30357 | val_0_auc: 0.93974 |  0:01:10s\n",
      "epoch 42 | loss: 0.32427 | val_0_auc: 0.93974 |  0:01:11s\n",
      "epoch 43 | loss: 0.32465 | val_0_auc: 0.95122 |  0:01:13s\n",
      "epoch 44 | loss: 0.29539 | val_0_auc: 0.94978 |  0:01:14s\n",
      "epoch 45 | loss: 0.29854 | val_0_auc: 0.94357 |  0:01:15s\n",
      "epoch 46 | loss: 0.28095 | val_0_auc: 0.94165 |  0:01:16s\n",
      "epoch 47 | loss: 0.29525 | val_0_auc: 0.91583 |  0:01:18s\n",
      "epoch 48 | loss: 0.29752 | val_0_auc: 0.93018 |  0:01:19s\n",
      "epoch 49 | loss: 0.28682 | val_0_auc: 0.91463 |  0:01:20s\n",
      "epoch 50 | loss: 0.29957 | val_0_auc: 0.92324 |  0:01:21s\n",
      "epoch 51 | loss: 0.29772 | val_0_auc: 0.9285  |  0:01:23s\n",
      "epoch 52 | loss: 0.30579 | val_0_auc: 0.92468 |  0:01:24s\n",
      "epoch 53 | loss: 0.29177 | val_0_auc: 0.94046 |  0:01:25s\n",
      "epoch 54 | loss: 0.2941  | val_0_auc: 0.91894 |  0:01:27s\n",
      "\n",
      "Early stopping occurred at epoch 54 with best_epoch = 4 and best_val_0_auc = 0.95696\n",
      "\n",
      "Fold 4 Test Metrics:\n",
      "Accuracy: 0.8696\n",
      "F1 Score: 0.8776\n",
      "Precision: 0.9149\n",
      "Recall: 0.8431\n",
      "AUPRC: 0.9703\n",
      "\n",
      "==================================================\n",
      "Fold 5/10\n",
      "==================================================\n",
      "epoch 0  | loss: 0.5505  | val_0_auc: 0.81875 |  0:00:01s\n",
      "epoch 1  | loss: 0.46077 | val_0_auc: 0.88666 |  0:00:02s\n",
      "epoch 2  | loss: 0.44052 | val_0_auc: 0.8869  |  0:00:04s\n",
      "epoch 3  | loss: 0.40476 | val_0_auc: 0.90842 |  0:00:06s\n",
      "epoch 4  | loss: 0.41259 | val_0_auc: 0.86824 |  0:00:08s\n",
      "epoch 5  | loss: 0.38042 | val_0_auc: 0.90579 |  0:00:10s\n",
      "epoch 6  | loss: 0.36087 | val_0_auc: 0.92396 |  0:00:12s\n",
      "epoch 7  | loss: 0.35759 | val_0_auc: 0.93066 |  0:00:13s\n",
      "epoch 8  | loss: 0.36695 | val_0_auc: 0.92874 |  0:00:15s\n",
      "epoch 9  | loss: 0.35648 | val_0_auc: 0.92492 |  0:00:17s\n",
      "epoch 10 | loss: 0.34031 | val_0_auc: 0.90985 |  0:00:19s\n",
      "epoch 11 | loss: 0.3384  | val_0_auc: 0.92396 |  0:00:20s\n",
      "epoch 12 | loss: 0.34865 | val_0_auc: 0.93735 |  0:00:22s\n",
      "epoch 13 | loss: 0.35427 | val_0_auc: 0.93113 |  0:00:24s\n",
      "epoch 14 | loss: 0.34274 | val_0_auc: 0.91918 |  0:00:25s\n",
      "epoch 15 | loss: 0.3224  | val_0_auc: 0.92539 |  0:00:27s\n",
      "epoch 16 | loss: 0.33296 | val_0_auc: 0.92587 |  0:00:29s\n",
      "epoch 17 | loss: 0.3324  | val_0_auc: 0.8955  |  0:00:31s\n",
      "epoch 18 | loss: 0.32771 | val_0_auc: 0.90531 |  0:00:32s\n",
      "epoch 19 | loss: 0.33075 | val_0_auc: 0.93735 |  0:00:33s\n",
      "epoch 20 | loss: 0.31722 | val_0_auc: 0.93018 |  0:00:35s\n",
      "epoch 21 | loss: 0.31795 | val_0_auc: 0.94094 |  0:00:36s\n",
      "epoch 22 | loss: 0.33116 | val_0_auc: 0.93161 |  0:00:37s\n",
      "epoch 23 | loss: 0.32504 | val_0_auc: 0.92587 |  0:00:39s\n",
      "epoch 24 | loss: 0.32075 | val_0_auc: 0.934   |  0:00:40s\n",
      "epoch 25 | loss: 0.30971 | val_0_auc: 0.912   |  0:00:41s\n",
      "epoch 26 | loss: 0.30866 | val_0_auc: 0.90913 |  0:00:42s\n",
      "epoch 27 | loss: 0.32317 | val_0_auc: 0.91774 |  0:00:44s\n",
      "epoch 28 | loss: 0.30016 | val_0_auc: 0.91726 |  0:00:45s\n",
      "epoch 29 | loss: 0.33074 | val_0_auc: 0.93879 |  0:00:46s\n",
      "epoch 30 | loss: 0.30811 | val_0_auc: 0.93209 |  0:00:47s\n",
      "epoch 31 | loss: 0.32693 | val_0_auc: 0.91559 |  0:00:48s\n",
      "epoch 32 | loss: 0.32578 | val_0_auc: 0.91774 |  0:00:50s\n",
      "epoch 33 | loss: 0.32193 | val_0_auc: 0.91846 |  0:00:51s\n",
      "epoch 34 | loss: 0.30818 | val_0_auc: 0.92826 |  0:00:52s\n",
      "epoch 35 | loss: 0.28448 | val_0_auc: 0.93089 |  0:00:53s\n",
      "epoch 36 | loss: 0.31494 | val_0_auc: 0.92468 |  0:00:55s\n",
      "epoch 37 | loss: 0.31658 | val_0_auc: 0.93281 |  0:00:56s\n",
      "epoch 38 | loss: 0.30065 | val_0_auc: 0.94548 |  0:00:57s\n",
      "epoch 39 | loss: 0.30284 | val_0_auc: 0.93783 |  0:00:58s\n",
      "epoch 40 | loss: 0.3052  | val_0_auc: 0.94476 |  0:01:00s\n",
      "epoch 41 | loss: 0.2924  | val_0_auc: 0.94309 |  0:01:01s\n",
      "epoch 42 | loss: 0.3236  | val_0_auc: 0.945   |  0:01:02s\n",
      "epoch 43 | loss: 0.29833 | val_0_auc: 0.94118 |  0:01:04s\n",
      "epoch 44 | loss: 0.29978 | val_0_auc: 0.9395  |  0:01:05s\n",
      "epoch 45 | loss: 0.32024 | val_0_auc: 0.94357 |  0:01:07s\n",
      "epoch 46 | loss: 0.30705 | val_0_auc: 0.93089 |  0:01:08s\n",
      "epoch 47 | loss: 0.31397 | val_0_auc: 0.91774 |  0:01:09s\n",
      "epoch 48 | loss: 0.31132 | val_0_auc: 0.92181 |  0:01:10s\n",
      "epoch 49 | loss: 0.29724 | val_0_auc: 0.93376 |  0:01:11s\n",
      "epoch 50 | loss: 0.29891 | val_0_auc: 0.93639 |  0:01:13s\n",
      "epoch 51 | loss: 0.29112 | val_0_auc: 0.934   |  0:01:14s\n",
      "epoch 52 | loss: 0.29372 | val_0_auc: 0.93592 |  0:01:15s\n",
      "epoch 53 | loss: 0.28902 | val_0_auc: 0.93831 |  0:01:16s\n",
      "epoch 54 | loss: 0.29444 | val_0_auc: 0.94022 |  0:01:18s\n",
      "epoch 55 | loss: 0.31867 | val_0_auc: 0.92013 |  0:01:19s\n",
      "epoch 56 | loss: 0.30714 | val_0_auc: 0.91918 |  0:01:20s\n",
      "epoch 57 | loss: 0.29406 | val_0_auc: 0.93544 |  0:01:21s\n",
      "epoch 58 | loss: 0.28108 | val_0_auc: 0.92826 |  0:01:22s\n",
      "epoch 59 | loss: 0.28802 | val_0_auc: 0.93496 |  0:01:24s\n",
      "epoch 60 | loss: 0.26345 | val_0_auc: 0.94835 |  0:01:25s\n",
      "epoch 61 | loss: 0.2806  | val_0_auc: 0.94692 |  0:01:26s\n",
      "epoch 62 | loss: 0.28457 | val_0_auc: 0.95935 |  0:01:27s\n",
      "epoch 63 | loss: 0.26996 | val_0_auc: 0.94548 |  0:01:28s\n",
      "epoch 64 | loss: 0.28351 | val_0_auc: 0.93783 |  0:01:30s\n",
      "epoch 65 | loss: 0.27982 | val_0_auc: 0.94452 |  0:01:31s\n",
      "epoch 66 | loss: 0.28622 | val_0_auc: 0.93879 |  0:01:32s\n",
      "epoch 67 | loss: 0.28991 | val_0_auc: 0.93831 |  0:01:33s\n",
      "epoch 68 | loss: 0.26477 | val_0_auc: 0.92731 |  0:01:35s\n",
      "epoch 69 | loss: 0.284   | val_0_auc: 0.91296 |  0:01:36s\n",
      "epoch 70 | loss: 0.31098 | val_0_auc: 0.92539 |  0:01:37s\n",
      "epoch 71 | loss: 0.29042 | val_0_auc: 0.93161 |  0:01:38s\n",
      "epoch 72 | loss: 0.26262 | val_0_auc: 0.92587 |  0:01:40s\n",
      "epoch 73 | loss: 0.2854  | val_0_auc: 0.934   |  0:01:41s\n",
      "epoch 74 | loss: 0.25833 | val_0_auc: 0.92922 |  0:01:41s\n",
      "epoch 75 | loss: 0.25908 | val_0_auc: 0.92683 |  0:01:42s\n",
      "epoch 76 | loss: 0.29713 | val_0_auc: 0.93352 |  0:01:43s\n",
      "epoch 77 | loss: 0.29632 | val_0_auc: 0.93974 |  0:01:45s\n",
      "epoch 78 | loss: 0.26637 | val_0_auc: 0.93687 |  0:01:46s\n",
      "epoch 79 | loss: 0.26866 | val_0_auc: 0.93687 |  0:01:47s\n",
      "epoch 80 | loss: 0.27891 | val_0_auc: 0.91966 |  0:01:48s\n",
      "epoch 81 | loss: 0.3042  | val_0_auc: 0.93305 |  0:01:49s\n",
      "epoch 82 | loss: 0.2975  | val_0_auc: 0.93305 |  0:01:50s\n",
      "epoch 83 | loss: 0.27044 | val_0_auc: 0.934   |  0:01:51s\n",
      "epoch 84 | loss: 0.24743 | val_0_auc: 0.94357 |  0:01:52s\n",
      "epoch 85 | loss: 0.25403 | val_0_auc: 0.92922 |  0:01:53s\n",
      "epoch 86 | loss: 0.25265 | val_0_auc: 0.9297  |  0:01:54s\n",
      "epoch 87 | loss: 0.26904 | val_0_auc: 0.93257 |  0:01:56s\n",
      "epoch 88 | loss: 0.26909 | val_0_auc: 0.92157 |  0:01:57s\n",
      "epoch 89 | loss: 0.28004 | val_0_auc: 0.94022 |  0:01:58s\n",
      "epoch 90 | loss: 0.29243 | val_0_auc: 0.92492 |  0:01:59s\n",
      "epoch 91 | loss: 0.29229 | val_0_auc: 0.92683 |  0:02:00s\n",
      "epoch 92 | loss: 0.27281 | val_0_auc: 0.93448 |  0:02:01s\n",
      "epoch 93 | loss: 0.2704  | val_0_auc: 0.93544 |  0:02:02s\n",
      "epoch 94 | loss: 0.26159 | val_0_auc: 0.93352 |  0:02:03s\n",
      "epoch 95 | loss: 0.26825 | val_0_auc: 0.92874 |  0:02:03s\n",
      "epoch 96 | loss: 0.26437 | val_0_auc: 0.92922 |  0:02:04s\n",
      "epoch 97 | loss: 0.24087 | val_0_auc: 0.94022 |  0:02:05s\n",
      "epoch 98 | loss: 0.2657  | val_0_auc: 0.94309 |  0:02:06s\n",
      "epoch 99 | loss: 0.26505 | val_0_auc: 0.94596 |  0:02:07s\n",
      "Stop training because you reached max_epochs = 100 with best_epoch = 62 and best_val_0_auc = 0.95935\n",
      "\n",
      "Fold 5 Test Metrics:\n",
      "Accuracy: 0.8913\n",
      "F1 Score: 0.9020\n",
      "Precision: 0.9020\n",
      "Recall: 0.9020\n",
      "AUPRC: 0.9687\n",
      "\n",
      "==================================================\n",
      "Fold 6/10\n",
      "==================================================\n",
      "epoch 0  | loss: 0.59501 | val_0_auc: 0.88283 |  0:00:00s\n",
      "epoch 1  | loss: 0.43562 | val_0_auc: 0.87494 |  0:00:01s\n",
      "epoch 2  | loss: 0.42116 | val_0_auc: 0.85557 |  0:00:02s\n",
      "epoch 3  | loss: 0.41595 | val_0_auc: 0.86657 |  0:00:03s\n",
      "epoch 4  | loss: 0.36081 | val_0_auc: 0.89742 |  0:00:03s\n",
      "epoch 5  | loss: 0.34779 | val_0_auc: 0.83309 |  0:00:04s\n",
      "epoch 6  | loss: 0.35125 | val_0_auc: 0.86992 |  0:00:05s\n",
      "epoch 7  | loss: 0.36829 | val_0_auc: 0.86322 |  0:00:05s\n",
      "epoch 8  | loss: 0.34437 | val_0_auc: 0.87661 |  0:00:06s\n",
      "epoch 9  | loss: 0.35932 | val_0_auc: 0.83525 |  0:00:07s\n",
      "epoch 10 | loss: 0.35288 | val_0_auc: 0.80392 |  0:00:08s\n",
      "epoch 11 | loss: 0.34482 | val_0_auc: 0.8417  |  0:00:09s\n",
      "epoch 12 | loss: 0.33475 | val_0_auc: 0.84744 |  0:00:10s\n",
      "epoch 13 | loss: 0.32507 | val_0_auc: 0.87064 |  0:00:10s\n",
      "epoch 14 | loss: 0.34387 | val_0_auc: 0.85868 |  0:00:11s\n",
      "epoch 15 | loss: 0.34361 | val_0_auc: 0.86251 |  0:00:12s\n",
      "epoch 16 | loss: 0.3301  | val_0_auc: 0.85772 |  0:00:13s\n",
      "epoch 17 | loss: 0.34855 | val_0_auc: 0.8582  |  0:00:14s\n",
      "epoch 18 | loss: 0.32913 | val_0_auc: 0.85127 |  0:00:15s\n",
      "epoch 19 | loss: 0.33157 | val_0_auc: 0.84529 |  0:00:16s\n",
      "epoch 20 | loss: 0.32862 | val_0_auc: 0.85725 |  0:00:17s\n",
      "epoch 21 | loss: 0.31997 | val_0_auc: 0.84625 |  0:00:18s\n",
      "epoch 22 | loss: 0.32198 | val_0_auc: 0.87781 |  0:00:19s\n",
      "epoch 23 | loss: 0.3047  | val_0_auc: 0.90124 |  0:00:20s\n",
      "epoch 24 | loss: 0.33011 | val_0_auc: 0.88211 |  0:00:20s\n",
      "epoch 25 | loss: 0.31435 | val_0_auc: 0.87111 |  0:00:21s\n",
      "epoch 26 | loss: 0.31879 | val_0_auc: 0.86394 |  0:00:21s\n",
      "epoch 27 | loss: 0.33278 | val_0_auc: 0.86705 |  0:00:22s\n",
      "epoch 28 | loss: 0.31204 | val_0_auc: 0.89479 |  0:00:23s\n",
      "epoch 29 | loss: 0.32161 | val_0_auc: 0.87351 |  0:00:23s\n",
      "epoch 30 | loss: 0.32355 | val_0_auc: 0.88331 |  0:00:24s\n",
      "epoch 31 | loss: 0.29923 | val_0_auc: 0.87542 |  0:00:25s\n",
      "epoch 32 | loss: 0.32175 | val_0_auc: 0.85175 |  0:00:25s\n",
      "epoch 33 | loss: 0.31502 | val_0_auc: 0.87829 |  0:00:26s\n",
      "epoch 34 | loss: 0.338   | val_0_auc: 0.86657 |  0:00:26s\n",
      "epoch 35 | loss: 0.31536 | val_0_auc: 0.85485 |  0:00:27s\n",
      "epoch 36 | loss: 0.32854 | val_0_auc: 0.85318 |  0:00:28s\n",
      "epoch 37 | loss: 0.32529 | val_0_auc: 0.84792 |  0:00:28s\n",
      "epoch 38 | loss: 0.32599 | val_0_auc: 0.85533 |  0:00:29s\n",
      "epoch 39 | loss: 0.31101 | val_0_auc: 0.86872 |  0:00:30s\n",
      "epoch 40 | loss: 0.33841 | val_0_auc: 0.84912 |  0:00:30s\n",
      "epoch 41 | loss: 0.31354 | val_0_auc: 0.85581 |  0:00:31s\n",
      "epoch 42 | loss: 0.30947 | val_0_auc: 0.86011 |  0:00:31s\n",
      "epoch 43 | loss: 0.30495 | val_0_auc: 0.85366 |  0:00:32s\n",
      "epoch 44 | loss: 0.32106 | val_0_auc: 0.85605 |  0:00:33s\n",
      "epoch 45 | loss: 0.29775 | val_0_auc: 0.85533 |  0:00:33s\n",
      "epoch 46 | loss: 0.29861 | val_0_auc: 0.85772 |  0:00:34s\n",
      "epoch 47 | loss: 0.30296 | val_0_auc: 0.8582  |  0:00:34s\n",
      "epoch 48 | loss: 0.32108 | val_0_auc: 0.83429 |  0:00:35s\n",
      "epoch 49 | loss: 0.31723 | val_0_auc: 0.85366 |  0:00:36s\n",
      "epoch 50 | loss: 0.31737 | val_0_auc: 0.81923 |  0:00:36s\n",
      "epoch 51 | loss: 0.3075  | val_0_auc: 0.83333 |  0:00:37s\n",
      "epoch 52 | loss: 0.30463 | val_0_auc: 0.84864 |  0:00:37s\n",
      "epoch 53 | loss: 0.29335 | val_0_auc: 0.86777 |  0:00:38s\n",
      "epoch 54 | loss: 0.30189 | val_0_auc: 0.86657 |  0:00:39s\n",
      "epoch 55 | loss: 0.32189 | val_0_auc: 0.86227 |  0:00:39s\n",
      "epoch 56 | loss: 0.31843 | val_0_auc: 0.87135 |  0:00:40s\n",
      "epoch 57 | loss: 0.32521 | val_0_auc: 0.83812 |  0:00:41s\n",
      "epoch 58 | loss: 0.30778 | val_0_auc: 0.85725 |  0:00:41s\n",
      "epoch 59 | loss: 0.31743 | val_0_auc: 0.85151 |  0:00:42s\n",
      "epoch 60 | loss: 0.35833 | val_0_auc: 0.88498 |  0:00:42s\n",
      "epoch 61 | loss: 0.33323 | val_0_auc: 0.86538 |  0:00:43s\n",
      "epoch 62 | loss: 0.32834 | val_0_auc: 0.86394 |  0:00:44s\n",
      "epoch 63 | loss: 0.33386 | val_0_auc: 0.87111 |  0:00:44s\n",
      "epoch 64 | loss: 0.32098 | val_0_auc: 0.87661 |  0:00:45s\n",
      "epoch 65 | loss: 0.33595 | val_0_auc: 0.89024 |  0:00:45s\n",
      "epoch 66 | loss: 0.32625 | val_0_auc: 0.87422 |  0:00:46s\n",
      "epoch 67 | loss: 0.32636 | val_0_auc: 0.86561 |  0:00:47s\n",
      "epoch 68 | loss: 0.31297 | val_0_auc: 0.86466 |  0:00:48s\n",
      "epoch 69 | loss: 0.31347 | val_0_auc: 0.86418 |  0:00:48s\n",
      "epoch 70 | loss: 0.29023 | val_0_auc: 0.87494 |  0:00:49s\n",
      "epoch 71 | loss: 0.29512 | val_0_auc: 0.87016 |  0:00:50s\n",
      "epoch 72 | loss: 0.31896 | val_0_auc: 0.88522 |  0:00:51s\n",
      "epoch 73 | loss: 0.28624 | val_0_auc: 0.87422 |  0:00:51s\n",
      "\n",
      "Early stopping occurred at epoch 73 with best_epoch = 23 and best_val_0_auc = 0.90124\n",
      "\n",
      "Fold 6 Test Metrics:\n",
      "Accuracy: 0.7935\n",
      "F1 Score: 0.8081\n",
      "Precision: 0.8333\n",
      "Recall: 0.7843\n",
      "AUPRC: 0.9228\n",
      "\n",
      "==================================================\n",
      "Fold 7/10\n",
      "==================================================\n",
      "epoch 0  | loss: 0.58432 | val_0_auc: 0.901   |  0:00:00s\n",
      "epoch 1  | loss: 0.46057 | val_0_auc: 0.91535 |  0:00:01s\n",
      "epoch 2  | loss: 0.41484 | val_0_auc: 0.9034  |  0:00:02s\n",
      "epoch 3  | loss: 0.40065 | val_0_auc: 0.9077  |  0:00:02s\n",
      "epoch 4  | loss: 0.40322 | val_0_auc: 0.88905 |  0:00:03s\n",
      "epoch 5  | loss: 0.40851 | val_0_auc: 0.88905 |  0:00:03s\n",
      "epoch 6  | loss: 0.40786 | val_0_auc: 0.92731 |  0:00:04s\n",
      "epoch 7  | loss: 0.42813 | val_0_auc: 0.87948 |  0:00:05s\n",
      "epoch 8  | loss: 0.40051 | val_0_auc: 0.88092 |  0:00:05s\n",
      "epoch 9  | loss: 0.40554 | val_0_auc: 0.88235 |  0:00:06s\n",
      "epoch 10 | loss: 0.37559 | val_0_auc: 0.92468 |  0:00:07s\n",
      "epoch 11 | loss: 0.35385 | val_0_auc: 0.92874 |  0:00:07s\n",
      "epoch 12 | loss: 0.37524 | val_0_auc: 0.90244 |  0:00:08s\n",
      "epoch 13 | loss: 0.35258 | val_0_auc: 0.8979  |  0:00:08s\n",
      "epoch 14 | loss: 0.35847 | val_0_auc: 0.89909 |  0:00:09s\n",
      "epoch 15 | loss: 0.37149 | val_0_auc: 0.90459 |  0:00:09s\n",
      "epoch 16 | loss: 0.34066 | val_0_auc: 0.92013 |  0:00:10s\n",
      "epoch 17 | loss: 0.35664 | val_0_auc: 0.91081 |  0:00:11s\n",
      "epoch 18 | loss: 0.34677 | val_0_auc: 0.91894 |  0:00:11s\n",
      "epoch 19 | loss: 0.34993 | val_0_auc: 0.92444 |  0:00:12s\n",
      "epoch 20 | loss: 0.34947 | val_0_auc: 0.91655 |  0:00:13s\n",
      "epoch 21 | loss: 0.33168 | val_0_auc: 0.91918 |  0:00:13s\n",
      "epoch 22 | loss: 0.32264 | val_0_auc: 0.92037 |  0:00:14s\n",
      "epoch 23 | loss: 0.34256 | val_0_auc: 0.90507 |  0:00:14s\n",
      "epoch 24 | loss: 0.33749 | val_0_auc: 0.91296 |  0:00:15s\n",
      "epoch 25 | loss: 0.33355 | val_0_auc: 0.90316 |  0:00:16s\n",
      "epoch 26 | loss: 0.33592 | val_0_auc: 0.90961 |  0:00:16s\n",
      "epoch 27 | loss: 0.32789 | val_0_auc: 0.92611 |  0:00:17s\n",
      "epoch 28 | loss: 0.33306 | val_0_auc: 0.92181 |  0:00:17s\n",
      "epoch 29 | loss: 0.3354  | val_0_auc: 0.92205 |  0:00:18s\n",
      "epoch 30 | loss: 0.30686 | val_0_auc: 0.91703 |  0:00:19s\n",
      "epoch 31 | loss: 0.32797 | val_0_auc: 0.91368 |  0:00:19s\n",
      "epoch 32 | loss: 0.3044  | val_0_auc: 0.90555 |  0:00:20s\n",
      "epoch 33 | loss: 0.31902 | val_0_auc: 0.91679 |  0:00:21s\n",
      "epoch 34 | loss: 0.32675 | val_0_auc: 0.91535 |  0:00:21s\n",
      "epoch 35 | loss: 0.32731 | val_0_auc: 0.90579 |  0:00:22s\n",
      "epoch 36 | loss: 0.31327 | val_0_auc: 0.90507 |  0:00:22s\n",
      "epoch 37 | loss: 0.31494 | val_0_auc: 0.90531 |  0:00:23s\n",
      "epoch 38 | loss: 0.30661 | val_0_auc: 0.91272 |  0:00:23s\n",
      "epoch 39 | loss: 0.30686 | val_0_auc: 0.90674 |  0:00:24s\n",
      "epoch 40 | loss: 0.2983  | val_0_auc: 0.90626 |  0:00:25s\n",
      "epoch 41 | loss: 0.30506 | val_0_auc: 0.91105 |  0:00:25s\n",
      "epoch 42 | loss: 0.30085 | val_0_auc: 0.9285  |  0:00:26s\n",
      "epoch 43 | loss: 0.3216  | val_0_auc: 0.93687 |  0:00:27s\n",
      "epoch 44 | loss: 0.33794 | val_0_auc: 0.93448 |  0:00:27s\n",
      "epoch 45 | loss: 0.32785 | val_0_auc: 0.93376 |  0:00:28s\n",
      "epoch 46 | loss: 0.33629 | val_0_auc: 0.912   |  0:00:28s\n",
      "epoch 47 | loss: 0.32201 | val_0_auc: 0.91248 |  0:00:29s\n",
      "epoch 48 | loss: 0.34589 | val_0_auc: 0.91966 |  0:00:30s\n",
      "epoch 49 | loss: 0.34012 | val_0_auc: 0.93089 |  0:00:31s\n",
      "epoch 50 | loss: 0.31741 | val_0_auc: 0.91583 |  0:00:31s\n",
      "epoch 51 | loss: 0.32062 | val_0_auc: 0.92348 |  0:00:32s\n",
      "epoch 52 | loss: 0.31457 | val_0_auc: 0.92516 |  0:00:33s\n",
      "epoch 53 | loss: 0.32923 | val_0_auc: 0.92563 |  0:00:33s\n",
      "epoch 54 | loss: 0.33055 | val_0_auc: 0.90674 |  0:00:34s\n",
      "epoch 55 | loss: 0.31091 | val_0_auc: 0.90387 |  0:00:34s\n",
      "epoch 56 | loss: 0.32041 | val_0_auc: 0.89359 |  0:00:35s\n",
      "epoch 57 | loss: 0.32694 | val_0_auc: 0.90435 |  0:00:36s\n",
      "epoch 58 | loss: 0.32335 | val_0_auc: 0.89742 |  0:00:36s\n",
      "epoch 59 | loss: 0.31891 | val_0_auc: 0.90387 |  0:00:37s\n",
      "epoch 60 | loss: 0.32203 | val_0_auc: 0.89718 |  0:00:37s\n",
      "epoch 61 | loss: 0.33177 | val_0_auc: 0.91942 |  0:00:38s\n",
      "epoch 62 | loss: 0.3321  | val_0_auc: 0.91272 |  0:00:39s\n",
      "epoch 63 | loss: 0.31141 | val_0_auc: 0.901   |  0:00:39s\n",
      "epoch 64 | loss: 0.3167  | val_0_auc: 0.91631 |  0:00:40s\n",
      "epoch 65 | loss: 0.31341 | val_0_auc: 0.91583 |  0:00:41s\n",
      "epoch 66 | loss: 0.30075 | val_0_auc: 0.912   |  0:00:41s\n",
      "epoch 67 | loss: 0.30827 | val_0_auc: 0.90722 |  0:00:42s\n",
      "epoch 68 | loss: 0.31311 | val_0_auc: 0.91416 |  0:00:42s\n",
      "epoch 69 | loss: 0.31457 | val_0_auc: 0.89646 |  0:00:43s\n",
      "epoch 70 | loss: 0.31794 | val_0_auc: 0.89933 |  0:00:44s\n",
      "epoch 71 | loss: 0.31316 | val_0_auc: 0.9022  |  0:00:44s\n",
      "epoch 72 | loss: 0.3088  | val_0_auc: 0.91822 |  0:00:45s\n",
      "epoch 73 | loss: 0.30222 | val_0_auc: 0.90268 |  0:00:46s\n",
      "epoch 74 | loss: 0.30342 | val_0_auc: 0.9132  |  0:00:46s\n",
      "epoch 75 | loss: 0.32628 | val_0_auc: 0.91129 |  0:00:47s\n",
      "epoch 76 | loss: 0.31136 | val_0_auc: 0.91368 |  0:00:48s\n",
      "epoch 77 | loss: 0.3096  | val_0_auc: 0.91846 |  0:00:48s\n",
      "epoch 78 | loss: 0.29051 | val_0_auc: 0.9065  |  0:00:49s\n",
      "epoch 79 | loss: 0.31838 | val_0_auc: 0.90985 |  0:00:50s\n",
      "epoch 80 | loss: 0.31251 | val_0_auc: 0.9175  |  0:00:50s\n",
      "epoch 81 | loss: 0.29104 | val_0_auc: 0.90842 |  0:00:51s\n",
      "epoch 82 | loss: 0.29463 | val_0_auc: 0.9065  |  0:00:51s\n",
      "epoch 83 | loss: 0.28396 | val_0_auc: 0.91129 |  0:00:52s\n",
      "epoch 84 | loss: 0.30305 | val_0_auc: 0.92372 |  0:00:52s\n",
      "epoch 85 | loss: 0.30147 | val_0_auc: 0.91009 |  0:00:53s\n",
      "epoch 86 | loss: 0.29526 | val_0_auc: 0.91176 |  0:00:54s\n",
      "epoch 87 | loss: 0.29917 | val_0_auc: 0.91966 |  0:00:54s\n",
      "epoch 88 | loss: 0.31042 | val_0_auc: 0.91248 |  0:00:55s\n",
      "epoch 89 | loss: 0.28802 | val_0_auc: 0.90961 |  0:00:55s\n",
      "epoch 90 | loss: 0.28516 | val_0_auc: 0.90913 |  0:00:56s\n",
      "epoch 91 | loss: 0.29919 | val_0_auc: 0.91774 |  0:00:57s\n",
      "epoch 92 | loss: 0.2964  | val_0_auc: 0.9034  |  0:00:57s\n",
      "epoch 93 | loss: 0.29078 | val_0_auc: 0.912   |  0:00:58s\n",
      "\n",
      "Early stopping occurred at epoch 93 with best_epoch = 43 and best_val_0_auc = 0.93687\n",
      "\n",
      "Fold 7 Test Metrics:\n",
      "Accuracy: 0.8804\n",
      "F1 Score: 0.8911\n",
      "Precision: 0.9000\n",
      "Recall: 0.8824\n",
      "AUPRC: 0.9431\n",
      "\n",
      "==================================================\n",
      "Fold 8/10\n",
      "==================================================\n",
      "epoch 0  | loss: 0.56848 | val_0_auc: 0.91463 |  0:00:00s\n",
      "epoch 1  | loss: 0.43783 | val_0_auc: 0.91009 |  0:00:01s\n",
      "epoch 2  | loss: 0.41453 | val_0_auc: 0.92061 |  0:00:02s\n",
      "epoch 3  | loss: 0.42432 | val_0_auc: 0.92587 |  0:00:02s\n",
      "epoch 4  | loss: 0.39761 | val_0_auc: 0.93926 |  0:00:03s\n",
      "epoch 5  | loss: 0.37328 | val_0_auc: 0.9285  |  0:00:03s\n",
      "epoch 6  | loss: 0.38257 | val_0_auc: 0.9407  |  0:00:04s\n",
      "epoch 7  | loss: 0.36175 | val_0_auc: 0.91033 |  0:00:05s\n",
      "epoch 8  | loss: 0.34745 | val_0_auc: 0.9407  |  0:00:05s\n",
      "epoch 9  | loss: 0.35835 | val_0_auc: 0.93926 |  0:00:06s\n",
      "epoch 10 | loss: 0.34341 | val_0_auc: 0.92731 |  0:00:07s\n",
      "epoch 11 | loss: 0.39647 | val_0_auc: 0.92539 |  0:00:08s\n",
      "epoch 12 | loss: 0.34448 | val_0_auc: 0.93042 |  0:00:09s\n",
      "epoch 13 | loss: 0.35301 | val_0_auc: 0.92587 |  0:00:10s\n",
      "epoch 14 | loss: 0.34372 | val_0_auc: 0.92683 |  0:00:11s\n",
      "epoch 15 | loss: 0.34233 | val_0_auc: 0.92898 |  0:00:12s\n",
      "epoch 16 | loss: 0.32969 | val_0_auc: 0.9144  |  0:00:13s\n",
      "epoch 17 | loss: 0.31964 | val_0_auc: 0.91511 |  0:00:14s\n",
      "epoch 18 | loss: 0.34335 | val_0_auc: 0.90961 |  0:00:15s\n",
      "epoch 19 | loss: 0.34379 | val_0_auc: 0.90913 |  0:00:16s\n",
      "epoch 20 | loss: 0.34604 | val_0_auc: 0.923   |  0:00:17s\n",
      "epoch 21 | loss: 0.33307 | val_0_auc: 0.93544 |  0:00:18s\n",
      "epoch 22 | loss: 0.34425 | val_0_auc: 0.92587 |  0:00:19s\n",
      "epoch 23 | loss: 0.32941 | val_0_auc: 0.93615 |  0:00:20s\n",
      "epoch 24 | loss: 0.33737 | val_0_auc: 0.95026 |  0:00:21s\n",
      "epoch 25 | loss: 0.33754 | val_0_auc: 0.95002 |  0:00:22s\n",
      "epoch 26 | loss: 0.32792 | val_0_auc: 0.94213 |  0:00:23s\n",
      "epoch 27 | loss: 0.32192 | val_0_auc: 0.93783 |  0:00:24s\n",
      "epoch 28 | loss: 0.30988 | val_0_auc: 0.93042 |  0:00:24s\n",
      "epoch 29 | loss: 0.32309 | val_0_auc: 0.9395  |  0:00:25s\n",
      "epoch 30 | loss: 0.32428 | val_0_auc: 0.94907 |  0:00:25s\n",
      "epoch 31 | loss: 0.32806 | val_0_auc: 0.93663 |  0:00:26s\n",
      "epoch 32 | loss: 0.33444 | val_0_auc: 0.92826 |  0:00:27s\n",
      "epoch 33 | loss: 0.32801 | val_0_auc: 0.91989 |  0:00:27s\n",
      "epoch 34 | loss: 0.33003 | val_0_auc: 0.92013 |  0:00:28s\n",
      "epoch 35 | loss: 0.32665 | val_0_auc: 0.92826 |  0:00:29s\n",
      "epoch 36 | loss: 0.31068 | val_0_auc: 0.92635 |  0:00:30s\n",
      "epoch 37 | loss: 0.31698 | val_0_auc: 0.93448 |  0:00:30s\n",
      "epoch 38 | loss: 0.32769 | val_0_auc: 0.94046 |  0:00:31s\n",
      "epoch 39 | loss: 0.33982 | val_0_auc: 0.93639 |  0:00:32s\n",
      "epoch 40 | loss: 0.3308  | val_0_auc: 0.93137 |  0:00:33s\n",
      "epoch 41 | loss: 0.32348 | val_0_auc: 0.93568 |  0:00:33s\n",
      "epoch 42 | loss: 0.3198  | val_0_auc: 0.93018 |  0:00:34s\n",
      "epoch 43 | loss: 0.31481 | val_0_auc: 0.934   |  0:00:35s\n",
      "epoch 44 | loss: 0.31291 | val_0_auc: 0.93137 |  0:00:35s\n",
      "epoch 45 | loss: 0.29455 | val_0_auc: 0.92563 |  0:00:36s\n",
      "epoch 46 | loss: 0.29839 | val_0_auc: 0.934   |  0:00:37s\n",
      "epoch 47 | loss: 0.29952 | val_0_auc: 0.93257 |  0:00:38s\n",
      "epoch 48 | loss: 0.31143 | val_0_auc: 0.91607 |  0:00:39s\n",
      "epoch 49 | loss: 0.30684 | val_0_auc: 0.93639 |  0:00:40s\n",
      "epoch 50 | loss: 0.32961 | val_0_auc: 0.92444 |  0:00:41s\n",
      "epoch 51 | loss: 0.30674 | val_0_auc: 0.92898 |  0:00:42s\n",
      "epoch 52 | loss: 0.30985 | val_0_auc: 0.93329 |  0:00:42s\n",
      "epoch 53 | loss: 0.30916 | val_0_auc: 0.93113 |  0:00:43s\n",
      "epoch 54 | loss: 0.28407 | val_0_auc: 0.92826 |  0:00:43s\n",
      "epoch 55 | loss: 0.31227 | val_0_auc: 0.91894 |  0:00:44s\n",
      "epoch 56 | loss: 0.32557 | val_0_auc: 0.91392 |  0:00:45s\n",
      "epoch 57 | loss: 0.31158 | val_0_auc: 0.92061 |  0:00:45s\n",
      "epoch 58 | loss: 0.3351  | val_0_auc: 0.93639 |  0:00:46s\n",
      "epoch 59 | loss: 0.29926 | val_0_auc: 0.91296 |  0:00:46s\n",
      "epoch 60 | loss: 0.3195  | val_0_auc: 0.92563 |  0:00:47s\n",
      "epoch 61 | loss: 0.31779 | val_0_auc: 0.91918 |  0:00:48s\n",
      "epoch 62 | loss: 0.30874 | val_0_auc: 0.9175  |  0:00:48s\n",
      "epoch 63 | loss: 0.28688 | val_0_auc: 0.91416 |  0:00:49s\n",
      "epoch 64 | loss: 0.2834  | val_0_auc: 0.92157 |  0:00:50s\n",
      "epoch 65 | loss: 0.2933  | val_0_auc: 0.9285  |  0:00:50s\n",
      "epoch 66 | loss: 0.28501 | val_0_auc: 0.92779 |  0:00:51s\n",
      "epoch 67 | loss: 0.29671 | val_0_auc: 0.93711 |  0:00:51s\n",
      "epoch 68 | loss: 0.30099 | val_0_auc: 0.93281 |  0:00:52s\n",
      "epoch 69 | loss: 0.30486 | val_0_auc: 0.93281 |  0:00:53s\n",
      "epoch 70 | loss: 0.31021 | val_0_auc: 0.94261 |  0:00:53s\n",
      "epoch 71 | loss: 0.28701 | val_0_auc: 0.93018 |  0:00:54s\n",
      "epoch 72 | loss: 0.29227 | val_0_auc: 0.93161 |  0:00:55s\n",
      "epoch 73 | loss: 0.29888 | val_0_auc: 0.92659 |  0:00:55s\n",
      "epoch 74 | loss: 0.28971 | val_0_auc: 0.91894 |  0:00:56s\n",
      "\n",
      "Early stopping occurred at epoch 74 with best_epoch = 24 and best_val_0_auc = 0.95026\n",
      "\n",
      "Fold 8 Test Metrics:\n",
      "Accuracy: 0.8696\n",
      "F1 Score: 0.8929\n",
      "Precision: 0.8197\n",
      "Recall: 0.9804\n",
      "AUPRC: 0.9513\n",
      "\n",
      "==================================================\n",
      "Fold 9/10\n",
      "==================================================\n",
      "epoch 0  | loss: 0.58511 | val_0_auc: 0.81073 |  0:00:00s\n",
      "epoch 1  | loss: 0.4175  | val_0_auc: 0.83268 |  0:00:01s\n",
      "epoch 2  | loss: 0.40076 | val_0_auc: 0.83927 |  0:00:02s\n",
      "epoch 3  | loss: 0.40937 | val_0_auc: 0.8422  |  0:00:03s\n",
      "epoch 4  | loss: 0.4014  | val_0_auc: 0.87171 |  0:00:03s\n",
      "epoch 5  | loss: 0.38053 | val_0_auc: 0.85951 |  0:00:04s\n",
      "epoch 6  | loss: 0.39366 | val_0_auc: 0.88829 |  0:00:05s\n",
      "epoch 7  | loss: 0.36856 | val_0_auc: 0.86634 |  0:00:05s\n",
      "epoch 8  | loss: 0.37537 | val_0_auc: 0.86439 |  0:00:06s\n",
      "epoch 9  | loss: 0.40213 | val_0_auc: 0.89366 |  0:00:06s\n",
      "epoch 10 | loss: 0.34426 | val_0_auc: 0.88341 |  0:00:07s\n",
      "epoch 11 | loss: 0.35605 | val_0_auc: 0.90878 |  0:00:08s\n",
      "epoch 12 | loss: 0.34188 | val_0_auc: 0.86341 |  0:00:08s\n",
      "epoch 13 | loss: 0.36662 | val_0_auc: 0.87073 |  0:00:09s\n",
      "epoch 14 | loss: 0.34621 | val_0_auc: 0.89561 |  0:00:09s\n",
      "epoch 15 | loss: 0.35285 | val_0_auc: 0.90341 |  0:00:10s\n",
      "epoch 16 | loss: 0.34499 | val_0_auc: 0.89049 |  0:00:11s\n",
      "epoch 17 | loss: 0.32543 | val_0_auc: 0.90098 |  0:00:11s\n",
      "epoch 18 | loss: 0.31013 | val_0_auc: 0.88098 |  0:00:12s\n",
      "epoch 19 | loss: 0.33085 | val_0_auc: 0.89659 |  0:00:13s\n",
      "epoch 20 | loss: 0.3161  | val_0_auc: 0.89756 |  0:00:13s\n",
      "epoch 21 | loss: 0.31042 | val_0_auc: 0.90585 |  0:00:14s\n",
      "epoch 22 | loss: 0.3182  | val_0_auc: 0.90146 |  0:00:14s\n",
      "epoch 23 | loss: 0.32523 | val_0_auc: 0.87951 |  0:00:15s\n",
      "epoch 24 | loss: 0.32617 | val_0_auc: 0.87488 |  0:00:16s\n",
      "epoch 25 | loss: 0.31631 | val_0_auc: 0.8761  |  0:00:16s\n",
      "epoch 26 | loss: 0.3248  | val_0_auc: 0.87659 |  0:00:17s\n",
      "epoch 27 | loss: 0.33376 | val_0_auc: 0.89659 |  0:00:17s\n",
      "epoch 28 | loss: 0.3072  | val_0_auc: 0.88829 |  0:00:18s\n",
      "epoch 29 | loss: 0.32003 | val_0_auc: 0.89317 |  0:00:19s\n",
      "epoch 30 | loss: 0.30571 | val_0_auc: 0.89268 |  0:00:20s\n",
      "epoch 31 | loss: 0.30154 | val_0_auc: 0.89561 |  0:00:20s\n",
      "epoch 32 | loss: 0.29455 | val_0_auc: 0.90293 |  0:00:21s\n",
      "epoch 33 | loss: 0.2961  | val_0_auc: 0.89854 |  0:00:22s\n",
      "epoch 34 | loss: 0.30639 | val_0_auc: 0.90049 |  0:00:22s\n",
      "epoch 35 | loss: 0.30177 | val_0_auc: 0.88293 |  0:00:23s\n",
      "epoch 36 | loss: 0.30197 | val_0_auc: 0.86488 |  0:00:24s\n",
      "epoch 37 | loss: 0.31528 | val_0_auc: 0.89073 |  0:00:24s\n",
      "epoch 38 | loss: 0.30148 | val_0_auc: 0.89317 |  0:00:25s\n",
      "epoch 39 | loss: 0.31015 | val_0_auc: 0.89463 |  0:00:25s\n",
      "epoch 40 | loss: 0.28945 | val_0_auc: 0.89512 |  0:00:26s\n",
      "epoch 41 | loss: 0.31014 | val_0_auc: 0.88878 |  0:00:27s\n",
      "epoch 42 | loss: 0.29182 | val_0_auc: 0.88732 |  0:00:27s\n",
      "epoch 43 | loss: 0.30871 | val_0_auc: 0.90878 |  0:00:28s\n",
      "epoch 44 | loss: 0.28945 | val_0_auc: 0.89439 |  0:00:28s\n",
      "epoch 45 | loss: 0.30012 | val_0_auc: 0.89634 |  0:00:29s\n",
      "epoch 46 | loss: 0.29956 | val_0_auc: 0.90244 |  0:00:29s\n",
      "epoch 47 | loss: 0.30522 | val_0_auc: 0.89073 |  0:00:30s\n",
      "epoch 48 | loss: 0.27958 | val_0_auc: 0.90146 |  0:00:30s\n",
      "epoch 49 | loss: 0.27915 | val_0_auc: 0.90098 |  0:00:31s\n",
      "epoch 50 | loss: 0.27554 | val_0_auc: 0.91122 |  0:00:32s\n",
      "epoch 51 | loss: 0.27993 | val_0_auc: 0.91122 |  0:00:32s\n",
      "epoch 52 | loss: 0.29332 | val_0_auc: 0.91439 |  0:00:33s\n",
      "epoch 53 | loss: 0.28114 | val_0_auc: 0.90756 |  0:00:34s\n",
      "epoch 54 | loss: 0.27725 | val_0_auc: 0.91634 |  0:00:34s\n",
      "epoch 55 | loss: 0.28395 | val_0_auc: 0.91049 |  0:00:35s\n",
      "epoch 56 | loss: 0.28243 | val_0_auc: 0.90683 |  0:00:35s\n",
      "epoch 57 | loss: 0.26874 | val_0_auc: 0.91537 |  0:00:36s\n",
      "epoch 58 | loss: 0.27546 | val_0_auc: 0.9161  |  0:00:37s\n",
      "epoch 59 | loss: 0.26825 | val_0_auc: 0.91707 |  0:00:37s\n",
      "epoch 60 | loss: 0.2708  | val_0_auc: 0.9161  |  0:00:38s\n",
      "epoch 61 | loss: 0.26172 | val_0_auc: 0.91268 |  0:00:38s\n",
      "epoch 62 | loss: 0.2445  | val_0_auc: 0.9061  |  0:00:39s\n",
      "epoch 63 | loss: 0.25911 | val_0_auc: 0.91268 |  0:00:39s\n",
      "epoch 64 | loss: 0.26688 | val_0_auc: 0.91171 |  0:00:40s\n",
      "epoch 65 | loss: 0.25743 | val_0_auc: 0.91098 |  0:00:41s\n",
      "epoch 66 | loss: 0.26359 | val_0_auc: 0.90049 |  0:00:41s\n",
      "epoch 67 | loss: 0.26026 | val_0_auc: 0.90756 |  0:00:42s\n",
      "epoch 68 | loss: 0.24327 | val_0_auc: 0.9     |  0:00:42s\n",
      "epoch 69 | loss: 0.2725  | val_0_auc: 0.90732 |  0:00:43s\n",
      "epoch 70 | loss: 0.26378 | val_0_auc: 0.90341 |  0:00:44s\n",
      "epoch 71 | loss: 0.25947 | val_0_auc: 0.9     |  0:00:44s\n",
      "epoch 72 | loss: 0.26646 | val_0_auc: 0.9     |  0:00:45s\n",
      "epoch 73 | loss: 0.28349 | val_0_auc: 0.91512 |  0:00:46s\n",
      "epoch 74 | loss: 0.26968 | val_0_auc: 0.89902 |  0:00:47s\n",
      "epoch 75 | loss: 0.26728 | val_0_auc: 0.89488 |  0:00:47s\n",
      "epoch 76 | loss: 0.26156 | val_0_auc: 0.9078  |  0:00:48s\n",
      "epoch 77 | loss: 0.25225 | val_0_auc: 0.90829 |  0:00:49s\n",
      "epoch 78 | loss: 0.2701  | val_0_auc: 0.91024 |  0:00:49s\n",
      "epoch 79 | loss: 0.24979 | val_0_auc: 0.9     |  0:00:50s\n",
      "epoch 80 | loss: 0.25484 | val_0_auc: 0.9     |  0:00:51s\n",
      "epoch 81 | loss: 0.23716 | val_0_auc: 0.91707 |  0:00:52s\n",
      "epoch 82 | loss: 0.26041 | val_0_auc: 0.91659 |  0:00:52s\n",
      "epoch 83 | loss: 0.23526 | val_0_auc: 0.90927 |  0:00:53s\n",
      "epoch 84 | loss: 0.26046 | val_0_auc: 0.92488 |  0:00:53s\n",
      "epoch 85 | loss: 0.25833 | val_0_auc: 0.92122 |  0:00:54s\n",
      "epoch 86 | loss: 0.24954 | val_0_auc: 0.91073 |  0:00:55s\n",
      "epoch 87 | loss: 0.23803 | val_0_auc: 0.91024 |  0:00:55s\n",
      "epoch 88 | loss: 0.26004 | val_0_auc: 0.9161  |  0:00:56s\n",
      "epoch 89 | loss: 0.23931 | val_0_auc: 0.91317 |  0:00:57s\n",
      "epoch 90 | loss: 0.23116 | val_0_auc: 0.91317 |  0:00:57s\n",
      "epoch 91 | loss: 0.24141 | val_0_auc: 0.92878 |  0:00:58s\n",
      "epoch 92 | loss: 0.26334 | val_0_auc: 0.92    |  0:00:58s\n",
      "epoch 93 | loss: 0.26187 | val_0_auc: 0.92073 |  0:00:59s\n",
      "epoch 94 | loss: 0.24746 | val_0_auc: 0.9061  |  0:01:00s\n",
      "epoch 95 | loss: 0.26739 | val_0_auc: 0.91634 |  0:01:00s\n",
      "epoch 96 | loss: 0.25162 | val_0_auc: 0.9222  |  0:01:01s\n",
      "epoch 97 | loss: 0.24271 | val_0_auc: 0.91976 |  0:01:01s\n",
      "epoch 98 | loss: 0.22874 | val_0_auc: 0.90927 |  0:01:02s\n",
      "epoch 99 | loss: 0.23799 | val_0_auc: 0.9161  |  0:01:03s\n",
      "Stop training because you reached max_epochs = 100 with best_epoch = 91 and best_val_0_auc = 0.92878\n",
      "\n",
      "Fold 9 Test Metrics:\n",
      "Accuracy: 0.8571\n",
      "F1 Score: 0.8713\n",
      "Precision: 0.8627\n",
      "Recall: 0.8800\n",
      "AUPRC: 0.9174\n",
      "\n",
      "==================================================\n",
      "Fold 10/10\n",
      "==================================================\n",
      "epoch 0  | loss: 0.58481 | val_0_auc: 0.8361  |  0:00:00s\n",
      "epoch 1  | loss: 0.45464 | val_0_auc: 0.90098 |  0:00:01s\n",
      "epoch 2  | loss: 0.42875 | val_0_auc: 0.92244 |  0:00:01s\n",
      "epoch 3  | loss: 0.41892 | val_0_auc: 0.92098 |  0:00:02s\n",
      "epoch 4  | loss: 0.3974  | val_0_auc: 0.93561 |  0:00:03s\n",
      "epoch 5  | loss: 0.3835  | val_0_auc: 0.91439 |  0:00:03s\n",
      "epoch 6  | loss: 0.39711 | val_0_auc: 0.92561 |  0:00:04s\n",
      "epoch 7  | loss: 0.36451 | val_0_auc: 0.93512 |  0:00:04s\n",
      "epoch 8  | loss: 0.34904 | val_0_auc: 0.92195 |  0:00:05s\n",
      "epoch 9  | loss: 0.35192 | val_0_auc: 0.89537 |  0:00:06s\n",
      "epoch 10 | loss: 0.36023 | val_0_auc: 0.94585 |  0:00:06s\n",
      "epoch 11 | loss: 0.3719  | val_0_auc: 0.9461  |  0:00:07s\n",
      "epoch 12 | loss: 0.36571 | val_0_auc: 0.90951 |  0:00:08s\n",
      "epoch 13 | loss: 0.3263  | val_0_auc: 0.92951 |  0:00:08s\n",
      "epoch 14 | loss: 0.33437 | val_0_auc: 0.92854 |  0:00:09s\n",
      "epoch 15 | loss: 0.33928 | val_0_auc: 0.93073 |  0:00:10s\n",
      "epoch 16 | loss: 0.33475 | val_0_auc: 0.90829 |  0:00:10s\n",
      "epoch 17 | loss: 0.33845 | val_0_auc: 0.92122 |  0:00:11s\n",
      "epoch 18 | loss: 0.33355 | val_0_auc: 0.90878 |  0:00:11s\n",
      "epoch 19 | loss: 0.33517 | val_0_auc: 0.90488 |  0:00:12s\n",
      "epoch 20 | loss: 0.33345 | val_0_auc: 0.90732 |  0:00:13s\n",
      "epoch 21 | loss: 0.33995 | val_0_auc: 0.91293 |  0:00:13s\n",
      "epoch 22 | loss: 0.33708 | val_0_auc: 0.92683 |  0:00:14s\n",
      "epoch 23 | loss: 0.33459 | val_0_auc: 0.93512 |  0:00:15s\n",
      "epoch 24 | loss: 0.3518  | val_0_auc: 0.92659 |  0:00:16s\n",
      "epoch 25 | loss: 0.34159 | val_0_auc: 0.93463 |  0:00:17s\n",
      "epoch 26 | loss: 0.34238 | val_0_auc: 0.91927 |  0:00:18s\n",
      "epoch 27 | loss: 0.34547 | val_0_auc: 0.93634 |  0:00:19s\n",
      "epoch 28 | loss: 0.32507 | val_0_auc: 0.95    |  0:00:20s\n",
      "epoch 29 | loss: 0.32614 | val_0_auc: 0.95463 |  0:00:21s\n",
      "epoch 30 | loss: 0.33199 | val_0_auc: 0.94146 |  0:00:22s\n",
      "epoch 31 | loss: 0.31606 | val_0_auc: 0.94878 |  0:00:23s\n",
      "epoch 32 | loss: 0.31638 | val_0_auc: 0.94098 |  0:00:24s\n",
      "epoch 33 | loss: 0.33081 | val_0_auc: 0.94049 |  0:00:25s\n",
      "epoch 34 | loss: 0.30182 | val_0_auc: 0.9361  |  0:00:26s\n",
      "epoch 35 | loss: 0.29711 | val_0_auc: 0.95317 |  0:00:28s\n",
      "epoch 36 | loss: 0.31298 | val_0_auc: 0.9422  |  0:00:30s\n",
      "epoch 37 | loss: 0.30652 | val_0_auc: 0.95756 |  0:00:31s\n",
      "epoch 38 | loss: 0.30038 | val_0_auc: 0.92415 |  0:00:32s\n",
      "epoch 39 | loss: 0.31139 | val_0_auc: 0.9422  |  0:00:34s\n",
      "epoch 40 | loss: 0.32972 | val_0_auc: 0.92341 |  0:00:35s\n",
      "epoch 41 | loss: 0.31477 | val_0_auc: 0.94439 |  0:00:36s\n",
      "epoch 42 | loss: 0.30076 | val_0_auc: 0.95463 |  0:00:37s\n",
      "epoch 43 | loss: 0.30539 | val_0_auc: 0.93854 |  0:00:39s\n",
      "epoch 44 | loss: 0.29553 | val_0_auc: 0.92927 |  0:00:41s\n",
      "epoch 45 | loss: 0.29014 | val_0_auc: 0.93707 |  0:00:43s\n",
      "epoch 46 | loss: 0.29457 | val_0_auc: 0.93951 |  0:00:44s\n",
      "epoch 47 | loss: 0.30303 | val_0_auc: 0.93024 |  0:00:45s\n",
      "epoch 48 | loss: 0.29685 | val_0_auc: 0.93854 |  0:00:47s\n",
      "epoch 49 | loss: 0.31546 | val_0_auc: 0.93268 |  0:00:49s\n",
      "epoch 50 | loss: 0.29515 | val_0_auc: 0.93902 |  0:00:50s\n",
      "epoch 51 | loss: 0.26985 | val_0_auc: 0.93854 |  0:00:52s\n",
      "epoch 52 | loss: 0.27793 | val_0_auc: 0.94634 |  0:00:53s\n",
      "epoch 53 | loss: 0.28893 | val_0_auc: 0.94732 |  0:00:54s\n",
      "epoch 54 | loss: 0.28449 | val_0_auc: 0.90805 |  0:00:56s\n",
      "epoch 55 | loss: 0.28296 | val_0_auc: 0.94049 |  0:00:58s\n",
      "epoch 56 | loss: 0.28056 | val_0_auc: 0.95512 |  0:00:59s\n",
      "epoch 57 | loss: 0.26543 | val_0_auc: 0.94634 |  0:01:01s\n",
      "epoch 58 | loss: 0.26103 | val_0_auc: 0.93415 |  0:01:02s\n",
      "epoch 59 | loss: 0.28672 | val_0_auc: 0.91902 |  0:01:04s\n",
      "epoch 60 | loss: 0.27608 | val_0_auc: 0.9361  |  0:01:06s\n",
      "epoch 61 | loss: 0.25235 | val_0_auc: 0.93268 |  0:01:07s\n",
      "epoch 62 | loss: 0.27555 | val_0_auc: 0.9322  |  0:01:08s\n",
      "epoch 63 | loss: 0.25478 | val_0_auc: 0.95122 |  0:01:11s\n",
      "epoch 64 | loss: 0.27099 | val_0_auc: 0.91683 |  0:01:13s\n",
      "epoch 65 | loss: 0.28056 | val_0_auc: 0.92585 |  0:01:14s\n",
      "epoch 66 | loss: 0.25881 | val_0_auc: 0.93561 |  0:01:16s\n",
      "epoch 67 | loss: 0.27087 | val_0_auc: 0.93805 |  0:01:16s\n",
      "epoch 68 | loss: 0.26411 | val_0_auc: 0.95073 |  0:01:18s\n",
      "epoch 69 | loss: 0.24155 | val_0_auc: 0.95098 |  0:01:19s\n",
      "epoch 70 | loss: 0.22145 | val_0_auc: 0.92805 |  0:01:20s\n",
      "epoch 71 | loss: 0.26845 | val_0_auc: 0.94537 |  0:01:21s\n",
      "epoch 72 | loss: 0.24845 | val_0_auc: 0.94    |  0:01:23s\n",
      "epoch 73 | loss: 0.25647 | val_0_auc: 0.93732 |  0:01:23s\n",
      "epoch 74 | loss: 0.2478  | val_0_auc: 0.94244 |  0:01:24s\n",
      "epoch 75 | loss: 0.26888 | val_0_auc: 0.95122 |  0:01:25s\n",
      "epoch 76 | loss: 0.25387 | val_0_auc: 0.93902 |  0:01:26s\n",
      "epoch 77 | loss: 0.24015 | val_0_auc: 0.93122 |  0:01:26s\n",
      "epoch 78 | loss: 0.26928 | val_0_auc: 0.9239  |  0:01:27s\n",
      "epoch 79 | loss: 0.24376 | val_0_auc: 0.92976 |  0:01:27s\n",
      "epoch 80 | loss: 0.25347 | val_0_auc: 0.93561 |  0:01:28s\n",
      "epoch 81 | loss: 0.24542 | val_0_auc: 0.90878 |  0:01:28s\n",
      "epoch 82 | loss: 0.24079 | val_0_auc: 0.93366 |  0:01:29s\n",
      "epoch 83 | loss: 0.2472  | val_0_auc: 0.93902 |  0:01:29s\n",
      "epoch 84 | loss: 0.25971 | val_0_auc: 0.93317 |  0:01:30s\n",
      "epoch 85 | loss: 0.25188 | val_0_auc: 0.90585 |  0:01:30s\n",
      "epoch 86 | loss: 0.24489 | val_0_auc: 0.92293 |  0:01:31s\n",
      "epoch 87 | loss: 0.23976 | val_0_auc: 0.94244 |  0:01:31s\n",
      "\n",
      "Early stopping occurred at epoch 87 with best_epoch = 37 and best_val_0_auc = 0.95756\n",
      "\n",
      "Fold 10 Test Metrics:\n",
      "Accuracy: 0.8901\n",
      "F1 Score: 0.8958\n",
      "Precision: 0.9348\n",
      "Recall: 0.8600\n",
      "AUPRC: 0.9735\n",
      "Successfully saved model at best_model.pkl.zip\n",
      "\n",
      "Saved best model with AUPRC 0.9735\n",
      "绘制最佳模型的混淆矩阵\n",
      "\n",
      "==================================================\n",
      "Final Cross-Validation Results:\n",
      "==================================================\n",
      "Average Accuracy: 0.8736 ± 0.0331\n",
      "Average F1 Score: 0.8864 ± 0.0309\n",
      "Average Precision: 0.8827 ± 0.0431\n",
      "Average Recall: 0.8936 ± 0.0551\n",
      "Average AUPRC: 0.9514 ± 0.0185\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import (accuracy_score, f1_score, precision_score,\n",
    "                             recall_score, average_precision_score,\n",
    "                             confusion_matrix, auc)\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def calculate_metrics(y_true, y_pred, y_scores):\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_true, y_pred),\n",
    "        'f1': f1_score(y_true, y_pred, zero_division=0),\n",
    "        'precision': precision_score(y_true, y_pred, zero_division=0),\n",
    "        'recall': recall_score(y_true, y_pred, zero_division=0),\n",
    "        'auprc': average_precision_score(y_true, y_scores[:, 1])  # 只取正类概率\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, dpi=720):\n",
    "    \"\"\"绘制最佳模型混淆矩阵\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    cm_percentage = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100  # 百分比表示\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    ax = sns.heatmap(cm_percentage, annot=False, fmt='.2f', cmap='Blues', square=True, cbar=False,\n",
    "                     linewidths=2, linecolor='black')\n",
    "\n",
    "    # 在每个格子中显示个数和百分比\n",
    "    for i in range(2):  # 假设是二分类，可以根据需要调整\n",
    "        for j in range(2):\n",
    "            text_color = 'white' if cm_percentage[i, j] > 50 else 'black'\n",
    "            ax.text(j + 0.5, i + 0.5, f'{cm[i, j]}\\n({cm_percentage[i, j]:.2f}%)',\n",
    "                    color=text_color, ha='center', va='center', fontsize=14, fontweight='bold')\n",
    "\n",
    "    # 添加中文标签\n",
    "    plt.xlabel('预测类别', fontsize=16, fontweight='bold')\n",
    "    plt.ylabel('实际类别', fontsize=16, fontweight='bold')\n",
    "    plt.xticks(ticks=np.arange(2) + 0.5, labels=np.arange(1, 3), fontsize=14, fontweight='bold')\n",
    "    plt.yticks(ticks=np.arange(2) + 0.5, labels=np.arange(1, 3), fontsize=14, fontweight='bold')\n",
    "\n",
    "    # 调整布局，减少空白边缘\n",
    "    plt.subplots_adjust(left=0.1, right=0.9, top=0.9, bottom=0.1)\n",
    "\n",
    "    # 保存混淆矩阵图\n",
    "    plt.savefig(f'best_model_confusion_matrix.png', dpi=dpi)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def interpolate_pr_curve(precision, recall):\n",
    "    \"\"\"插值PR曲线到固定长度的点\"\"\"\n",
    "    f = interpolate.interp1d(recall, precision, bounds_error=False, fill_value=(1.0, 0.0))\n",
    "    new_recall = np.linspace(0, 1, 100)\n",
    "    new_precision = f(new_recall)\n",
    "    return new_precision, new_recall\n",
    "\n",
    "def plot_pr_curve(y_true, y_scores, fold):\n",
    "    try:\n",
    "        precision, recall, _ = precision_recall_curve(y_true, y_scores[:, 1])  # 只取正类概率\n",
    "        auprc = auc(recall, precision)\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(recall, precision, label=f'Fold {fold} (AUPRC = {auprc:.2f})')\n",
    "        plt.xlabel('Recall')\n",
    "        plt.ylabel('Precision')\n",
    "        plt.title(f'Precision-Recall Curve (Fold {fold})')\n",
    "        plt.legend()\n",
    "        plt.savefig(f'pr_curve_fold{fold}.png')\n",
    "        plt.close()\n",
    "        return precision, recall\n",
    "    except Exception as e:\n",
    "        print(f\"无法绘制Fold {fold}的PR曲线: {str(e)}\")\n",
    "        return None, None\n",
    "\n",
    "def train_test_split(X, y, splits=10, batch_size=32):\n",
    "    print(f\"数据形状: X={X.shape}, y={y.shape}\")\n",
    "    print(f\"类别分布: {np.bincount(y)}\")\n",
    "\n",
    "    X = np.nan_to_num(X)\n",
    "    y = np.nan_to_num(y).astype(int)\n",
    "\n",
    "    k_fold = StratifiedKFold(n_splits=splits, shuffle=True, random_state=2025)\n",
    "    results = []\n",
    "    best_model_info = {'val_score': -float('inf'), 'model': None, 'y_true': None, 'y_pred': None}\n",
    "\n",
    "    interp_precisions = []\n",
    "    interp_recalls = np.linspace(0, 1, 100)\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(k_fold.split(X, y)):\n",
    "        print(f'\\n{\"=\" * 50}')\n",
    "        print(f'Fold {fold + 1}/{splits}')\n",
    "        print(f'{\"=\" * 50}')\n",
    "\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "        # Use SMOTE for balancing the dataset\n",
    "        smote = SMOTE(random_state=42)\n",
    "        X_res, y_res = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "        model = TabNetClassifier(\n",
    "            n_d=8, n_a=8, n_steps=3, gamma=1.3, lambda_sparse=0,\n",
    "            optimizer_fn=torch.optim.Adam, optimizer_params=dict(lr=2e-2),\n",
    "            mask_type='sparsemax', scheduler_params=dict(step_size=50, gamma=0.9),\n",
    "            scheduler_fn=torch.optim.lr_scheduler.StepLR\n",
    "        )\n",
    "\n",
    "        model.fit(X_res, y_res, eval_set=[(X_test, y_test)], patience=50, batch_size=batch_size)\n",
    "\n",
    "        y_scores = model.predict_proba(X_test)\n",
    "        y_pred = np.argmax(y_scores, axis=1)\n",
    "        metrics = calculate_metrics(y_test, y_pred, y_scores)\n",
    "\n",
    "        precision, recall = plot_pr_curve(y_test, y_scores, fold + 1)\n",
    "\n",
    "        if precision is not None and recall is not None:\n",
    "            interp_precision, _ = interpolate_pr_curve(precision, recall)\n",
    "            interp_precisions.append(interp_precision)\n",
    "\n",
    "        results.append(metrics)\n",
    "\n",
    "        if metrics['auprc'] > best_model_info['val_score']:\n",
    "            best_model_info['val_score'] = metrics['auprc']\n",
    "            best_model_info['model'] = model\n",
    "            best_model_info['y_true'] = y_test\n",
    "            best_model_info['y_pred'] = y_pred\n",
    "\n",
    "        print(f'\\nFold {fold + 1} Test Metrics:')\n",
    "        print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
    "        print(f\"F1 Score: {metrics['f1']:.4f}\")\n",
    "        print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "        print(f\"Recall: {metrics['recall']:.4f}\")\n",
    "        print(f\"AUPRC: {metrics['auprc']:.4f}\")\n",
    "\n",
    "    # Save the best model\n",
    "    best_model_info['model'].save_model('best_model.pkl')\n",
    "    print(f\"\\nSaved best model with AUPRC {best_model_info['val_score']:.4f}\")\n",
    "\n",
    "    # Plot confusion matrix for the best model\n",
    "    print(f\"绘制最佳模型的混淆矩阵\")\n",
    "    plot_confusion_matrix(best_model_info['y_true'], best_model_info['y_pred'])\n",
    "\n",
    "    avg_metrics = {\n",
    "        'accuracy': np.mean([r['accuracy'] for r in results]),\n",
    "        'f1': np.mean([r['f1'] for r in results]),\n",
    "        'precision': np.mean([r['precision'] for r in results]),\n",
    "        'recall': np.mean([r['recall'] for r in results]),\n",
    "        'auprc': np.mean([r['auprc'] for r in results])\n",
    "    }\n",
    "\n",
    "    std_metrics = {\n",
    "        'accuracy': np.std([r['accuracy'] for r in results]),\n",
    "        'f1': np.std([r['f1'] for r in results]),\n",
    "        'precision': np.std([r['precision'] for r in results]),\n",
    "        'recall': np.std([r['recall'] for r in results]),\n",
    "        'auprc': np.std([r['auprc'] for r in results])\n",
    "    }\n",
    "\n",
    "    print('\\n' + '=' * 50)\n",
    "    print('Final Cross-Validation Results:')\n",
    "    print('=' * 50)\n",
    "    print(f\"Average Accuracy: {avg_metrics['accuracy']:.4f} ± {std_metrics['accuracy']:.4f}\")\n",
    "    print(f\"Average F1 Score: {avg_metrics['f1']:.4f} ± {std_metrics['f1']:.4f}\")\n",
    "    print(f\"Average Precision: {avg_metrics['precision']:.4f} ± {std_metrics['precision']:.4f}\")\n",
    "    print(f\"Average Recall: {avg_metrics['recall']:.4f} ± {std_metrics['recall']:.4f}\")\n",
    "    print(f\"Average AUPRC: {avg_metrics['auprc']:.4f} ± {std_metrics['auprc']:.4f}\")\n",
    "\n",
    "    if interp_precisions:\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        mean_precision = np.mean(interp_precisions, axis=0)\n",
    "        mean_auprc = auc(interp_recalls, mean_precision)\n",
    "\n",
    "        for i, prec in enumerate(interp_precisions):\n",
    "            plt.plot(interp_recalls, prec, alpha=0.3, label=f'Fold {i + 1}')\n",
    "\n",
    "        plt.plot(interp_recalls, mean_precision, 'k-',\n",
    "                 label=f'Mean (AUPRC = {mean_auprc:.2f})', linewidth=2)\n",
    "        plt.xlabel('Recall')\n",
    "        plt.ylabel('Precision')\n",
    "        plt.title('Average Precision-Recall Curve')\n",
    "        plt.legend()\n",
    "        plt.savefig('average_pr_curve.png')\n",
    "        plt.close()\n",
    "\n",
    "# Load and preprocess data\n",
    "data = pd.read_csv('preparations/heart_output.csv')\n",
    "\n",
    "X = data.drop('HeartDisease', axis=1).values\n",
    "y = data['HeartDisease'].values\n",
    "\n",
    "X = X.astype(np.float32)\n",
    "y = y.astype(np.int64)\n",
    "\n",
    "train_test_split(X, y, splits=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ea9cbfe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据形状: X=(918, 11), y=(918,)\n",
      "类别分布: [410 508]\n",
      "\n",
      "==================================================\n",
      "Fold 1/10\n",
      "==================================================\n",
      "\n",
      "Fold 1 Test Metrics:\n",
      "Accuracy: 0.8152\n",
      "F1 Score: 0.8317\n",
      "Precision: 0.8400\n",
      "Recall: 0.8235\n",
      "AUPRC: 0.8615\n",
      "\n",
      "==================================================\n",
      "Fold 2/10\n",
      "==================================================\n",
      "\n",
      "Fold 2 Test Metrics:\n",
      "Accuracy: 0.8696\n",
      "F1 Score: 0.8824\n",
      "Precision: 0.8824\n",
      "Recall: 0.8824\n",
      "AUPRC: 0.9441\n",
      "\n",
      "==================================================\n",
      "Fold 3/10\n",
      "==================================================\n",
      "\n",
      "Fold 3 Test Metrics:\n",
      "Accuracy: 0.8261\n",
      "F1 Score: 0.8431\n",
      "Precision: 0.8431\n",
      "Recall: 0.8431\n",
      "AUPRC: 0.8651\n",
      "\n",
      "==================================================\n",
      "Fold 4/10\n",
      "==================================================\n",
      "\n",
      "Fold 4 Test Metrics:\n",
      "Accuracy: 0.8696\n",
      "F1 Score: 0.8776\n",
      "Precision: 0.9149\n",
      "Recall: 0.8431\n",
      "AUPRC: 0.9607\n",
      "\n",
      "==================================================\n",
      "Fold 5/10\n",
      "==================================================\n",
      "\n",
      "Fold 5 Test Metrics:\n",
      "Accuracy: 0.8261\n",
      "F1 Score: 0.8462\n",
      "Precision: 0.8302\n",
      "Recall: 0.8627\n",
      "AUPRC: 0.9323\n",
      "\n",
      "==================================================\n",
      "Fold 6/10\n",
      "==================================================\n",
      "\n",
      "Fold 6 Test Metrics:\n",
      "Accuracy: 0.8261\n",
      "F1 Score: 0.8462\n",
      "Precision: 0.8302\n",
      "Recall: 0.8627\n",
      "AUPRC: 0.8772\n",
      "\n",
      "==================================================\n",
      "Fold 7/10\n",
      "==================================================\n",
      "\n",
      "Fold 7 Test Metrics:\n",
      "Accuracy: 0.8913\n",
      "F1 Score: 0.9038\n",
      "Precision: 0.8868\n",
      "Recall: 0.9216\n",
      "AUPRC: 0.8818\n",
      "\n",
      "==================================================\n",
      "Fold 8/10\n",
      "==================================================\n",
      "\n",
      "Fold 8 Test Metrics:\n",
      "Accuracy: 0.8478\n",
      "F1 Score: 0.8704\n",
      "Precision: 0.8246\n",
      "Recall: 0.9216\n",
      "AUPRC: 0.9310\n",
      "\n",
      "==================================================\n",
      "Fold 9/10\n",
      "==================================================\n",
      "\n",
      "Fold 9 Test Metrics:\n",
      "Accuracy: 0.8462\n",
      "F1 Score: 0.8654\n",
      "Precision: 0.8333\n",
      "Recall: 0.9000\n",
      "AUPRC: 0.9018\n",
      "\n",
      "==================================================\n",
      "Fold 10/10\n",
      "==================================================\n",
      "\n",
      "Fold 10 Test Metrics:\n",
      "Accuracy: 0.8901\n",
      "F1 Score: 0.8980\n",
      "Precision: 0.9167\n",
      "Recall: 0.8800\n",
      "AUPRC: 0.9637\n",
      "\n",
      "Saved best model with AUPRC 0.9637\n",
      "绘制最佳模型的混淆矩阵\n",
      "\n",
      "==================================================\n",
      "Final Cross-Validation Results:\n",
      "==================================================\n",
      "Average Accuracy: 0.8508 ± 0.0265\n",
      "Average F1 Score: 0.8665 ± 0.0232\n",
      "Average Precision: 0.8602 ± 0.0345\n",
      "Average Recall: 0.8741 ± 0.0316\n",
      "Average AUPRC: 0.9119 ± 0.0372\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import (accuracy_score, f1_score, precision_score,\n",
    "                             recall_score, average_precision_score,\n",
    "                             confusion_matrix, auc)\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import interpolate\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def calculate_metrics(y_true, y_pred, y_scores):\n",
    "    # Check for NaN values\n",
    "    if np.isnan(y_scores).any():\n",
    "        y_scores = np.nan_to_num(y_scores)\n",
    "\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_true, y_pred),\n",
    "        'f1': f1_score(y_true, y_pred, zero_division=0),\n",
    "        'precision': precision_score(y_true, y_pred, zero_division=0),\n",
    "        'recall': recall_score(y_true, y_pred, zero_division=0),\n",
    "    }\n",
    "\n",
    "    # Calculate AUPRC\n",
    "    try:\n",
    "        metrics['auprc'] = average_precision_score(y_true, y_scores)\n",
    "    except:\n",
    "        print(\"无法计算AUPRC，使用默认值0\")\n",
    "        metrics['auprc'] = 0\n",
    "\n",
    "    return metrics\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, dpi=720):\n",
    "    \"\"\"绘制最佳模型混淆矩阵\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    cm_percentage = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100  # 百分比表示\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    ax = sns.heatmap(cm_percentage, annot=False, fmt='.2f', cmap='Blues', square=True, cbar=False,\n",
    "                     linewidths=2, linecolor='black')\n",
    "\n",
    "    # 在每个格子中显示个数和百分比\n",
    "    for i in range(2):  # 假设是二分类，可以根据需要调整\n",
    "        for j in range(2):\n",
    "            text_color = 'white' if cm_percentage[i, j] > 50 else 'black'\n",
    "            ax.text(j + 0.5, i + 0.5, f'{cm[i, j]}\\n({cm_percentage[i, j]:.2f}%)',\n",
    "                    color=text_color, ha='center', va='center', fontsize=14, fontweight='bold')\n",
    "\n",
    "    # 添加中文标签\n",
    "    plt.xlabel('预测类别', fontsize=16, fontweight='bold')\n",
    "    plt.ylabel('实际类别', fontsize=16, fontweight='bold')\n",
    "    plt.xticks(ticks=np.arange(2) + 0.5, labels=np.arange(1, 3), fontsize=14, fontweight='bold')\n",
    "    plt.yticks(ticks=np.arange(2) + 0.5, labels=np.arange(1, 3), fontsize=14, fontweight='bold')\n",
    "\n",
    "    # 调整布局，减少空白边缘\n",
    "    plt.subplots_adjust(left=0.1, right=0.9, top=0.9, bottom=0.1)\n",
    "\n",
    "    # 保存混淆矩阵图\n",
    "    plt.savefig(f'best_model_confusion_matrix.png', dpi=dpi)\n",
    "    plt.close()\n",
    "\n",
    "def interpolate_pr_curve(precision, recall):\n",
    "    \"\"\"插值PR曲线到固定长度的点\"\"\"\n",
    "    f = interpolate.interp1d(recall, precision, bounds_error=False, fill_value=(1.0, 0.0))\n",
    "    new_recall = np.linspace(0, 1, 100)\n",
    "    new_precision = f(new_recall)\n",
    "    return new_precision, new_recall\n",
    "\n",
    "def plot_pr_curve(y_true, y_scores, fold):\n",
    "    try:\n",
    "        precision, recall, _ = precision_recall_curve(y_true, y_scores)\n",
    "        auprc = auc(recall, precision)\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(recall, precision, label=f'Fold {fold} (AUPRC = {auprc:.2f})')\n",
    "        plt.xlabel('Recall')\n",
    "        plt.ylabel('Precision')\n",
    "        plt.title('Precision-Recall Curve')\n",
    "        plt.legend()\n",
    "        plt.savefig(f'pr_curve_fold{fold}.png')\n",
    "        plt.close()\n",
    "        return precision, recall\n",
    "    except Exception as e:\n",
    "        print(f\"无法绘制Fold {fold}的PR曲线: {str(e)}\")\n",
    "        return None, None\n",
    "\n",
    "def train_test_split(X, y, splits=10, batch_size=32):\n",
    "    # Check data\n",
    "    print(f\"数据形状: X={X.shape}, y={y.shape}\")\n",
    "    print(f\"类别分布: {np.bincount(y)}\")\n",
    "\n",
    "    # Handle possible NaN values\n",
    "    X = np.nan_to_num(X)\n",
    "    y = np.nan_to_num(y).astype(int)\n",
    "\n",
    "    k_fold = StratifiedKFold(n_splits=splits, shuffle=True, random_state=2025)\n",
    "    results = []\n",
    "    best_model_info = {'val_score': -float('inf'), 'model': None, 'y_true': None, 'y_pred': None}\n",
    "\n",
    "    # Store PR curve data for all folds (interpolated)\n",
    "    interp_precisions = []\n",
    "    interp_recalls = np.linspace(0, 1, 100)  # Fixed 100 recall points\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(k_fold.split(X, y)):\n",
    "        print(f'\\n{\"=\" * 50}')\n",
    "        print(f'Fold {fold + 1}/{splits}')\n",
    "        print(f'{\"=\" * 50}')\n",
    "\n",
    "        # Split data\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "        # Standardization\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "        # Create XGBoost model\n",
    "        xgb_model = xgb.XGBClassifier(n_estimators=100, random_state=42, n_jobs=-1, use_label_encoder=False)\n",
    "\n",
    "        # Train model\n",
    "        xgb_model.fit(X_train, y_train)\n",
    "\n",
    "        # Evaluate on test set\n",
    "        y_scores = xgb_model.predict_proba(X_test)[:, 1]  # Get the probability for class 1 (positive class)\n",
    "        y_pred = (y_scores > 0.5).astype(int)\n",
    "        metrics = calculate_metrics(y_test, y_pred, y_scores)\n",
    "\n",
    "        # Plot and save PR curve for current fold\n",
    "        precision, recall = plot_pr_curve(y_test, y_scores, fold + 1)\n",
    "\n",
    "        # Interpolate PR curve to fixed length\n",
    "        if precision is not None and recall is not None:\n",
    "            interp_precision, _ = interpolate_pr_curve(precision, recall)\n",
    "            interp_precisions.append(interp_precision)\n",
    "\n",
    "        # Save results\n",
    "        results.append(metrics)\n",
    "\n",
    "        # Update the best model based on AUPRC\n",
    "        if metrics['auprc'] > best_model_info['val_score']:\n",
    "            best_model_info['val_score'] = metrics['auprc']\n",
    "            best_model_info['model'] = xgb_model\n",
    "            best_model_info['y_true'] = y_test\n",
    "            best_model_info['y_pred'] = y_pred\n",
    "\n",
    "        # Print current fold results\n",
    "        print(f'\\nFold {fold + 1} Test Metrics:')\n",
    "        print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
    "        print(f\"F1 Score: {metrics['f1']:.4f}\")\n",
    "        print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "        print(f\"Recall: {metrics['recall']:.4f}\")\n",
    "        print(f\"AUPRC: {metrics['auprc']:.4f}\")\n",
    "\n",
    "    # Save the best model\n",
    "    best_model_info['model'].save_model('best_model.xgb')\n",
    "    print(f\"\\nSaved best model with AUPRC {best_model_info['val_score']:.4f}\")\n",
    "\n",
    "    # Plot confusion matrix for the best model\n",
    "    print(f\"绘制最佳模型的混淆矩阵\")\n",
    "    plot_confusion_matrix(best_model_info['y_true'], best_model_info['y_pred'])\n",
    "\n",
    "    avg_metrics = {\n",
    "        'accuracy': np.mean([r['accuracy'] for r in results]),\n",
    "        'f1': np.mean([r['f1'] for r in results]),\n",
    "        'precision': np.mean([r['precision'] for r in results]),\n",
    "        'recall': np.mean([r['recall'] for r in results]),\n",
    "        'auprc': np.mean([r['auprc'] for r in results])\n",
    "    }\n",
    "\n",
    "    std_metrics = {\n",
    "        'accuracy': np.std([r['accuracy'] for r in results]),\n",
    "        'f1': np.std([r['f1'] for r in results]),\n",
    "        'precision': np.std([r['precision'] for r in results]),\n",
    "        'recall': np.std([r['recall'] for r in results]),\n",
    "        'auprc': np.std([r['auprc'] for r in results])\n",
    "    }\n",
    "\n",
    "    print('\\n' + '=' * 50)\n",
    "    print('Final Cross-Validation Results:')\n",
    "    print('=' * 50)\n",
    "    print(f\"Average Accuracy: {avg_metrics['accuracy']:.4f} ± {std_metrics['accuracy']:.4f}\")\n",
    "    print(f\"Average F1 Score: {avg_metrics['f1']:.4f} ± {std_metrics['f1']:.4f}\")\n",
    "    print(f\"Average Precision: {avg_metrics['precision']:.4f} ± {std_metrics['precision']:.4f}\")\n",
    "    print(f\"Average Recall: {avg_metrics['recall']:.4f} ± {std_metrics['recall']:.4f}\")\n",
    "    print(f\"Average AUPRC: {avg_metrics['auprc']:.4f} ± {std_metrics['auprc']:.4f}\")\n",
    "\n",
    "    # Plot average PR curve (using interpolated data)\n",
    "    if interp_precisions:\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        mean_precision = np.mean(interp_precisions, axis=0)\n",
    "        mean_auprc = auc(interp_recalls, mean_precision)\n",
    "\n",
    "        for i, prec in enumerate(interp_precisions):\n",
    "            plt.plot(interp_recalls, prec, alpha=0.3, label=f'Fold {i + 1}')\n",
    "\n",
    "        plt.plot(interp_recalls, mean_precision, 'k-',\n",
    "                 label=f'Mean (AUPRC = {mean_auprc:.2f})', linewidth=2)\n",
    "        plt.xlabel('Recall')\n",
    "        plt.ylabel('Precision')\n",
    "        plt.title('Average Precision-Recall Curve')\n",
    "        plt.legend()\n",
    "        plt.savefig('average_pr_curve.png')\n",
    "        plt.close()\n",
    "\n",
    "# Load and preprocess data\n",
    "data = pd.read_csv('preparations/heart_output.csv')\n",
    "\n",
    "X = data.drop('HeartDisease', axis=1).values\n",
    "y = data['HeartDisease'].values\n",
    "\n",
    "X = X.astype(np.float32)\n",
    "y = y.astype(np.int64)\n",
    "\n",
    "train_test_split(X, y, splits=10, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9a382805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Fold 1/10\n",
      "==================================================\n",
      "Epoch 10/500 - Train Loss: 0.0262, Val Loss: 0.0296\n",
      "Val Metrics - Acc: 0.7378, F1: 0.2209, Precision: 0.1293, Recall: 0.7600, AUPRC: 0.1644\n",
      "Epoch 20/500 - Train Loss: 0.0237, Val Loss: 0.0308\n",
      "Val Metrics - Acc: 0.7397, F1: 0.2130, Precision: 0.1250, Recall: 0.7200, AUPRC: 0.1681\n",
      "Epoch 30/500 - Train Loss: 0.0220, Val Loss: 0.0267\n",
      "Val Metrics - Acc: 0.7769, F1: 0.2192, Precision: 0.1322, Recall: 0.6400, AUPRC: 0.1763\n",
      "Epoch 40/500 - Train Loss: 0.0204, Val Loss: 0.0275\n",
      "Val Metrics - Acc: 0.7750, F1: 0.2282, Precision: 0.1371, Recall: 0.6800, AUPRC: 0.1834\n",
      "Epoch 50/500 - Train Loss: 0.0189, Val Loss: 0.0274\n",
      "Val Metrics - Acc: 0.7906, F1: 0.2517, Precision: 0.1525, Recall: 0.7200, AUPRC: 0.1929\n",
      "Epoch 60/500 - Train Loss: 0.0179, Val Loss: 0.0291\n",
      "Val Metrics - Acc: 0.7926, F1: 0.2535, Precision: 0.1538, Recall: 0.7200, AUPRC: 0.1777\n",
      "Epoch 70/500 - Train Loss: 0.0170, Val Loss: 0.0303\n",
      "Val Metrics - Acc: 0.7828, F1: 0.2345, Precision: 0.1417, Recall: 0.6800, AUPRC: 0.1884\n",
      "Early stopping at epoch 77\n",
      "\n",
      "Fold 1 Test Metrics:\n",
      "Accuracy: 0.8141\n",
      "F1 Score: 0.2520\n",
      "Precision: 0.1569\n",
      "Recall: 0.6400\n",
      "AUPRC: 0.1852\n",
      "\n",
      "==================================================\n",
      "Fold 2/10\n",
      "==================================================\n",
      "Epoch 10/500 - Train Loss: 0.0259, Val Loss: 0.0306\n",
      "Val Metrics - Acc: 0.7397, F1: 0.2486, Precision: 0.1447, Recall: 0.8800, AUPRC: 0.2111\n",
      "Epoch 20/500 - Train Loss: 0.0234, Val Loss: 0.0265\n",
      "Val Metrics - Acc: 0.7750, F1: 0.2384, Precision: 0.1429, Recall: 0.7200, AUPRC: 0.1744\n",
      "Epoch 30/500 - Train Loss: 0.0215, Val Loss: 0.0245\n",
      "Val Metrics - Acc: 0.8043, F1: 0.2188, Precision: 0.1359, Recall: 0.5600, AUPRC: 0.1650\n",
      "Epoch 40/500 - Train Loss: 0.0200, Val Loss: 0.0267\n",
      "Val Metrics - Acc: 0.7671, F1: 0.1678, Precision: 0.1017, Recall: 0.4800, AUPRC: 0.1520\n",
      "Epoch 50/500 - Train Loss: 0.0189, Val Loss: 0.0234\n",
      "Val Metrics - Acc: 0.8317, F1: 0.1731, Precision: 0.1139, Recall: 0.3600, AUPRC: 0.1452\n",
      "Epoch 60/500 - Train Loss: 0.0179, Val Loss: 0.0252\n",
      "Val Metrics - Acc: 0.8200, F1: 0.1636, Precision: 0.1059, Recall: 0.3600, AUPRC: 0.1362\n",
      "Early stopping at epoch 62\n",
      "\n",
      "Fold 2 Test Metrics:\n",
      "Accuracy: 0.8454\n",
      "F1 Score: 0.2020\n",
      "Precision: 0.1351\n",
      "Recall: 0.4000\n",
      "AUPRC: 0.1368\n",
      "\n",
      "==================================================\n",
      "Fold 3/10\n",
      "==================================================\n",
      "Epoch 10/500 - Train Loss: 0.0265, Val Loss: 0.0298\n",
      "Val Metrics - Acc: 0.7495, F1: 0.2289, Precision: 0.1348, Recall: 0.7600, AUPRC: 0.2161\n",
      "Epoch 20/500 - Train Loss: 0.0240, Val Loss: 0.0312\n",
      "Val Metrics - Acc: 0.7378, F1: 0.2209, Precision: 0.1293, Recall: 0.7600, AUPRC: 0.2230\n",
      "Epoch 30/500 - Train Loss: 0.0223, Val Loss: 0.0274\n",
      "Val Metrics - Acc: 0.7886, F1: 0.2603, Precision: 0.1570, Recall: 0.7600, AUPRC: 0.2103\n",
      "Epoch 40/500 - Train Loss: 0.0209, Val Loss: 0.0279\n",
      "Val Metrics - Acc: 0.7867, F1: 0.2378, Precision: 0.1441, Recall: 0.6800, AUPRC: 0.1998\n",
      "Epoch 50/500 - Train Loss: 0.0199, Val Loss: 0.0269\n",
      "Val Metrics - Acc: 0.8004, F1: 0.2154, Precision: 0.1333, Recall: 0.5600, AUPRC: 0.2070\n",
      "Early stopping at epoch 51\n",
      "\n",
      "Fold 3 Test Metrics:\n",
      "Accuracy: 0.8043\n",
      "F1 Score: 0.2537\n",
      "Precision: 0.1560\n",
      "Recall: 0.6800\n",
      "AUPRC: 0.2130\n",
      "\n",
      "==================================================\n",
      "Fold 4/10\n",
      "==================================================\n",
      "Epoch 10/500 - Train Loss: 0.0261, Val Loss: 0.0311\n",
      "Val Metrics - Acc: 0.7358, F1: 0.2105, Precision: 0.1233, Recall: 0.7200, AUPRC: 0.1202\n",
      "Epoch 20/500 - Train Loss: 0.0238, Val Loss: 0.0289\n",
      "Val Metrics - Acc: 0.7593, F1: 0.2166, Precision: 0.1288, Recall: 0.6800, AUPRC: 0.1263\n",
      "Epoch 30/500 - Train Loss: 0.0220, Val Loss: 0.0267\n",
      "Val Metrics - Acc: 0.7828, F1: 0.2128, Precision: 0.1293, Recall: 0.6000, AUPRC: 0.1293\n",
      "Epoch 40/500 - Train Loss: 0.0205, Val Loss: 0.0287\n",
      "Val Metrics - Acc: 0.7789, F1: 0.1986, Precision: 0.1207, Recall: 0.5600, AUPRC: 0.1303\n",
      "Early stopping at epoch 46\n",
      "\n",
      "Fold 4 Test Metrics:\n",
      "Accuracy: 0.7847\n",
      "F1 Score: 0.2143\n",
      "Precision: 0.1304\n",
      "Recall: 0.6000\n",
      "AUPRC: 0.1249\n",
      "\n",
      "==================================================\n",
      "Fold 5/10\n",
      "==================================================\n",
      "Epoch 10/500 - Train Loss: 0.0260, Val Loss: 0.0272\n",
      "Val Metrics - Acc: 0.7789, F1: 0.2614, Precision: 0.1562, Recall: 0.8000, AUPRC: 0.1809\n",
      "Epoch 20/500 - Train Loss: 0.0234, Val Loss: 0.0274\n",
      "Val Metrics - Acc: 0.7886, F1: 0.2394, Precision: 0.1453, Recall: 0.6800, AUPRC: 0.1537\n",
      "Epoch 30/500 - Train Loss: 0.0216, Val Loss: 0.0258\n",
      "Val Metrics - Acc: 0.8141, F1: 0.2520, Precision: 0.1569, Recall: 0.6400, AUPRC: 0.1280\n",
      "Epoch 40/500 - Train Loss: 0.0201, Val Loss: 0.0249\n",
      "Val Metrics - Acc: 0.8258, F1: 0.2124, Precision: 0.1364, Recall: 0.4800, AUPRC: 0.1192\n",
      "Epoch 50/500 - Train Loss: 0.0186, Val Loss: 0.0275\n",
      "Val Metrics - Acc: 0.8063, F1: 0.1681, Precision: 0.1064, Recall: 0.4000, AUPRC: 0.0985\n",
      "Early stopping at epoch 51\n",
      "\n",
      "Fold 5 Test Metrics:\n",
      "Accuracy: 0.8278\n",
      "F1 Score: 0.2143\n",
      "Precision: 0.1379\n",
      "Recall: 0.4800\n",
      "AUPRC: 0.1285\n",
      "\n",
      "==================================================\n",
      "Fold 6/10\n",
      "==================================================\n",
      "Epoch 10/500 - Train Loss: 0.0255, Val Loss: 0.0266\n",
      "Val Metrics - Acc: 0.7886, F1: 0.2059, Precision: 0.1261, Recall: 0.5600, AUPRC: 0.1334\n",
      "Epoch 20/500 - Train Loss: 0.0229, Val Loss: 0.0274\n",
      "Val Metrics - Acc: 0.7847, F1: 0.1667, Precision: 0.1028, Recall: 0.4400, AUPRC: 0.1281\n",
      "Epoch 30/500 - Train Loss: 0.0214, Val Loss: 0.0270\n",
      "Val Metrics - Acc: 0.7984, F1: 0.1626, Precision: 0.1020, Recall: 0.4000, AUPRC: 0.1328\n",
      "Epoch 40/500 - Train Loss: 0.0202, Val Loss: 0.0301\n",
      "Val Metrics - Acc: 0.7808, F1: 0.1515, Precision: 0.0935, Recall: 0.4000, AUPRC: 0.1182\n",
      "Early stopping at epoch 41\n",
      "\n",
      "Fold 6 Test Metrics:\n",
      "Accuracy: 0.8102\n",
      "F1 Score: 0.1983\n",
      "Precision: 0.1250\n",
      "Recall: 0.4800\n",
      "AUPRC: 0.1244\n",
      "\n",
      "==================================================\n",
      "Fold 7/10\n",
      "==================================================\n",
      "Epoch 10/500 - Train Loss: 0.0268, Val Loss: 0.0278\n",
      "Val Metrics - Acc: 0.7534, F1: 0.2317, Precision: 0.1367, Recall: 0.7600, AUPRC: 0.2205\n",
      "Epoch 20/500 - Train Loss: 0.0244, Val Loss: 0.0289\n",
      "Val Metrics - Acc: 0.7515, F1: 0.2395, Precision: 0.1408, Recall: 0.8000, AUPRC: 0.2073\n",
      "Epoch 30/500 - Train Loss: 0.0225, Val Loss: 0.0270\n",
      "Val Metrics - Acc: 0.7632, F1: 0.1987, Precision: 0.1190, Recall: 0.6000, AUPRC: 0.1997\n",
      "Epoch 40/500 - Train Loss: 0.0211, Val Loss: 0.0253\n",
      "Val Metrics - Acc: 0.7926, F1: 0.2090, Precision: 0.1284, Recall: 0.5600, AUPRC: 0.1631\n",
      "Early stopping at epoch 48\n",
      "\n",
      "Fold 7 Test Metrics:\n",
      "Accuracy: 0.7965\n",
      "F1 Score: 0.2353\n",
      "Precision: 0.1441\n",
      "Recall: 0.6400\n",
      "AUPRC: 0.1805\n",
      "\n",
      "==================================================\n",
      "Fold 8/10\n",
      "==================================================\n",
      "Epoch 10/500 - Train Loss: 0.0266, Val Loss: 0.0264\n",
      "Val Metrics - Acc: 0.7495, F1: 0.2289, Precision: 0.1348, Recall: 0.7600, AUPRC: 0.2611\n",
      "Epoch 20/500 - Train Loss: 0.0244, Val Loss: 0.0271\n",
      "Val Metrics - Acc: 0.7573, F1: 0.2346, Precision: 0.1387, Recall: 0.7600, AUPRC: 0.2356\n",
      "Epoch 30/500 - Train Loss: 0.0227, Val Loss: 0.0241\n",
      "Val Metrics - Acc: 0.7886, F1: 0.2394, Precision: 0.1453, Recall: 0.6800, AUPRC: 0.2253\n",
      "Epoch 40/500 - Train Loss: 0.0209, Val Loss: 0.0267\n",
      "Val Metrics - Acc: 0.7789, F1: 0.2207, Precision: 0.1333, Recall: 0.6400, AUPRC: 0.2038\n",
      "Early stopping at epoch 49\n",
      "\n",
      "Fold 8 Test Metrics:\n",
      "Accuracy: 0.7945\n",
      "F1 Score: 0.2446\n",
      "Precision: 0.1491\n",
      "Recall: 0.6800\n",
      "AUPRC: 0.2427\n",
      "\n",
      "==================================================\n",
      "Fold 9/10\n",
      "==================================================\n",
      "Epoch 10/500 - Train Loss: 0.0273, Val Loss: 0.0254\n",
      "Val Metrics - Acc: 0.7906, F1: 0.2914, Precision: 0.1746, Recall: 0.8800, AUPRC: 0.3138\n",
      "Epoch 20/500 - Train Loss: 0.0242, Val Loss: 0.0243\n",
      "Val Metrics - Acc: 0.8082, F1: 0.2794, Precision: 0.1712, Recall: 0.7600, AUPRC: 0.2315\n",
      "Epoch 30/500 - Train Loss: 0.0223, Val Loss: 0.0237\n",
      "Val Metrics - Acc: 0.8160, F1: 0.2769, Precision: 0.1714, Recall: 0.7200, AUPRC: 0.2121\n",
      "Epoch 40/500 - Train Loss: 0.0208, Val Loss: 0.0230\n",
      "Val Metrics - Acc: 0.8239, F1: 0.2742, Precision: 0.1717, Recall: 0.6800, AUPRC: 0.2071\n",
      "Epoch 50/500 - Train Loss: 0.0194, Val Loss: 0.0219\n",
      "Val Metrics - Acc: 0.8376, F1: 0.2523, Precision: 0.1628, Recall: 0.5600, AUPRC: 0.1946\n",
      "Epoch 60/500 - Train Loss: 0.0186, Val Loss: 0.0248\n",
      "Val Metrics - Acc: 0.8258, F1: 0.2521, Precision: 0.1596, Recall: 0.6000, AUPRC: 0.1693\n",
      "Epoch 70/500 - Train Loss: 0.0177, Val Loss: 0.0230\n",
      "Val Metrics - Acc: 0.8513, F1: 0.2692, Precision: 0.1772, Recall: 0.5600, AUPRC: 0.1655\n",
      "Early stopping at epoch 71\n",
      "\n",
      "Fold 9 Test Metrics:\n",
      "Accuracy: 0.8493\n",
      "F1 Score: 0.2667\n",
      "Precision: 0.1750\n",
      "Recall: 0.5600\n",
      "AUPRC: 0.1978\n",
      "\n",
      "==================================================\n",
      "Fold 10/10\n",
      "==================================================\n",
      "Epoch 10/500 - Train Loss: 0.0267, Val Loss: 0.0279\n",
      "Val Metrics - Acc: 0.7652, F1: 0.1892, Precision: 0.1129, Recall: 0.5833, AUPRC: 0.1960\n",
      "Epoch 20/500 - Train Loss: 0.0235, Val Loss: 0.0263\n",
      "Val Metrics - Acc: 0.7965, F1: 0.1875, Precision: 0.1154, Recall: 0.5000, AUPRC: 0.1520\n",
      "Epoch 30/500 - Train Loss: 0.0213, Val Loss: 0.0281\n",
      "Val Metrics - Acc: 0.7906, F1: 0.1705, Precision: 0.1048, Recall: 0.4583, AUPRC: 0.1606\n",
      "Early stopping at epoch 40\n",
      "\n",
      "Fold 10 Test Metrics:\n",
      "Accuracy: 0.7965\n",
      "F1 Score: 0.1875\n",
      "Precision: 0.1154\n",
      "Recall: 0.5000\n",
      "AUPRC: 0.1520\n",
      "\n",
      "==================================================\n",
      "Final Cross-Validation Results:\n",
      "==================================================\n",
      "Average Accuracy: 0.8123 ± 0.0209\n",
      "Average F1 Score: 0.2269 ± 0.0257\n",
      "Average Precision: 0.1425 ± 0.0166\n",
      "Average Recall: 0.5660 ± 0.0921\n",
      "Average AUPRC: 0.1686 ± 0.0394\n",
      "\n",
      "Plotting confusion matrix for best fold: 8\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import (accuracy_score, f1_score, precision_score,\n",
    "                             recall_score, average_precision_score,\n",
    "                             precision_recall_curve, auc, confusion_matrix)\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 设置字体为黑体，确保中文可见\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# TeLU激活函数\n",
    "class TeLU(nn.Module):\n",
    "    def __init__(self, alpha=0.15):\n",
    "        super(TeLU, self).__init__()\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.where(x >= 0, x, self.alpha * (torch.exp(x) - 1))\n",
    "\n",
    "\n",
    "# 前馈神经网络\n",
    "class FFNN(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(FFNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 32)\n",
    "        self.telu1 = TeLU(alpha=0.15)\n",
    "        self.fc2 = nn.Linear(32, 64)\n",
    "        self.telu2 = TeLU(alpha=0.1)\n",
    "        self.fc3 = nn.Linear(64, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.telu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.telu2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# 焦点损失函数\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        BCE_loss = nn.CrossEntropyLoss(reduction='none')(inputs, targets)\n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        F_loss = self.alpha * (1 - pt) ** self.gamma * BCE_loss\n",
    "        return F_loss.mean()\n",
    "\n",
    "\n",
    "def calculate_metrics(y_true, y_pred, y_scores):\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_true, y_pred),\n",
    "        'f1': f1_score(y_true, y_pred, zero_division=0),\n",
    "        'precision': precision_score(y_true, y_pred, zero_division=0),\n",
    "        'recall': recall_score(y_true, y_pred, zero_division=0),\n",
    "        'auprc': average_precision_score(y_true, y_scores[:, 1])\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, fold, dpi=720):\n",
    "    \"\"\"绘制正方形混淆矩阵\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    cm_percentage = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100  # 百分比表示\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    ax = sns.heatmap(cm_percentage, annot=False, fmt='.2f', cmap='Blues', square=True, cbar=False,\n",
    "                     linewidths=2, linecolor='black')\n",
    "\n",
    "    # 在每个格子中显示个数和百分比\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            # 判断字体颜色，深色背景用白色字体，浅色背景用黑色字体\n",
    "            text_color = 'white' if cm_percentage[i, j] > 50 else 'black'\n",
    "\n",
    "            # 将个数和百分比分行显示，个数在上，百分比在下\n",
    "            ax.text(j + 0.5, i + 0.5, f'{cm[i, j]}\\n({cm_percentage[i, j]:.2f}%)',\n",
    "                    color=text_color, ha='center', va='center', fontsize=14, fontweight='bold')\n",
    "\n",
    "    # 添加中文标签\n",
    "    plt.xlabel('预测类别', fontsize=16, fontweight='bold')\n",
    "    plt.ylabel('实际类别', fontsize=16, fontweight='bold')\n",
    "    plt.xticks(ticks=np.arange(2) + 0.5, labels=['0', '1'], fontsize=14, fontweight='bold')\n",
    "    plt.yticks(ticks=np.arange(2) + 0.5, labels=['0', '1'], fontsize=14, fontweight='bold')\n",
    "\n",
    "    # 调整布局，减少空白边缘\n",
    "    plt.subplots_adjust(left=0.1, right=0.9, top=0.9, bottom=0.1)\n",
    "\n",
    "    # 保存混淆矩阵图\n",
    "    plt.savefig(f'CI_FFNN_best_fold_confusion_matrix_fold{fold}.png', dpi=dpi)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_pr_curve(y_true, y_scores, fold):\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_scores[:, 1])\n",
    "    auprc = auc(recall, precision)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(recall, precision, label=f'Fold {fold} (AUPRC = {auprc:.2f})')\n",
    "    plt.xlabel('召回率', fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('精确率', fontsize=14, fontweight='bold')\n",
    "    plt.title('Precision-Recall 曲线', fontsize=16, fontweight='bold')\n",
    "    plt.legend()\n",
    "    plt.savefig(f'pr_curve_fold{fold}.png')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs=500, patience=20):\n",
    "    best_val_loss = float('inf')\n",
    "    best_model = None\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        # 验证\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        all_scores = []\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                all_preds.extend(predicted.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                all_scores.extend(torch.softmax(outputs, dim=1).cpu().numpy())\n",
    "\n",
    "        # 计算指标\n",
    "        train_loss = train_loss / len(train_loader.dataset)\n",
    "        val_loss = val_loss / len(val_loader.dataset)\n",
    "        metrics = calculate_metrics(all_labels, all_preds, np.array(all_scores))\n",
    "\n",
    "        # 早停\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model = copy.deepcopy(model.state_dict())\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f'Early stopping at epoch {epoch + 1}')\n",
    "                break\n",
    "\n",
    "        # 打印进度\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch {epoch + 1}/{epochs} - Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "            print(f\"Val Metrics - Acc: {metrics['accuracy']:.4f}, F1: {metrics['f1']:.4f}, \"\n",
    "                  f\"Precision: {metrics['precision']:.4f}, Recall: {metrics['recall']:.4f}, \"\n",
    "                  f\"AUPRC: {metrics['auprc']:.4f}\")\n",
    "\n",
    "    # 加载最佳模型\n",
    "    model.load_state_dict(best_model)\n",
    "    return model\n",
    "\n",
    "\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_scores = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_scores.extend(torch.softmax(outputs, dim=1).cpu().numpy())\n",
    "\n",
    "    return all_labels, all_preds, np.array(all_scores)\n",
    "\n",
    "\n",
    "def train_test_split(X, y, splits=10, epochs=500, batch_size=512, lr=0.001):\n",
    "    k_fold = StratifiedKFold(n_splits=splits, shuffle=True, random_state=2025)\n",
    "    results = []\n",
    "\n",
    "    # 存储所有折的PR曲线数据\n",
    "    all_precisions = []\n",
    "    all_recalls = []\n",
    "    \n",
    "    best_fold_metrics = None\n",
    "    best_fold = -1\n",
    "    best_y_true = None\n",
    "    best_y_pred = None\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(k_fold.split(X, y)):\n",
    "        print(f'\\n{\"=\" * 50}')\n",
    "        print(f'Fold {fold + 1}/{splits}')\n",
    "        print(f'{\"=\" * 50}')\n",
    "\n",
    "        # 分割数据\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "        # 1. 先标准化数据\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "        # 2. 应用SMOTE过采样（仅对训练数据）\n",
    "        # 检查少数类样本数量是否足够\n",
    "        min_samples = sum(y_train == 1)\n",
    "        k_neighbors = min(5, min_samples - 1) if min_samples > 1 else 1\n",
    "\n",
    "        smote = SMOTE(random_state=42, k_neighbors=k_neighbors)\n",
    "        X_train_res, y_train_res = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "        # 转换为张量\n",
    "        X_train_tensor = torch.FloatTensor(X_train_res)\n",
    "        y_train_tensor = torch.LongTensor(y_train_res)\n",
    "        X_test_tensor = torch.FloatTensor(X_test_scaled)\n",
    "        y_test_tensor = torch.LongTensor(y_test)\n",
    "\n",
    "        # 创建数据集和数据加载器\n",
    "        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "        test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "        # 初始化模型、损失函数和优化器\n",
    "        model = FFNN(input_size=X.shape[1])\n",
    "        criterion = FocalLoss(alpha=0.25, gamma=2)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "        # 学习率调度器\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=10, factor=0.5)\n",
    "\n",
    "        # 训练模型\n",
    "        model = train_model(model, train_loader, test_loader, criterion, optimizer,\n",
    "                            epochs=epochs, patience=20)\n",
    "\n",
    "        # 评估测试集\n",
    "        y_true, y_pred, y_scores = evaluate_model(model, test_loader)\n",
    "        metrics = calculate_metrics(y_true, y_pred, y_scores)\n",
    "\n",
    "        # 跟踪最佳折叠（根据AUPRC）\n",
    "        if best_fold_metrics is None or metrics['auprc'] > best_fold_metrics['auprc']:\n",
    "            best_fold_metrics = metrics\n",
    "            best_fold = fold + 1\n",
    "            best_y_true = y_true\n",
    "            best_y_pred = y_pred\n",
    "\n",
    "        # 保存PR曲线数据\n",
    "        precision, recall, _ = precision_recall_curve(y_true, y_scores[:, 1])\n",
    "        all_precisions.append(precision)\n",
    "        all_recalls.append(recall)\n",
    "\n",
    "        # 绘制当前折的PR曲线\n",
    "        plot_pr_curve(y_true, y_scores, fold + 1)\n",
    "\n",
    "        # 保存结果\n",
    "        results.append(metrics)\n",
    "\n",
    "        # 打印当前折的结果\n",
    "        print(f'\\nFold {fold + 1} Test Metrics:')\n",
    "        print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
    "        print(f\"F1 Score: {metrics['f1']:.4f}\")\n",
    "        print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "        print(f\"Recall: {metrics['recall']:.4f}\")\n",
    "        print(f\"AUPRC: {metrics['auprc']:.4f}\")\n",
    "\n",
    "    # 计算并打印平均指标\n",
    "    avg_metrics = {\n",
    "        'accuracy': np.mean([r['accuracy'] for r in results]),\n",
    "        'f1': np.mean([r['f1'] for r in results]),\n",
    "        'precision': np.mean([r['precision'] for r in results]),\n",
    "        'recall': np.mean([r['recall'] for r in results]),\n",
    "        'auprc': np.mean([r['auprc'] for r in results])\n",
    "    }\n",
    "\n",
    "    std_metrics = {\n",
    "        'accuracy': np.std([r['accuracy'] for r in results]),\n",
    "        'f1': np.std([r['f1'] for r in results]),\n",
    "        'precision': np.std([r['precision'] for r in results]),\n",
    "        'recall': np.std([r['recall'] for r in results]),\n",
    "        'auprc': np.std([r['auprc'] for r in results])\n",
    "    }\n",
    "\n",
    "    print('\\n' + '=' * 50)\n",
    "    print('Final Cross-Validation Results:')\n",
    "    print('=' * 50)\n",
    "    print(f\"Average Accuracy: {avg_metrics['accuracy']:.4f} ± {std_metrics['accuracy']:.4f}\")\n",
    "    print(f\"Average F1 Score: {avg_metrics['f1']:.4f} ± {std_metrics['f1']:.4f}\")\n",
    "    print(f\"Average Precision: {avg_metrics['precision']:.4f} ± {std_metrics['precision']:.4f}\")\n",
    "    print(f\"Average Recall: {avg_metrics['recall']:.4f} ± {std_metrics['recall']:.4f}\")\n",
    "    print(f\"Average AUPRC: {avg_metrics['auprc']:.4f} ± {std_metrics['auprc']:.4f}\")\n",
    "\n",
    "    # 绘制最佳折的混淆矩阵\n",
    "    if best_y_true is not None and best_y_pred is not None:\n",
    "        print(f\"\\nPlotting confusion matrix for best fold: {best_fold}\")\n",
    "        plot_confusion_matrix(best_y_true, best_y_pred, best_fold, dpi=720)\n",
    "\n",
    "    # 绘制平均PR曲线\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    for i in range(splits):\n",
    "        plt.plot(all_recalls[i], all_precisions[i], alpha=0.3, label=f'Fold {i + 1}')\n",
    "\n",
    "    mean_precision = np.mean([p for p in all_precisions], axis=0)\n",
    "    mean_recall = np.mean([r for r in all_recalls], axis=0)\n",
    "    mean_auprc = auc(mean_recall, mean_precision)\n",
    "\n",
    "    plt.plot(mean_recall, mean_precision, 'k-',\n",
    "             label=f'Mean (AUPRC = {mean_auprc:.2f})', linewidth=2)\n",
    "    plt.xlabel('召回率', fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('精确率', fontsize=14, fontweight='bold')\n",
    "    plt.title('平均 Precision-Recall 曲线', fontsize=16, fontweight='bold')\n",
    "    plt.legend()\n",
    "    plt.savefig('average_pr_curve.png')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# 加载数据\n",
    "data = pd.read_csv('preparations/stroke_output.csv')\n",
    "\n",
    "# 预处理分类变量\n",
    "categorical_cols = ['ever_married', 'work_type', 'smoking_status']\n",
    "data[categorical_cols] = data[categorical_cols].astype('category')\n",
    "\n",
    "# 分离特征和目标\n",
    "X = data.drop('stroke', axis=1)\n",
    "y = data['stroke'].values\n",
    "\n",
    "# 对分类变量进行独热编码\n",
    "X = pd.get_dummies(X, columns=categorical_cols)\n",
    "\n",
    "# 转换为numpy数组\n",
    "X = X.values.astype(np.float32)\n",
    "\n",
    "# 运行训练和评估\n",
    "train_test_split(X, y, splits=10, epochs=500, batch_size=512, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "423b70ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Fold 1/10\n",
      "==================================================\n",
      "Epoch 10/500 - Train Loss: 0.0115, Val Loss: 0.0117\n",
      "Val Metrics - Acc: 0.9511, F1: 0.0000, Precision: 0.0000, Recall: 0.0000, AUPRC: 0.1855\n",
      "Epoch 20/500 - Train Loss: 0.0105, Val Loss: 0.0109\n",
      "Val Metrics - Acc: 0.9511, F1: 0.0000, Precision: 0.0000, Recall: 0.0000, AUPRC: 0.2081\n",
      "Epoch 30/500 - Train Loss: 0.0101, Val Loss: 0.0110\n",
      "Val Metrics - Acc: 0.9511, F1: 0.0000, Precision: 0.0000, Recall: 0.0000, AUPRC: 0.1870\n",
      "Epoch 40/500 - Train Loss: 0.0100, Val Loss: 0.0111\n",
      "Val Metrics - Acc: 0.9491, F1: 0.0000, Precision: 0.0000, Recall: 0.0000, AUPRC: 0.1816\n",
      "Early stopping at epoch 42\n",
      "\n",
      "Fold 1 Test Metrics:\n",
      "Accuracy: 0.9511\n",
      "F1 Score: 0.0000\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "AUPRC: 0.2012\n",
      "\n",
      "==================================================\n",
      "Fold 2/10\n",
      "==================================================\n",
      "Epoch 10/500 - Train Loss: 0.0112, Val Loss: 0.0107\n",
      "Val Metrics - Acc: 0.9511, F1: 0.0000, Precision: 0.0000, Recall: 0.0000, AUPRC: 0.1902\n",
      "Epoch 20/500 - Train Loss: 0.0105, Val Loss: 0.0103\n",
      "Val Metrics - Acc: 0.9511, F1: 0.0000, Precision: 0.0000, Recall: 0.0000, AUPRC: 0.1974\n",
      "Epoch 30/500 - Train Loss: 0.0102, Val Loss: 0.0103\n",
      "Val Metrics - Acc: 0.9511, F1: 0.0000, Precision: 0.0000, Recall: 0.0000, AUPRC: 0.1894\n",
      "Epoch 40/500 - Train Loss: 0.0100, Val Loss: 0.0104\n",
      "Val Metrics - Acc: 0.9491, F1: 0.0000, Precision: 0.0000, Recall: 0.0000, AUPRC: 0.1990\n",
      "Early stopping at epoch 47\n",
      "\n",
      "Fold 2 Test Metrics:\n",
      "Accuracy: 0.9511\n",
      "F1 Score: 0.0000\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "AUPRC: 0.1990\n",
      "\n",
      "==================================================\n",
      "Fold 3/10\n",
      "==================================================\n",
      "Epoch 10/500 - Train Loss: 0.0112, Val Loss: 0.0105\n",
      "Val Metrics - Acc: 0.9511, F1: 0.0000, Precision: 0.0000, Recall: 0.0000, AUPRC: 0.2649\n",
      "Epoch 20/500 - Train Loss: 0.0105, Val Loss: 0.0103\n",
      "Val Metrics - Acc: 0.9511, F1: 0.0000, Precision: 0.0000, Recall: 0.0000, AUPRC: 0.2990\n",
      "Epoch 30/500 - Train Loss: 0.0102, Val Loss: 0.0103\n",
      "Val Metrics - Acc: 0.9511, F1: 0.0000, Precision: 0.0000, Recall: 0.0000, AUPRC: 0.2988\n",
      "Epoch 40/500 - Train Loss: 0.0100, Val Loss: 0.0103\n",
      "Val Metrics - Acc: 0.9511, F1: 0.0000, Precision: 0.0000, Recall: 0.0000, AUPRC: 0.2817\n",
      "Early stopping at epoch 46\n",
      "\n",
      "Fold 3 Test Metrics:\n",
      "Accuracy: 0.9511\n",
      "F1 Score: 0.0000\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "AUPRC: 0.2968\n",
      "\n",
      "==================================================\n",
      "Fold 4/10\n",
      "==================================================\n",
      "Epoch 10/500 - Train Loss: 0.0108, Val Loss: 0.0116\n",
      "Val Metrics - Acc: 0.9511, F1: 0.0000, Precision: 0.0000, Recall: 0.0000, AUPRC: 0.1479\n",
      "Epoch 20/500 - Train Loss: 0.0102, Val Loss: 0.0115\n",
      "Val Metrics - Acc: 0.9511, F1: 0.0000, Precision: 0.0000, Recall: 0.0000, AUPRC: 0.1423\n",
      "Epoch 30/500 - Train Loss: 0.0100, Val Loss: 0.0115\n",
      "Val Metrics - Acc: 0.9511, F1: 0.0000, Precision: 0.0000, Recall: 0.0000, AUPRC: 0.1368\n",
      "Epoch 40/500 - Train Loss: 0.0098, Val Loss: 0.0115\n",
      "Val Metrics - Acc: 0.9511, F1: 0.0000, Precision: 0.0000, Recall: 0.0000, AUPRC: 0.1383\n",
      "Early stopping at epoch 49\n",
      "\n",
      "Fold 4 Test Metrics:\n",
      "Accuracy: 0.9511\n",
      "F1 Score: 0.0000\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "AUPRC: 0.1402\n",
      "\n",
      "==================================================\n",
      "Fold 5/10\n",
      "==================================================\n",
      "Epoch 10/500 - Train Loss: 0.0109, Val Loss: 0.0117\n",
      "Val Metrics - Acc: 0.9511, F1: 0.0000, Precision: 0.0000, Recall: 0.0000, AUPRC: 0.1544\n",
      "Epoch 20/500 - Train Loss: 0.0103, Val Loss: 0.0114\n",
      "Val Metrics - Acc: 0.9511, F1: 0.0000, Precision: 0.0000, Recall: 0.0000, AUPRC: 0.1763\n",
      "Epoch 30/500 - Train Loss: 0.0100, Val Loss: 0.0114\n",
      "Val Metrics - Acc: 0.9511, F1: 0.0000, Precision: 0.0000, Recall: 0.0000, AUPRC: 0.1786\n",
      "Early stopping at epoch 37\n",
      "\n",
      "Fold 5 Test Metrics:\n",
      "Accuracy: 0.9511\n",
      "F1 Score: 0.0000\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "AUPRC: 0.1817\n",
      "\n",
      "==================================================\n",
      "Fold 6/10\n",
      "==================================================\n",
      "Epoch 10/500 - Train Loss: 0.0114, Val Loss: 0.0125\n",
      "Val Metrics - Acc: 0.9511, F1: 0.0000, Precision: 0.0000, Recall: 0.0000, AUPRC: 0.1101\n",
      "Epoch 20/500 - Train Loss: 0.0104, Val Loss: 0.0122\n",
      "Val Metrics - Acc: 0.9511, F1: 0.0000, Precision: 0.0000, Recall: 0.0000, AUPRC: 0.1652\n",
      "Epoch 30/500 - Train Loss: 0.0100, Val Loss: 0.0125\n",
      "Val Metrics - Acc: 0.9511, F1: 0.0000, Precision: 0.0000, Recall: 0.0000, AUPRC: 0.1579\n",
      "Early stopping at epoch 38\n",
      "\n",
      "Fold 6 Test Metrics:\n",
      "Accuracy: 0.9511\n",
      "F1 Score: 0.0000\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "AUPRC: 0.1395\n",
      "\n",
      "==================================================\n",
      "Fold 7/10\n",
      "==================================================\n",
      "Epoch 10/500 - Train Loss: 0.0111, Val Loss: 0.0116\n",
      "Val Metrics - Acc: 0.9511, F1: 0.0000, Precision: 0.0000, Recall: 0.0000, AUPRC: 0.1999\n",
      "Epoch 20/500 - Train Loss: 0.0104, Val Loss: 0.0110\n",
      "Val Metrics - Acc: 0.9511, F1: 0.0000, Precision: 0.0000, Recall: 0.0000, AUPRC: 0.1777\n",
      "Epoch 30/500 - Train Loss: 0.0101, Val Loss: 0.0110\n",
      "Val Metrics - Acc: 0.9511, F1: 0.0000, Precision: 0.0000, Recall: 0.0000, AUPRC: 0.1607\n",
      "Epoch 40/500 - Train Loss: 0.0100, Val Loss: 0.0109\n",
      "Val Metrics - Acc: 0.9511, F1: 0.0000, Precision: 0.0000, Recall: 0.0000, AUPRC: 0.1680\n",
      "Epoch 50/500 - Train Loss: 0.0098, Val Loss: 0.0110\n",
      "Val Metrics - Acc: 0.9511, F1: 0.0000, Precision: 0.0000, Recall: 0.0000, AUPRC: 0.1650\n",
      "Epoch 60/500 - Train Loss: 0.0097, Val Loss: 0.0110\n",
      "Val Metrics - Acc: 0.9491, F1: 0.0000, Precision: 0.0000, Recall: 0.0000, AUPRC: 0.1672\n",
      "Early stopping at epoch 68\n",
      "\n",
      "Fold 7 Test Metrics:\n",
      "Accuracy: 0.9491\n",
      "F1 Score: 0.0000\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "AUPRC: 0.1686\n",
      "\n",
      "==================================================\n",
      "Fold 8/10\n",
      "==================================================\n",
      "Epoch 10/500 - Train Loss: 0.0112, Val Loss: 0.0108\n",
      "Val Metrics - Acc: 0.9511, F1: 0.0000, Precision: 0.0000, Recall: 0.0000, AUPRC: 0.1616\n",
      "Epoch 20/500 - Train Loss: 0.0105, Val Loss: 0.0102\n",
      "Val Metrics - Acc: 0.9511, F1: 0.0000, Precision: 0.0000, Recall: 0.0000, AUPRC: 0.1997\n",
      "Epoch 30/500 - Train Loss: 0.0102, Val Loss: 0.0099\n",
      "Val Metrics - Acc: 0.9511, F1: 0.0000, Precision: 0.0000, Recall: 0.0000, AUPRC: 0.2488\n",
      "Epoch 40/500 - Train Loss: 0.0100, Val Loss: 0.0098\n",
      "Val Metrics - Acc: 0.9511, F1: 0.0000, Precision: 0.0000, Recall: 0.0000, AUPRC: 0.2605\n",
      "Epoch 50/500 - Train Loss: 0.0099, Val Loss: 0.0098\n",
      "Val Metrics - Acc: 0.9511, F1: 0.0000, Precision: 0.0000, Recall: 0.0000, AUPRC: 0.2758\n",
      "Epoch 60/500 - Train Loss: 0.0097, Val Loss: 0.0098\n",
      "Val Metrics - Acc: 0.9530, F1: 0.0769, Precision: 1.0000, Recall: 0.0400, AUPRC: 0.2777\n",
      "Epoch 70/500 - Train Loss: 0.0096, Val Loss: 0.0098\n",
      "Val Metrics - Acc: 0.9530, F1: 0.0769, Precision: 1.0000, Recall: 0.0400, AUPRC: 0.2744\n",
      "Early stopping at epoch 74\n",
      "\n",
      "Fold 8 Test Metrics:\n",
      "Accuracy: 0.9511\n",
      "F1 Score: 0.0000\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "AUPRC: 0.2800\n",
      "\n",
      "==================================================\n",
      "Fold 9/10\n",
      "==================================================\n",
      "Epoch 10/500 - Train Loss: 0.0112, Val Loss: 0.0113\n",
      "Val Metrics - Acc: 0.9511, F1: 0.0000, Precision: 0.0000, Recall: 0.0000, AUPRC: 0.1543\n",
      "Epoch 20/500 - Train Loss: 0.0105, Val Loss: 0.0103\n",
      "Val Metrics - Acc: 0.9511, F1: 0.0000, Precision: 0.0000, Recall: 0.0000, AUPRC: 0.2063\n",
      "Epoch 30/500 - Train Loss: 0.0101, Val Loss: 0.0101\n",
      "Val Metrics - Acc: 0.9530, F1: 0.0769, Precision: 1.0000, Recall: 0.0400, AUPRC: 0.2148\n",
      "Epoch 40/500 - Train Loss: 0.0100, Val Loss: 0.0100\n",
      "Val Metrics - Acc: 0.9530, F1: 0.0769, Precision: 1.0000, Recall: 0.0400, AUPRC: 0.2613\n",
      "Epoch 50/500 - Train Loss: 0.0098, Val Loss: 0.0101\n",
      "Val Metrics - Acc: 0.9530, F1: 0.0769, Precision: 1.0000, Recall: 0.0400, AUPRC: 0.2704\n",
      "Early stopping at epoch 60\n",
      "\n",
      "Fold 9 Test Metrics:\n",
      "Accuracy: 0.9530\n",
      "F1 Score: 0.0769\n",
      "Precision: 1.0000\n",
      "Recall: 0.0400\n",
      "AUPRC: 0.2613\n",
      "\n",
      "==================================================\n",
      "Fold 10/10\n",
      "==================================================\n",
      "Epoch 10/500 - Train Loss: 0.0110, Val Loss: 0.0121\n",
      "Val Metrics - Acc: 0.9530, F1: 0.0000, Precision: 0.0000, Recall: 0.0000, AUPRC: 0.1700\n",
      "Epoch 20/500 - Train Loss: 0.0103, Val Loss: 0.0121\n",
      "Val Metrics - Acc: 0.9530, F1: 0.0000, Precision: 0.0000, Recall: 0.0000, AUPRC: 0.2010\n",
      "Epoch 30/500 - Train Loss: 0.0100, Val Loss: 0.0123\n",
      "Val Metrics - Acc: 0.9530, F1: 0.0000, Precision: 0.0000, Recall: 0.0000, AUPRC: 0.1955\n",
      "Early stopping at epoch 32\n",
      "\n",
      "Fold 10 Test Metrics:\n",
      "Accuracy: 0.9530\n",
      "F1 Score: 0.0000\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "AUPRC: 0.1677\n",
      "\n",
      "==================================================\n",
      "Final Cross-Validation Results:\n",
      "==================================================\n",
      "Average Accuracy: 0.9513 ± 0.0011\n",
      "Average F1 Score: 0.0077 ± 0.0231\n",
      "Average Precision: 0.1000 ± 0.3000\n",
      "Average Recall: 0.0040 ± 0.0120\n",
      "Average AUPRC: 0.2036 ± 0.0539\n",
      "\n",
      "Plotting confusion matrix for best fold: 3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import (accuracy_score, f1_score, precision_score,\n",
    "                             recall_score, average_precision_score,\n",
    "                             precision_recall_curve, auc, confusion_matrix)\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "# 设置字体为黑体，确保中文可见\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# TeLU激活函数\n",
    "class TeLU(nn.Module):\n",
    "    def __init__(self, alpha=0.15):\n",
    "        super(TeLU, self).__init__()\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.where(x >= 0, x, self.alpha * (torch.exp(x) - 1))\n",
    "\n",
    "\n",
    "# 前馈神经网络\n",
    "class FFNN(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(FFNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 32)\n",
    "        self.telu1 = TeLU(alpha=0.15)\n",
    "        self.fc2 = nn.Linear(32, 64)\n",
    "        self.telu2 = TeLU(alpha=0.1)\n",
    "        self.fc3 = nn.Linear(64, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.telu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.telu2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# 焦点损失函数\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        BCE_loss = nn.CrossEntropyLoss(reduction='none')(inputs, targets)\n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        F_loss = self.alpha * (1 - pt) ** self.gamma * BCE_loss\n",
    "        return F_loss.mean()\n",
    "\n",
    "\n",
    "def calculate_metrics(y_true, y_pred, y_scores):\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_true, y_pred),\n",
    "        'f1': f1_score(y_true, y_pred),\n",
    "        'precision': precision_score(y_true, y_pred),\n",
    "        'recall': recall_score(y_true, y_pred),\n",
    "        'auprc': average_precision_score(y_true, y_scores[:, 1])\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, fold, dpi=720):\n",
    "    \"\"\"绘制正方形混淆矩阵\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    cm_percentage = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100  # 百分比表示\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    ax = sns.heatmap(cm_percentage, annot=False, fmt='.2f', cmap='Blues', square=True, cbar=False,\n",
    "                     linewidths=2, linecolor='black')\n",
    "\n",
    "    # 在每个格子中显示个数和百分比\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            # 判断字体颜色，深色背景用白色字体，浅色背景用黑色字体\n",
    "            text_color = 'white' if cm_percentage[i, j] > 50 else 'black'\n",
    "            ax.text(j + 0.5, i + 0.5, f'{cm[i, j]}\\n({cm_percentage[i, j]:.2f}%)',\n",
    "                    color=text_color, ha='center', va='center', fontsize=14, fontweight='bold')\n",
    "\n",
    "    # 添加中文标签\n",
    "    plt.xlabel('预测类别', fontsize=16, fontweight='bold')\n",
    "    plt.ylabel('实际类别', fontsize=16, fontweight='bold')\n",
    "    plt.xticks(ticks=np.arange(cm.shape[1]) + 0.5, labels=np.arange(cm.shape[1]), fontsize=14, fontweight='bold')\n",
    "    plt.yticks(ticks=np.arange(cm.shape[0]) + 0.5, labels=np.arange(cm.shape[0]), fontsize=14, fontweight='bold')\n",
    "\n",
    "    # 调整布局，减少空白边缘\n",
    "    plt.subplots_adjust(left=0.1, right=0.9, top=0.9, bottom=0.1)\n",
    "\n",
    "    # 保存混淆矩阵图\n",
    "    plt.savefig(f'FFNN_best_fold_confusion_matrix_fold{fold}.png', dpi=dpi)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_pr_curve(y_true, y_scores, fold):\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_scores[:, 1])\n",
    "    auprc = auc(recall, precision)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(recall, precision, label=f'Fold {fold} (AUPRC = {auprc:.2f})')\n",
    "    plt.xlabel('召回率', fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('精确率', fontsize=14, fontweight='bold')\n",
    "    plt.title('Precision-Recall 曲线', fontsize=16, fontweight='bold')\n",
    "    plt.legend()\n",
    "    plt.savefig(f'pr_curve_fold{fold}.png')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs=500, patience=20):\n",
    "    best_val_loss = float('inf')\n",
    "    best_model = None\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        # 验证\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        all_scores = []\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                all_preds.extend(predicted.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                all_scores.extend(torch.softmax(outputs, dim=1).cpu().numpy())\n",
    "\n",
    "        # 计算指标\n",
    "        train_loss = train_loss / len(train_loader.dataset)\n",
    "        val_loss = val_loss / len(val_loader.dataset)\n",
    "        metrics = calculate_metrics(all_labels, all_preds, np.array(all_scores))\n",
    "\n",
    "        # 早停\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model = copy.deepcopy(model.state_dict())\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f'Early stopping at epoch {epoch + 1}')\n",
    "                break\n",
    "\n",
    "        # 打印进度\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch {epoch + 1}/{epochs} - Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "            print(f\"Val Metrics - Acc: {metrics['accuracy']:.4f}, F1: {metrics['f1']:.4f}, \"\n",
    "                  f\"Precision: {metrics['precision']:.4f}, Recall: {metrics['recall']:.4f}, \"\n",
    "                  f\"AUPRC: {metrics['auprc']:.4f}\")\n",
    "\n",
    "    # 加载最佳模型\n",
    "    model.load_state_dict(best_model)\n",
    "    return model\n",
    "\n",
    "\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_scores = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_scores.extend(torch.softmax(outputs, dim=1).cpu().numpy())\n",
    "\n",
    "    return all_labels, all_preds, np.array(all_scores)\n",
    "\n",
    "\n",
    "def train_test_split(X, y, splits=10, epochs=500, batch_size=512, lr=0.001):\n",
    "    k_fold = StratifiedKFold(n_splits=splits, shuffle=True, random_state=2025)\n",
    "    results = []\n",
    "\n",
    "    # 存储所有折的PR曲线数据\n",
    "    all_precisions = []\n",
    "    all_recalls = []\n",
    "    \n",
    "    best_fold_metrics = None\n",
    "    best_fold = -1\n",
    "    best_y_true = None\n",
    "    best_y_pred = None\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(k_fold.split(X, y)):\n",
    "        print(f'\\n{\"=\" * 50}')\n",
    "        print(f'Fold {fold + 1}/{splits}')\n",
    "        print(f'{\"=\" * 50}')\n",
    "\n",
    "        # 分割数据\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "        # 标准化\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "        # 转换为张量\n",
    "        X_train_tensor = torch.FloatTensor(X_train)\n",
    "        y_train_tensor = torch.LongTensor(y_train)\n",
    "        X_test_tensor = torch.FloatTensor(X_test)\n",
    "        y_test_tensor = torch.LongTensor(y_test)\n",
    "\n",
    "        # 创建数据集和数据加载器\n",
    "        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "        test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "        # 初始化模型、损失函数和优化器\n",
    "        model = FFNN(input_size=X.shape[1])\n",
    "        criterion = FocalLoss(alpha=0.25, gamma=2)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "        # 学习率调度器\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=10, factor=0.5)\n",
    "\n",
    "        # 训练模型\n",
    "        model = train_model(model, train_loader, test_loader, criterion, optimizer,\n",
    "                            epochs=epochs, patience=20)\n",
    "\n",
    "        # 评估测试集\n",
    "        y_true, y_pred, y_scores = evaluate_model(model, test_loader)\n",
    "        metrics = calculate_metrics(y_true, y_pred, y_scores)\n",
    "\n",
    "        # 跟踪最佳折叠（根据AUPRC）\n",
    "        if best_fold_metrics is None or metrics['auprc'] > best_fold_metrics['auprc']:\n",
    "            best_fold_metrics = metrics\n",
    "            best_fold = fold + 1\n",
    "            best_y_true = y_true\n",
    "            best_y_pred = y_pred\n",
    "\n",
    "        # 保存PR曲线数据\n",
    "        precision, recall, _ = precision_recall_curve(y_true, y_scores[:, 1])\n",
    "        all_precisions.append(precision)\n",
    "        all_recalls.append(recall)\n",
    "\n",
    "        # 绘制当前折的PR曲线\n",
    "        plot_pr_curve(y_true, y_scores, fold + 1)\n",
    "\n",
    "        # 保存结果\n",
    "        results.append(metrics)\n",
    "\n",
    "        # 打印当前折的结果\n",
    "        print(f'\\nFold {fold + 1} Test Metrics:')\n",
    "        print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
    "        print(f\"F1 Score: {metrics['f1']:.4f}\")\n",
    "        print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "        print(f\"Recall: {metrics['recall']:.4f}\")\n",
    "        print(f\"AUPRC: {metrics['auprc']:.4f}\")\n",
    "\n",
    "    # 计算并打印平均指标\n",
    "    avg_metrics = {\n",
    "        'accuracy': np.mean([r['accuracy'] for r in results]),\n",
    "        'f1': np.mean([r['f1'] for r in results]),\n",
    "        'precision': np.mean([r['precision'] for r in results]),\n",
    "        'recall': np.mean([r['recall'] for r in results]),\n",
    "        'auprc': np.mean([r['auprc'] for r in results])\n",
    "    }\n",
    "\n",
    "    std_metrics = {\n",
    "        'accuracy': np.std([r['accuracy'] for r in results]),\n",
    "        'f1': np.std([r['f1'] for r in results]),\n",
    "        'precision': np.std([r['precision'] for r in results]),\n",
    "        'recall': np.std([r['recall'] for r in results]),\n",
    "        'auprc': np.std([r['auprc'] for r in results])\n",
    "    }\n",
    "\n",
    "    print('\\n' + '=' * 50)\n",
    "    print('Final Cross-Validation Results:')\n",
    "    print('=' * 50)\n",
    "    print(f\"Average Accuracy: {avg_metrics['accuracy']:.4f} ± {std_metrics['accuracy']:.4f}\")\n",
    "    print(f\"Average F1 Score: {avg_metrics['f1']:.4f} ± {std_metrics['f1']:.4f}\")\n",
    "    print(f\"Average Precision: {avg_metrics['precision']:.4f} ± {std_metrics['precision']:.4f}\")\n",
    "    print(f\"Average Recall: {avg_metrics['recall']:.4f} ± {std_metrics['recall']:.4f}\")\n",
    "    print(f\"Average AUPRC: {avg_metrics['auprc']:.4f} ± {std_metrics['auprc']:.4f}\")\n",
    "\n",
    "    # 绘制最佳折的混淆矩阵\n",
    "    if best_y_true is not None and best_y_pred is not None:\n",
    "        print(f\"\\nPlotting confusion matrix for best fold: {best_fold}\")\n",
    "        plot_confusion_matrix(best_y_true, best_y_pred, best_fold, dpi=720)\n",
    "\n",
    "    # 绘制平均PR曲线\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    for i in range(splits):\n",
    "        plt.plot(all_recalls[i], all_precisions[i], alpha=0.3, label=f'Fold {i + 1}')\n",
    "\n",
    "    mean_precision = np.mean([p for p in all_precisions], axis=0)\n",
    "    mean_recall = np.mean([r for r in all_recalls], axis=0)\n",
    "    mean_auprc = auc(mean_recall, mean_precision)\n",
    "\n",
    "    plt.plot(mean_recall, mean_precision, 'k-',\n",
    "             label=f'Mean (AUPRC = {mean_auprc:.2f})', linewidth=2)\n",
    "    plt.xlabel('召回率', fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('精确率', fontsize=14, fontweight='bold')\n",
    "    plt.title('平均 Precision-Recall 曲线', fontsize=16, fontweight='bold')\n",
    "    plt.legend()\n",
    "    plt.savefig('average_pr_curve.png')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# 加载数据\n",
    "data = pd.read_csv('preparations/stroke_output.csv')\n",
    "\n",
    "# 预处理分类变量\n",
    "categorical_cols = ['ever_married', 'work_type', 'smoking_status']\n",
    "data[categorical_cols] = data[categorical_cols].astype('category')\n",
    "\n",
    "# 分离特征和目标\n",
    "X = data.drop('stroke', axis=1)\n",
    "y = data['stroke'].values\n",
    "\n",
    "# 对分类变量进行独热编码\n",
    "X = pd.get_dummies(X, columns=categorical_cols)\n",
    "\n",
    "# 转换为numpy数组\n",
    "X = X.values.astype(np.float32)\n",
    "\n",
    "# 运行训练和评估\n",
    "train_test_split(X, y, splits=10, epochs=500, batch_size=512, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ffe381a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting cross-validation...\n",
      "\n",
      "======================================== Fold 1/10 ========================================\n",
      "[50]\tvalid_0's binary_logloss: 0.303693\tvalid_0's auc: 0.839095\n",
      "\n",
      "Fold 1 Metrics:\n",
      "Accuracy: 0.8356\n",
      "F1 Score: 0.2759\n",
      "Precision: 0.1758\n",
      "Recall/Sensitivity: 0.6400\n",
      "Specificity: 0.8457\n",
      "AUPRC: 0.2036\n",
      "Confusion Matrix:\n",
      "TN: 411 | FP: 75\n",
      "FN: 9 | TP: 16\n",
      "\n",
      "======================================== Fold 2/10 ========================================\n",
      "[50]\tvalid_0's binary_logloss: 0.30531\tvalid_0's auc: 0.789712\n",
      "\n",
      "Fold 2 Metrics:\n",
      "Accuracy: 0.8121\n",
      "F1 Score: 0.2500\n",
      "Precision: 0.1553\n",
      "Recall/Sensitivity: 0.6400\n",
      "Specificity: 0.8210\n",
      "AUPRC: 0.1868\n",
      "Confusion Matrix:\n",
      "TN: 399 | FP: 87\n",
      "FN: 9 | TP: 16\n",
      "\n",
      "======================================== Fold 3/10 ========================================\n",
      "[50]\tvalid_0's binary_logloss: 0.320179\tvalid_0's auc: 0.878107\n",
      "\n",
      "Fold 3 Metrics:\n",
      "Accuracy: 0.7671\n",
      "F1 Score: 0.2699\n",
      "Precision: 0.1594\n",
      "Recall/Sensitivity: 0.8800\n",
      "Specificity: 0.7613\n",
      "AUPRC: 0.2353\n",
      "Confusion Matrix:\n",
      "TN: 370 | FP: 116\n",
      "FN: 3 | TP: 22\n",
      "\n",
      "======================================== Fold 4/10 ========================================\n",
      "[50]\tvalid_0's binary_logloss: 0.318758\tvalid_0's auc: 0.832716\n",
      "[100]\tvalid_0's binary_logloss: 0.240525\tvalid_0's auc: 0.816214\n",
      "\n",
      "Fold 4 Metrics:\n",
      "Accuracy: 0.8787\n",
      "F1 Score: 0.2791\n",
      "Precision: 0.1967\n",
      "Recall/Sensitivity: 0.4800\n",
      "Specificity: 0.8992\n",
      "AUPRC: 0.1755\n",
      "Confusion Matrix:\n",
      "TN: 437 | FP: 49\n",
      "FN: 13 | TP: 12\n",
      "\n",
      "======================================== Fold 5/10 ========================================\n",
      "[50]\tvalid_0's binary_logloss: 0.308531\tvalid_0's auc: 0.837778\n",
      "\n",
      "Fold 5 Metrics:\n",
      "Accuracy: 0.8748\n",
      "F1 Score: 0.3043\n",
      "Precision: 0.2090\n",
      "Recall/Sensitivity: 0.5600\n",
      "Specificity: 0.8909\n",
      "AUPRC: 0.1868\n",
      "Confusion Matrix:\n",
      "TN: 433 | FP: 53\n",
      "FN: 11 | TP: 14\n",
      "\n",
      "======================================== Fold 6/10 ========================================\n",
      "[50]\tvalid_0's binary_logloss: 0.314935\tvalid_0's auc: 0.820247\n",
      "\n",
      "Fold 6 Metrics:\n",
      "Accuracy: 0.7945\n",
      "F1 Score: 0.2553\n",
      "Precision: 0.1552\n",
      "Recall/Sensitivity: 0.7200\n",
      "Specificity: 0.7984\n",
      "AUPRC: 0.1909\n",
      "Confusion Matrix:\n",
      "TN: 388 | FP: 98\n",
      "FN: 7 | TP: 18\n",
      "\n",
      "======================================== Fold 7/10 ========================================\n",
      "[50]\tvalid_0's binary_logloss: 0.296162\tvalid_0's auc: 0.835597\n",
      "\n",
      "Fold 7 Metrics:\n",
      "Accuracy: 0.8063\n",
      "F1 Score: 0.2774\n",
      "Precision: 0.1696\n",
      "Recall/Sensitivity: 0.7600\n",
      "Specificity: 0.8086\n",
      "AUPRC: 0.1849\n",
      "Confusion Matrix:\n",
      "TN: 393 | FP: 93\n",
      "FN: 6 | TP: 19\n",
      "\n",
      "======================================== Fold 8/10 ========================================\n",
      "[50]\tvalid_0's binary_logloss: 0.308999\tvalid_0's auc: 0.867407\n",
      "[100]\tvalid_0's binary_logloss: 0.230597\tvalid_0's auc: 0.85679\n",
      "\n",
      "Fold 8 Metrics:\n",
      "Accuracy: 0.8571\n",
      "F1 Score: 0.3048\n",
      "Precision: 0.2000\n",
      "Recall/Sensitivity: 0.6400\n",
      "Specificity: 0.8683\n",
      "AUPRC: 0.1979\n",
      "Confusion Matrix:\n",
      "TN: 422 | FP: 64\n",
      "FN: 9 | TP: 16\n",
      "\n",
      "======================================== Fold 9/10 ========================================\n",
      "[50]\tvalid_0's binary_logloss: 0.324469\tvalid_0's auc: 0.815556\n",
      "[100]\tvalid_0's binary_logloss: 0.236927\tvalid_0's auc: 0.831605\n",
      "\n",
      "Fold 9 Metrics:\n",
      "Accuracy: 0.9100\n",
      "F1 Score: 0.3235\n",
      "Precision: 0.2558\n",
      "Recall/Sensitivity: 0.4400\n",
      "Specificity: 0.9342\n",
      "AUPRC: 0.2305\n",
      "Confusion Matrix:\n",
      "TN: 454 | FP: 32\n",
      "FN: 14 | TP: 11\n",
      "\n",
      "======================================== Fold 10/10 ========================================\n",
      "[50]\tvalid_0's binary_logloss: 0.333716\tvalid_0's auc: 0.804415\n",
      "\n",
      "Fold 10 Metrics:\n",
      "Accuracy: 0.8004\n",
      "F1 Score: 0.2031\n",
      "Precision: 0.1250\n",
      "Recall/Sensitivity: 0.5417\n",
      "Specificity: 0.8131\n",
      "AUPRC: 0.1702\n",
      "Confusion Matrix:\n",
      "TN: 396 | FP: 91\n",
      "FN: 11 | TP: 13\n",
      "\n",
      "Plotting confusion matrix for best fold: 3\n",
      "\n",
      "==================================================\n",
      "Final Cross-Validation Results:\n",
      "==================================================\n",
      "Average Accuracy: 0.8337 ± 0.0429\n",
      "Average F1 Score: 0.2743 ± 0.0321\n",
      "Average Precision: 0.1802 ± 0.0347\n",
      "Average Recall/Sensitivity: 0.6302 ± 0.1261\n",
      "Average Specificity: 0.8441 ± 0.0506\n",
      "Average AUPRC: 0.1962 ± 0.0205\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import (accuracy_score, f1_score, precision_score,\n",
    "                             recall_score, average_precision_score,\n",
    "                             precision_recall_curve, auc, confusion_matrix)\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import interpolate\n",
    "\n",
    "# 设置字体为黑体，确保中文可见\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, fold, dpi=720):\n",
    "    \"\"\"绘制正方形混淆矩阵\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    cm_percentage = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100  # 百分比表示\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    ax = sns.heatmap(cm_percentage, annot=False, fmt='.2f', cmap='Blues', square=True, cbar=False,\n",
    "                     linewidths=2, linecolor='black')\n",
    "\n",
    "    # 在每个格子中显示个数和百分比\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            # 判断字体颜色，深色背景用白色字体，浅色背景用黑色字体\n",
    "            text_color = 'white' if cm_percentage[i, j] > 50 else 'black'\n",
    "            ax.text(j + 0.5, i + 0.5, f'{cm[i, j]}\\n({cm_percentage[i, j]:.2f}%)',\n",
    "                    color=text_color, ha='center', va='center', fontsize=14, fontweight='bold')\n",
    "\n",
    "    # 添加中文标签\n",
    "    plt.xlabel('预测类别', fontsize=16, fontweight='bold')\n",
    "    plt.ylabel('实际类别', fontsize=16, fontweight='bold')\n",
    "    plt.xticks(ticks=np.arange(cm.shape[1]) + 0.5, labels=['0', '1'], fontsize=14, fontweight='bold')\n",
    "    plt.yticks(ticks=np.arange(cm.shape[0]) + 0.5, labels=['0', '1'], fontsize=14, fontweight='bold')\n",
    "\n",
    "    # 调整布局，减少空白边缘\n",
    "    plt.subplots_adjust(left=0.1, right=0.9, top=0.9, bottom=0.1)\n",
    "\n",
    "    # 保存混淆矩阵图\n",
    "    plt.savefig(f'LGB_best_fold_confusion_matrix_fold{fold}.png', dpi=dpi)\n",
    "    plt.close()\n",
    "\n",
    "# 解决PR曲线长度不一致的问题\n",
    "def interpolate_pr_curve(precision, recall, num_points=100):\n",
    "    \"\"\"将PR曲线插值到固定长度的点\"\"\"\n",
    "    if len(precision) < 2 or len(recall) < 2:\n",
    "        return np.linspace(0, 1, num_points), np.linspace(1, 0, num_points)\n",
    "\n",
    "    # 确保recall是单调递增的\n",
    "    sorted_indices = np.argsort(recall)\n",
    "    recall = np.array(recall)[sorted_indices]\n",
    "    precision = np.array(precision)[sorted_indices]\n",
    "\n",
    "    # 插值\n",
    "    f = interpolate.interp1d(recall, precision, bounds_error=False, fill_value=(precision[0], precision[-1]))\n",
    "    new_recall = np.linspace(0, 1, num_points)\n",
    "    new_precision = f(new_recall)\n",
    "    return new_precision, new_recall\n",
    "\n",
    "\n",
    "def calculate_metrics(y_true, y_pred, y_scores):\n",
    "    \"\"\"计算评估指标\"\"\"\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_true, y_pred),\n",
    "        'f1': f1_score(y_true, y_pred, zero_division=0),\n",
    "        'precision': precision_score(y_true, y_pred, zero_division=0),\n",
    "        'recall': recall_score(y_true, y_pred, zero_division=0),\n",
    "        'auprc': average_precision_score(y_true, y_scores)\n",
    "    }\n",
    "\n",
    "    # 添加混淆矩阵信息\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    metrics.update({\n",
    "        'confusion_matrix': {\n",
    "            'TN': tn, 'FP': fp, 'FN': fn, 'TP': tp\n",
    "        },\n",
    "        'specificity': tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    })\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def plot_pr_curve(y_true, y_scores, fold, save_path=None):\n",
    "    \"\"\"绘制PR曲线并返回插值后的数据\"\"\"\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_scores)\n",
    "    auprc = auc(recall, precision)\n",
    "\n",
    "    # 插值到固定长度\n",
    "    interp_precision, interp_recall = interpolate_pr_curve(precision, recall)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(recall, precision, label=f'Fold {fold} (AUPRC = {auprc:.3f})')\n",
    "    plt.xlabel('召回率', fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('精确率', fontsize=14, fontweight='bold')\n",
    "    plt.title(f'PR Curve (Fold {fold})', fontsize=16, fontweight='bold')\n",
    "    plt.legend()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "    return interp_precision, interp_recall, auprc\n",
    "\n",
    "\n",
    "def train_lgb_model(X_train, y_train, X_val, y_val):\n",
    "    \"\"\"训练LightGBM模型\"\"\"\n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': ['binary_logloss', 'auc'],\n",
    "        'boosting_type': 'gbdt',\n",
    "        'num_leaves': 31,\n",
    "        'learning_rate': 0.05,\n",
    "        'feature_fraction': 0.8,\n",
    "        'bagging_fraction': 0.8,\n",
    "        'bagging_freq': 5,\n",
    "        'verbose': -1,\n",
    "        'seed': 42,\n",
    "        'is_unbalance': True,  # 处理类别不平衡\n",
    "        'min_child_samples': 20\n",
    "    }\n",
    "\n",
    "    train_data = lgb.Dataset(X_train, label=y_train)\n",
    "    val_data = lgb.Dataset(X_val, label=y_val)\n",
    "\n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        train_data,\n",
    "        valid_sets=[val_data],\n",
    "        num_boost_round=1000,\n",
    "        callbacks=[\n",
    "            lgb.early_stopping(stopping_rounds=50, verbose=False),\n",
    "            lgb.log_evaluation(period=50)\n",
    "        ]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def cross_validate(X, y, n_splits=10):\n",
    "    \"\"\"执行交叉验证\"\"\"\n",
    "    kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    results = []\n",
    "    all_interp_precisions = []\n",
    "    interp_recall = np.linspace(0, 1, 100)  # 固定100个recall点\n",
    "    \n",
    "    best_fold_metrics = None\n",
    "    best_fold = -1\n",
    "    best_y_true = None\n",
    "    best_y_pred = None\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(kf.split(X, y), 1):\n",
    "        print(f'\\n{\"=\" * 40} Fold {fold}/{n_splits} {\"=\" * 40}')\n",
    "\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "        # 标准化\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "        # SMOTE过采样\n",
    "        smote = SMOTE(random_state=42, k_neighbors=min(5, sum(y_train == 1) - 1))\n",
    "        X_res, y_res = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "        # 训练模型\n",
    "        model = train_lgb_model(X_res, y_res, X_test_scaled, y_test)\n",
    "\n",
    "        # 预测\n",
    "        y_scores = model.predict(X_test_scaled)\n",
    "        y_pred = (y_scores >= 0.5).astype(int)\n",
    "\n",
    "        # 计算指标\n",
    "        metrics = calculate_metrics(y_test, y_pred, y_scores)\n",
    "        results.append(metrics)\n",
    "        \n",
    "        # 跟踪最佳折叠（根据AUPRC）\n",
    "        if best_fold_metrics is None or metrics['auprc'] > best_fold_metrics['auprc']:\n",
    "            best_fold_metrics = metrics\n",
    "            best_fold = fold\n",
    "            best_y_true = y_test\n",
    "            best_y_pred = y_pred\n",
    "\n",
    "        # PR曲线\n",
    "        interp_precision, _, _ = plot_pr_curve(\n",
    "            y_test, y_scores, fold,\n",
    "            save_path=f'pr_curve_fold{fold}.png'\n",
    "        )\n",
    "        all_interp_precisions.append(interp_precision)\n",
    "\n",
    "        # 打印结果\n",
    "        print(f\"\\nFold {fold} Metrics:\")\n",
    "        print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
    "        print(f\"F1 Score: {metrics['f1']:.4f}\")\n",
    "        print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "        print(f\"Recall/Sensitivity: {metrics['recall']:.4f}\")\n",
    "        print(f\"Specificity: {metrics['specificity']:.4f}\")\n",
    "        print(f\"AUPRC: {metrics['auprc']:.4f}\")\n",
    "        print(f\"Confusion Matrix:\")\n",
    "        print(f\"TN: {metrics['confusion_matrix']['TN']} | FP: {metrics['confusion_matrix']['FP']}\")\n",
    "        print(f\"FN: {metrics['confusion_matrix']['FN']} | TP: {metrics['confusion_matrix']['TP']}\")\n",
    "\n",
    "    # 绘制最佳折的混淆矩阵\n",
    "    if best_y_true is not None and best_y_pred is not None:\n",
    "        print(f\"\\nPlotting confusion matrix for best fold: {best_fold}\")\n",
    "        plot_confusion_matrix(best_y_true, best_y_pred, best_fold, dpi=720)\n",
    "\n",
    "    # 计算平均指标\n",
    "    avg_metrics = {\n",
    "        'accuracy': np.mean([r['accuracy'] for r in results]),\n",
    "        'f1': np.mean([r['f1'] for r in results]),\n",
    "        'precision': np.mean([r['precision'] for r in results]),\n",
    "        'recall': np.mean([r['recall'] for r in results]),\n",
    "        'specificity': np.mean([r['specificity'] for r in results]),\n",
    "        'auprc': np.mean([r['auprc'] for r in results]),\n",
    "    }\n",
    "\n",
    "    std_metrics = {\n",
    "        'accuracy': np.std([r['accuracy'] for r in results]),\n",
    "        'f1': np.std([r['f1'] for r in results]),\n",
    "        'precision': np.std([r['precision'] for r in results]),\n",
    "        'recall': np.std([r['recall'] for r in results]),\n",
    "        'specificity': np.std([r['specificity'] for r in results]),\n",
    "        'auprc': np.std([r['auprc'] for r in results]),\n",
    "    }\n",
    "\n",
    "    # 绘制平均PR曲线\n",
    "    if all_interp_precisions:\n",
    "        mean_precision = np.mean(all_interp_precisions, axis=0)\n",
    "        mean_auprc = auc(interp_recall, mean_precision)\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        for i, prec in enumerate(all_interp_precisions, 1):\n",
    "            plt.plot(interp_recall, prec, alpha=0.2, label=f'Fold {i}')\n",
    "\n",
    "        plt.plot(interp_recall, mean_precision, 'r-',\n",
    "                 linewidth=3, label=f'Mean (AUPRC = {mean_auprc:.3f})')\n",
    "        plt.xlabel('召回率', fontsize=14, fontweight='bold')\n",
    "        plt.ylabel('精确率', fontsize=14, fontweight='bold')\n",
    "        plt.title('平均 Precision-Recall 曲线', fontsize=16, fontweight='bold')\n",
    "        plt.legend()\n",
    "        plt.savefig('average_pr_curve.png')\n",
    "        plt.close()\n",
    "\n",
    "    return avg_metrics, std_metrics\n",
    "\n",
    "\n",
    "# 主程序\n",
    "def main():\n",
    "    # 加载数据\n",
    "    data = pd.read_csv('preparations/stroke_output.csv')\n",
    "\n",
    "    # 预处理\n",
    "    categorical_cols = ['ever_married', 'work_type', 'smoking_status']\n",
    "    data[categorical_cols] = data[categorical_cols].astype('category')\n",
    "    X = data.drop('stroke', axis=1)\n",
    "    y = data['stroke'].values\n",
    "\n",
    "    # 独热编码\n",
    "    X = pd.get_dummies(X, columns=categorical_cols)\n",
    "    X = X.values.astype(np.float32)\n",
    "\n",
    "    # 执行交叉验证\n",
    "    print(\"Starting cross-validation...\")\n",
    "    avg_metrics, std_metrics = cross_validate(X, y, n_splits=10)\n",
    "\n",
    "    # 打印最终结果\n",
    "    print('\\n' + '=' * 50)\n",
    "    print('Final Cross-Validation Results:')\n",
    "    print('=' * 50)\n",
    "    print(f\"Average Accuracy: {avg_metrics['accuracy']:.4f} ± {std_metrics['accuracy']:.4f}\")\n",
    "    print(f\"Average F1 Score: {avg_metrics['f1']:.4f} ± {std_metrics['f1']:.4f}\")\n",
    "    print(f\"Average Precision: {avg_metrics['precision']:.4f} ± {std_metrics['precision']:.4f}\")\n",
    "    print(f\"Average Recall/Sensitivity: {avg_metrics['recall']:.4f} ± {std_metrics['recall']:.4f}\")\n",
    "    print(f\"Average Specificity: {avg_metrics['specificity']:.4f} ± {std_metrics['specificity']:.4f}\")\n",
    "    print(f\"Average AUPRC: {avg_metrics['auprc']:.4f} ± {std_metrics['auprc']:.4f}\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "48fde9e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据前5行:\n",
      "    age  hypertension  heart_disease  ever_married  work_type  \\\n",
      "0  67.0             0              1             1        3.0   \n",
      "1  61.0             0              0             1        2.0   \n",
      "2  80.0             0              1             1        3.0   \n",
      "3  49.0             0              0             1        3.0   \n",
      "4  79.0             1              0             1        2.0   \n",
      "\n",
      "   avg_glucose_level        bmi  smoking_status  stroke  \n",
      "0             228.69  36.600000               1       1  \n",
      "1             202.21  28.893237               0       1  \n",
      "2             105.92  32.500000               0       1  \n",
      "3             171.23  34.400000               2       1  \n",
      "4             174.12  24.000000               0       1  \n",
      "\n",
      "类别分布:\n",
      "stroke\n",
      "0    4861\n",
      "1     249\n",
      "Name: count, dtype: int64\n",
      "数据形状: X=(5110, 8), y=(5110,)\n",
      "类别分布: [4861  249]\n",
      "\n",
      "==================================================\n",
      "Fold 1/10\n",
      "==================================================\n",
      "\n",
      "Fold 1 测试集指标:\n",
      "准确度: 0.9511\n",
      "F1分数: 0.0000\n",
      "精确率: 0.0000\n",
      "召回率: 0.0000\n",
      "特异性: 1.0000\n",
      "AUPRC: 0.2353\n",
      "AUROC: 0.8531\n",
      "\n",
      "分类报告:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.97       486\n",
      "           1       0.00      0.00      0.00        25\n",
      "\n",
      "    accuracy                           0.95       511\n",
      "   macro avg       0.48      0.50      0.49       511\n",
      "weighted avg       0.90      0.95      0.93       511\n",
      "\n",
      "\n",
      "==================================================\n",
      "Fold 2/10\n",
      "==================================================\n",
      "\n",
      "Fold 2 测试集指标:\n",
      "准确度: 0.9511\n",
      "F1分数: 0.0000\n",
      "精确率: 0.0000\n",
      "召回率: 0.0000\n",
      "特异性: 1.0000\n",
      "AUPRC: 0.2911\n",
      "AUROC: 0.8798\n",
      "\n",
      "分类报告:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.97       486\n",
      "           1       0.00      0.00      0.00        25\n",
      "\n",
      "    accuracy                           0.95       511\n",
      "   macro avg       0.48      0.50      0.49       511\n",
      "weighted avg       0.90      0.95      0.93       511\n",
      "\n",
      "\n",
      "==================================================\n",
      "Fold 3/10\n",
      "==================================================\n",
      "\n",
      "Fold 3 测试集指标:\n",
      "准确度: 0.9511\n",
      "F1分数: 0.0000\n",
      "精确率: 0.0000\n",
      "召回率: 0.0000\n",
      "特异性: 1.0000\n",
      "AUPRC: 0.3207\n",
      "AUROC: 0.8670\n",
      "\n",
      "分类报告:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.97       486\n",
      "           1       0.00      0.00      0.00        25\n",
      "\n",
      "    accuracy                           0.95       511\n",
      "   macro avg       0.48      0.50      0.49       511\n",
      "weighted avg       0.90      0.95      0.93       511\n",
      "\n",
      "\n",
      "==================================================\n",
      "Fold 4/10\n",
      "==================================================\n",
      "\n",
      "Fold 4 测试集指标:\n",
      "准确度: 0.9511\n",
      "F1分数: 0.0000\n",
      "精确率: 0.0000\n",
      "召回率: 0.0000\n",
      "特异性: 1.0000\n",
      "AUPRC: 0.1598\n",
      "AUROC: 0.8141\n",
      "\n",
      "分类报告:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.97       486\n",
      "           1       0.00      0.00      0.00        25\n",
      "\n",
      "    accuracy                           0.95       511\n",
      "   macro avg       0.48      0.50      0.49       511\n",
      "weighted avg       0.90      0.95      0.93       511\n",
      "\n",
      "\n",
      "==================================================\n",
      "Fold 5/10\n",
      "==================================================\n",
      "\n",
      "Fold 5 测试集指标:\n",
      "准确度: 0.9511\n",
      "F1分数: 0.0000\n",
      "精确率: 0.0000\n",
      "召回率: 0.0000\n",
      "特异性: 1.0000\n",
      "AUPRC: 0.1557\n",
      "AUROC: 0.8278\n",
      "\n",
      "分类报告:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.97       486\n",
      "           1       0.00      0.00      0.00        25\n",
      "\n",
      "    accuracy                           0.95       511\n",
      "   macro avg       0.48      0.50      0.49       511\n",
      "weighted avg       0.90      0.95      0.93       511\n",
      "\n",
      "\n",
      "==================================================\n",
      "Fold 6/10\n",
      "==================================================\n",
      "\n",
      "Fold 6 测试集指标:\n",
      "准确度: 0.9511\n",
      "F1分数: 0.0000\n",
      "精确率: 0.0000\n",
      "召回率: 0.0000\n",
      "特异性: 1.0000\n",
      "AUPRC: 0.1502\n",
      "AUROC: 0.7978\n",
      "\n",
      "分类报告:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.97       486\n",
      "           1       0.00      0.00      0.00        25\n",
      "\n",
      "    accuracy                           0.95       511\n",
      "   macro avg       0.48      0.50      0.49       511\n",
      "weighted avg       0.90      0.95      0.93       511\n",
      "\n",
      "\n",
      "==================================================\n",
      "Fold 7/10\n",
      "==================================================\n",
      "\n",
      "Fold 7 测试集指标:\n",
      "准确度: 0.9511\n",
      "F1分数: 0.0000\n",
      "精确率: 0.0000\n",
      "召回率: 0.0000\n",
      "特异性: 1.0000\n",
      "AUPRC: 0.2210\n",
      "AUROC: 0.8363\n",
      "\n",
      "分类报告:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.97       486\n",
      "           1       0.00      0.00      0.00        25\n",
      "\n",
      "    accuracy                           0.95       511\n",
      "   macro avg       0.48      0.50      0.49       511\n",
      "weighted avg       0.90      0.95      0.93       511\n",
      "\n",
      "\n",
      "==================================================\n",
      "Fold 8/10\n",
      "==================================================\n",
      "\n",
      "Fold 8 测试集指标:\n",
      "准确度: 0.9511\n",
      "F1分数: 0.0000\n",
      "精确率: 0.0000\n",
      "召回率: 0.0000\n",
      "特异性: 1.0000\n",
      "AUPRC: 0.2402\n",
      "AUROC: 0.8685\n",
      "\n",
      "分类报告:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.97       486\n",
      "           1       0.00      0.00      0.00        25\n",
      "\n",
      "    accuracy                           0.95       511\n",
      "   macro avg       0.48      0.50      0.49       511\n",
      "weighted avg       0.90      0.95      0.93       511\n",
      "\n",
      "\n",
      "==================================================\n",
      "Fold 9/10\n",
      "==================================================\n",
      "\n",
      "Fold 9 测试集指标:\n",
      "准确度: 0.9511\n",
      "F1分数: 0.0000\n",
      "精确率: 0.0000\n",
      "召回率: 0.0000\n",
      "特异性: 1.0000\n",
      "AUPRC: 0.2298\n",
      "AUROC: 0.8759\n",
      "\n",
      "分类报告:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.97       486\n",
      "           1       0.00      0.00      0.00        25\n",
      "\n",
      "    accuracy                           0.95       511\n",
      "   macro avg       0.48      0.50      0.49       511\n",
      "weighted avg       0.90      0.95      0.93       511\n",
      "\n",
      "\n",
      "==================================================\n",
      "Fold 10/10\n",
      "==================================================\n",
      "\n",
      "Fold 10 测试集指标:\n",
      "准确度: 0.9530\n",
      "F1分数: 0.0000\n",
      "精确率: 0.0000\n",
      "召回率: 0.0000\n",
      "特异性: 1.0000\n",
      "AUPRC: 0.2523\n",
      "AUROC: 0.7977\n",
      "\n",
      "分类报告:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.98       487\n",
      "           1       0.00      0.00      0.00        24\n",
      "\n",
      "    accuracy                           0.95       511\n",
      "   macro avg       0.48      0.50      0.49       511\n",
      "weighted avg       0.91      0.95      0.93       511\n",
      "\n",
      "\n",
      "绘制最佳折的混淆矩阵 (Fold 3)\n",
      "\n",
      "==================================================\n",
      "交叉验证最终结果:\n",
      "==================================================\n",
      "平均准确度: 0.9513 ± 0.0006\n",
      "平均F1分数: 0.0000 ± 0.0000\n",
      "平均精确率: 0.0000 ± 0.0000\n",
      "平均召回率: 0.0000 ± 0.0000\n",
      "平均特异性: 1.0000 ± 0.0000\n",
      "平均AUPRC: 0.2256 ± 0.0541\n",
      "平均AUROC: 0.8418 ± 0.0299\n",
      "\n",
      "整体分类报告:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.98      4861\n",
      "           1       0.00      0.00      0.00       249\n",
      "\n",
      "    accuracy                           0.95      5110\n",
      "   macro avg       0.48      0.50      0.49      5110\n",
      "weighted avg       0.90      0.95      0.93      5110\n",
      "\n",
      "\n",
      "==================================================\n",
      "各折模型权重:\n",
      "==================================================\n",
      "\n",
      "Fold 1 权重:\n",
      "截距项 (bias): -3.9060\n",
      "age: 1.5032\n",
      "hypertension: 0.1358\n",
      "heart_disease: 0.0727\n",
      "ever_married: -0.1207\n",
      "work_type: 0.1377\n",
      "avg_glucose_level: 0.1742\n",
      "bmi: 0.0328\n",
      "smoking_status: 0.1089\n",
      "\n",
      "Fold 2 权重:\n",
      "截距项 (bias): -3.9044\n",
      "age: 1.5014\n",
      "hypertension: 0.0896\n",
      "heart_disease: 0.0703\n",
      "ever_married: -0.0958\n",
      "work_type: 0.1381\n",
      "avg_glucose_level: 0.1792\n",
      "bmi: 0.0246\n",
      "smoking_status: 0.0517\n",
      "\n",
      "Fold 3 权重:\n",
      "截距项 (bias): -3.8936\n",
      "age: 1.5301\n",
      "hypertension: 0.0911\n",
      "heart_disease: 0.0552\n",
      "ever_married: -0.1221\n",
      "work_type: 0.1057\n",
      "avg_glucose_level: 0.1780\n",
      "bmi: -0.0054\n",
      "smoking_status: 0.0941\n",
      "\n",
      "Fold 4 权重:\n",
      "截距项 (bias): -3.9479\n",
      "age: 1.5155\n",
      "hypertension: 0.1351\n",
      "heart_disease: 0.0803\n",
      "ever_married: -0.0714\n",
      "work_type: 0.1091\n",
      "avg_glucose_level: 0.1973\n",
      "bmi: -0.0183\n",
      "smoking_status: 0.0614\n",
      "\n",
      "Fold 5 权重:\n",
      "截距项 (bias): -3.9080\n",
      "age: 1.5186\n",
      "hypertension: 0.1415\n",
      "heart_disease: 0.0559\n",
      "ever_married: -0.1403\n",
      "work_type: 0.0557\n",
      "avg_glucose_level: 0.2078\n",
      "bmi: 0.0498\n",
      "smoking_status: 0.1177\n",
      "\n",
      "Fold 6 权重:\n",
      "截距项 (bias): -3.9873\n",
      "age: 1.5702\n",
      "hypertension: 0.1112\n",
      "heart_disease: 0.0691\n",
      "ever_married: -0.0956\n",
      "work_type: 0.1440\n",
      "avg_glucose_level: 0.2025\n",
      "bmi: 0.0153\n",
      "smoking_status: 0.0904\n",
      "\n",
      "Fold 7 权重:\n",
      "截距项 (bias): -3.9240\n",
      "age: 1.5221\n",
      "hypertension: 0.1381\n",
      "heart_disease: 0.0623\n",
      "ever_married: -0.1036\n",
      "work_type: 0.1249\n",
      "avg_glucose_level: 0.1795\n",
      "bmi: -0.0063\n",
      "smoking_status: 0.0923\n",
      "\n",
      "Fold 8 权重:\n",
      "截距项 (bias): -3.8836\n",
      "age: 1.4905\n",
      "hypertension: 0.1212\n",
      "heart_disease: 0.0857\n",
      "ever_married: -0.0688\n",
      "work_type: 0.0719\n",
      "avg_glucose_level: 0.1660\n",
      "bmi: -0.0473\n",
      "smoking_status: 0.0807\n",
      "\n",
      "Fold 9 权重:\n",
      "截距项 (bias): -3.8938\n",
      "age: 1.4852\n",
      "hypertension: 0.1071\n",
      "heart_disease: 0.0755\n",
      "ever_married: -0.0734\n",
      "work_type: 0.1001\n",
      "avg_glucose_level: 0.1986\n",
      "bmi: -0.0096\n",
      "smoking_status: 0.0768\n",
      "\n",
      "Fold 10 权重:\n",
      "截距项 (bias): -3.9452\n",
      "age: 1.5367\n",
      "hypertension: 0.1153\n",
      "heart_disease: 0.0771\n",
      "ever_married: -0.1068\n",
      "work_type: 0.1463\n",
      "avg_glucose_level: 0.1965\n",
      "bmi: -0.0112\n",
      "smoking_status: 0.0848\n",
      "\n",
      "==================================================\n",
      "跨折平均权重:\n",
      "==================================================\n",
      "平均截距项: -3.9194\n",
      "age: 1.5174\n",
      "hypertension: 0.1186\n",
      "heart_disease: 0.0704\n",
      "ever_married: -0.0999\n",
      "work_type: 0.1134\n",
      "avg_glucose_level: 0.1880\n",
      "bmi: 0.0024\n",
      "smoking_status: 0.0859\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (accuracy_score, f1_score, precision_score,\n",
    "                             recall_score, average_precision_score,\n",
    "                             precision_recall_curve, auc, roc_auc_score,\n",
    "                             confusion_matrix, classification_report)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import interpolate\n",
    "\n",
    "# 设置中文字体\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']  # 设置中文字体为黑体\n",
    "plt.rcParams['axes.unicode_minus'] = False  # 解决负号显示问题\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, fold, dpi=720):\n",
    "    \"\"\"绘制正方形混淆矩阵\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    cm_percentage = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100  # 百分比表示\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    ax = sns.heatmap(cm_percentage, annot=False, fmt='.2f', cmap='Blues', square=True, cbar=False,\n",
    "                     linewidths=2, linecolor='black')\n",
    "\n",
    "    # 在每个格子中显示个数和百分比\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            # 判断字体颜色，深色背景用白色字体，浅色背景用黑色字体\n",
    "            text_color = 'white' if cm_percentage[i, j] > 50 else 'black'\n",
    "            ax.text(j + 0.5, i + 0.5, f'{cm[i, j]}\\n({cm_percentage[i, j]:.2f}%)',\n",
    "                    color=text_color, ha='center', va='center', fontsize=14, fontweight='bold')\n",
    "\n",
    "    # 添加中文标签\n",
    "    plt.xlabel('预测类别', fontsize=16, fontweight='bold')\n",
    "    plt.ylabel('实际类别', fontsize=16, fontweight='bold')\n",
    "    plt.xticks(ticks=np.arange(cm.shape[1]) + 0.5, labels=['0', '1'], fontsize=14, fontweight='bold')\n",
    "    plt.yticks(ticks=np.arange(cm.shape[0]) + 0.5, labels=['0', '1'], fontsize=14, fontweight='bold')\n",
    "\n",
    "    # 调整布局，减少空白边缘\n",
    "    plt.subplots_adjust(left=0.1, right=0.9, top=0.9, bottom=0.1)\n",
    "\n",
    "    # 保存混淆矩阵图\n",
    "    plt.savefig(f'LR_best_fold_confusion_matrix_fold{fold}.png', dpi=dpi)\n",
    "    plt.close()\n",
    "\n",
    "def calculate_metrics(y_true, y_pred, y_scores):\n",
    "    if np.isnan(y_scores).any():\n",
    "        y_scores = np.nan_to_num(y_scores)\n",
    "\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_true, y_pred),\n",
    "        'f1': f1_score(y_true, y_pred, zero_division=0),\n",
    "        'precision': precision_score(y_true, y_pred, zero_division=0),\n",
    "        'recall': recall_score(y_true, y_pred, zero_division=0),\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        metrics['auprc'] = average_precision_score(y_true, y_scores)\n",
    "        metrics['auroc'] = roc_auc_score(y_true, y_scores)\n",
    "    except:\n",
    "        print(\"无法计算AUPRC/AUROC，使用默认值0\")\n",
    "        metrics['auprc'] = 0\n",
    "        metrics['auroc'] = 0\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    metrics['specificity'] = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    metrics['sensitivity'] = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "\n",
    "    return metrics\n",
    "\n",
    "def interpolate_pr_curve(precision, recall):\n",
    "    f = interpolate.interp1d(recall, precision, bounds_error=False, fill_value=(1.0, 0.0))\n",
    "    new_recall = np.linspace(0, 1, 100)\n",
    "    new_precision = f(new_recall)\n",
    "    return new_precision, new_recall\n",
    "\n",
    "def plot_pr_curve(y_true, y_scores, fold):\n",
    "    try:\n",
    "        precision, recall, _ = precision_recall_curve(y_true, y_scores)\n",
    "        auprc = auc(recall, precision)\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(recall, precision, label=f'Fold {fold} (AUPRC = {auprc:.2f})')\n",
    "        plt.xlabel('召回率', fontsize=14, fontweight='bold')\n",
    "        plt.ylabel('精确率', fontsize=14, fontweight='bold')\n",
    "        plt.title('精确率-召回率曲线', fontsize=16, fontweight='bold')\n",
    "        plt.legend()\n",
    "        plt.savefig(f'pr_curve_fold{fold}.png')\n",
    "        plt.close()\n",
    "        return precision, recall\n",
    "    except Exception as e:\n",
    "        print(f\"无法绘制Fold {fold}的PR曲线: {str(e)}\")\n",
    "        return None, None\n",
    "\n",
    "def train_test_split(X, y, splits=10):\n",
    "    print(f\"数据形状: X={X.shape}, y={y.shape}\")\n",
    "    print(f\"类别分布: {np.bincount(y)}\")\n",
    "\n",
    "    try:\n",
    "        feature_names = data.drop('stroke', axis=1).columns.tolist()\n",
    "    except:\n",
    "        feature_names = [f'Feature_{i}' for i in range(X.shape[1])]\n",
    "\n",
    "    X = np.nan_to_num(X)\n",
    "    y = np.nan_to_num(y).astype(int)\n",
    "\n",
    "    k_fold = StratifiedKFold(n_splits=splits, shuffle=True, random_state=2025)\n",
    "    results = []\n",
    "    weights = []\n",
    "    all_y_true = []\n",
    "    all_y_pred = []\n",
    "    all_y_scores = []\n",
    "\n",
    "    interp_precisions = []\n",
    "    interp_recalls = np.linspace(0, 1, 100)\n",
    "    \n",
    "    best_fold_metrics = None\n",
    "    best_fold = -1\n",
    "    best_y_true = None\n",
    "    best_y_pred = None\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(k_fold.split(X, y)):\n",
    "        print(f'\\n{\"=\" * 50}')\n",
    "        print(f'Fold {fold + 1}/{splits}')\n",
    "        print(f'{\"=\" * 50}')\n",
    "\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "        model = LogisticRegression(penalty='l2', C=1.0, solver='liblinear', random_state=42)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        fold_weights = {\n",
    "            'intercept': model.intercept_[0],\n",
    "            'coefficients': model.coef_[0]\n",
    "        }\n",
    "        weights.append(fold_weights)\n",
    "\n",
    "        y_scores = model.predict_proba(X_test)[:, 1]\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        all_y_true.extend(y_test)\n",
    "        all_y_pred.extend(y_pred)\n",
    "        all_y_scores.extend(y_scores)\n",
    "\n",
    "        metrics = calculate_metrics(y_test, y_pred, y_scores)\n",
    "        \n",
    "        # 跟踪最佳折叠（根据AUPRC）\n",
    "        if best_fold_metrics is None or metrics['auprc'] > best_fold_metrics['auprc']:\n",
    "            best_fold_metrics = metrics\n",
    "            best_fold = fold + 1\n",
    "            best_y_true = y_test\n",
    "            best_y_pred = y_pred\n",
    "\n",
    "        precision, recall = plot_pr_curve(y_test, y_scores, fold + 1)\n",
    "        if precision is not None and recall is not None:\n",
    "            interp_precision, _ = interpolate_pr_curve(precision, recall)\n",
    "            interp_precisions.append(interp_precision)\n",
    "\n",
    "        results.append(metrics)\n",
    "\n",
    "        print(f'\\nFold {fold + 1} 测试集指标:')\n",
    "        print(f\"准确度: {metrics['accuracy']:.4f}\")\n",
    "        print(f\"F1分数: {metrics['f1']:.4f}\")\n",
    "        print(f\"精确率: {metrics['precision']:.4f}\")\n",
    "        print(f\"召回率: {metrics['recall']:.4f}\")\n",
    "        print(f\"特异性: {metrics['specificity']:.4f}\")\n",
    "        print(f\"AUPRC: {metrics['auprc']:.4f}\")\n",
    "        print(f\"AUROC: {metrics['auroc']:.4f}\")\n",
    "        print(\"\\n分类报告:\")\n",
    "        print(classification_report(y_test, y_pred, zero_division=0))\n",
    "\n",
    "    # 绘制最佳折的混淆矩阵\n",
    "    if best_y_true is not None and best_y_pred is not None:\n",
    "        print(f\"\\n绘制最佳折的混淆矩阵 (Fold {best_fold})\")\n",
    "        plot_confusion_matrix(best_y_true, best_y_pred, best_fold, dpi=720)\n",
    "\n",
    "    avg_metrics = {\n",
    "        'accuracy': np.mean([r['accuracy'] for r in results]),\n",
    "        'f1': np.mean([r['f1'] for r in results]),\n",
    "        'precision': np.mean([r['precision'] for r in results]),\n",
    "        'recall': np.mean([r['recall'] for r in results]),\n",
    "        'specificity': np.mean([r['specificity'] for r in results]),\n",
    "        'auprc': np.mean([r['auprc'] for r in results]),\n",
    "        'auroc': np.mean([r['auroc'] for r in results])\n",
    "    }\n",
    "\n",
    "    std_metrics = {\n",
    "        'accuracy': np.std([r['accuracy'] for r in results]),\n",
    "        'f1': np.std([r['f1'] for r in results]),\n",
    "        'precision': np.std([r['precision'] for r in results]),\n",
    "        'recall': np.std([r['recall'] for r in results]),\n",
    "        'specificity': np.std([r['specificity'] for r in results]),\n",
    "        'auprc': np.std([r['auprc'] for r in results]),\n",
    "        'auroc': np.std([r['auroc'] for r in results])\n",
    "    }\n",
    "\n",
    "    print('\\n' + '=' * 50)\n",
    "    print('交叉验证最终结果:')\n",
    "    print('=' * 50)\n",
    "    print(f\"平均准确度: {avg_metrics['accuracy']:.4f} ± {std_metrics['accuracy']:.4f}\")\n",
    "    print(f\"平均F1分数: {avg_metrics['f1']:.4f} ± {std_metrics['f1']:.4f}\")\n",
    "    print(f\"平均精确率: {avg_metrics['precision']:.4f} ± {std_metrics['precision']:.4f}\")\n",
    "    print(f\"平均召回率: {avg_metrics['recall']:.4f} ± {std_metrics['recall']:.4f}\")\n",
    "    print(f\"平均特异性: {avg_metrics['specificity']:.4f} ± {std_metrics['specificity']:.4f}\")\n",
    "    print(f\"平均AUPRC: {avg_metrics['auprc']:.4f} ± {std_metrics['auprc']:.4f}\")\n",
    "    print(f\"平均AUROC: {avg_metrics['auroc']:.4f} ± {std_metrics['auroc']:.4f}\")\n",
    "\n",
    "    print('\\n整体分类报告:')\n",
    "    print(classification_report(all_y_true, all_y_pred, zero_division=0))\n",
    "\n",
    "    print('\\n' + '=' * 50)\n",
    "    print('各折模型权重:')\n",
    "    print('=' * 50)\n",
    "    for i, fold_weight in enumerate(weights):\n",
    "        print(f'\\nFold {i + 1} 权重:')\n",
    "        print(f\"截距项 (bias): {fold_weight['intercept']:.4f}\")\n",
    "        for name, coef in zip(feature_names, fold_weight['coefficients']):\n",
    "            print(f\"{name}: {coef:.4f}\")\n",
    "\n",
    "    avg_intercept = np.mean([w['intercept'] for w in weights])\n",
    "    avg_coefficients = np.mean([w['coefficients'] for w in weights], axis=0)\n",
    "\n",
    "    print('\\n' + '=' * 50)\n",
    "    print('跨折平均权重:')\n",
    "    print('=' * 50)\n",
    "    print(f\"平均截距项: {avg_intercept:.4f}\")\n",
    "    for name, coef in zip(feature_names, avg_coefficients):\n",
    "        print(f\"{name}: {coef:.4f}\")\n",
    "\n",
    "    if interp_precisions:\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        mean_precision = np.mean(interp_precisions, axis=0)\n",
    "        mean_auprc = auc(interp_recalls, mean_precision)\n",
    "\n",
    "        for i, prec in enumerate(interp_precisions):\n",
    "            plt.plot(interp_recalls, prec, alpha=0.3, label=f'Fold {i + 1}')\n",
    "\n",
    "        plt.plot(interp_recalls, mean_precision, 'k-',\n",
    "                 label=f'平均 (AUPRC = {mean_auprc:.2f})', linewidth=2)\n",
    "        plt.xlabel('召回率', fontsize=14, fontweight='bold')\n",
    "        plt.ylabel('精确率', fontsize=14, fontweight='bold')\n",
    "        plt.title('平均精确率-召回率曲线', fontsize=16, fontweight='bold')\n",
    "        plt.legend()\n",
    "        plt.savefig('average_pr_curve.png')\n",
    "        plt.close()\n",
    "\n",
    "# 加载数据\n",
    "data = pd.read_csv('preparations/stroke_output.csv')  # 请替换为您的实际文件路径\n",
    "\n",
    "# 检查数据\n",
    "print(\"数据前5行:\")\n",
    "print(data.head())\n",
    "print(\"\\n类别分布:\")\n",
    "print(data['stroke'].value_counts())\n",
    "\n",
    "# 分离特征和目标\n",
    "X = data.drop('stroke', axis=1).values\n",
    "y = data['stroke'].values\n",
    "\n",
    "# 运行训练和评估\n",
    "train_test_split(X, y, splits=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ef4c1b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting cross-validation...\n",
      "\n",
      "======================================== Fold 1/10 ========================================\n",
      "\n",
      "Fold 1 Metrics:\n",
      "Accuracy: 0.7123\n",
      "F1 Score: 0.2054\n",
      "Precision: 0.1187\n",
      "Recall/Sensitivity: 0.7600\n",
      "Specificity: 0.7099\n",
      "AUPRC: 0.2017\n",
      "Confusion Matrix:\n",
      "TN: 345 | FP: 141\n",
      "FN: 6 | TP: 19\n",
      "\n",
      "======================================== Fold 2/10 ========================================\n",
      "\n",
      "Fold 2 Metrics:\n",
      "Accuracy: 0.7025\n",
      "F1 Score: 0.1828\n",
      "Precision: 0.1056\n",
      "Recall/Sensitivity: 0.6800\n",
      "Specificity: 0.7037\n",
      "AUPRC: 0.2360\n",
      "Confusion Matrix:\n",
      "TN: 342 | FP: 144\n",
      "FN: 8 | TP: 17\n",
      "\n",
      "======================================== Fold 3/10 ========================================\n",
      "\n",
      "Fold 3 Metrics:\n",
      "Accuracy: 0.6908\n",
      "F1 Score: 0.2255\n",
      "Precision: 0.1285\n",
      "Recall/Sensitivity: 0.9200\n",
      "Specificity: 0.6790\n",
      "AUPRC: 0.2447\n",
      "Confusion Matrix:\n",
      "TN: 330 | FP: 156\n",
      "FN: 2 | TP: 23\n",
      "\n",
      "======================================== Fold 4/10 ========================================\n",
      "\n",
      "Fold 4 Metrics:\n",
      "Accuracy: 0.7182\n",
      "F1 Score: 0.2088\n",
      "Precision: 0.1210\n",
      "Recall/Sensitivity: 0.7600\n",
      "Specificity: 0.7160\n",
      "AUPRC: 0.1400\n",
      "Confusion Matrix:\n",
      "TN: 348 | FP: 138\n",
      "FN: 6 | TP: 19\n",
      "\n",
      "======================================== Fold 5/10 ========================================\n",
      "\n",
      "Fold 5 Metrics:\n",
      "Accuracy: 0.7123\n",
      "F1 Score: 0.2304\n",
      "Precision: 0.1325\n",
      "Recall/Sensitivity: 0.8800\n",
      "Specificity: 0.7037\n",
      "AUPRC: 0.1738\n",
      "Confusion Matrix:\n",
      "TN: 342 | FP: 144\n",
      "FN: 3 | TP: 22\n",
      "\n",
      "======================================== Fold 6/10 ========================================\n",
      "\n",
      "Fold 6 Metrics:\n",
      "Accuracy: 0.6986\n",
      "F1 Score: 0.1979\n",
      "Precision: 0.1138\n",
      "Recall/Sensitivity: 0.7600\n",
      "Specificity: 0.6955\n",
      "AUPRC: 0.1549\n",
      "Confusion Matrix:\n",
      "TN: 338 | FP: 148\n",
      "FN: 6 | TP: 19\n",
      "\n",
      "======================================== Fold 7/10 ========================================\n",
      "\n",
      "Fold 7 Metrics:\n",
      "Accuracy: 0.7436\n",
      "F1 Score: 0.2249\n",
      "Precision: 0.1319\n",
      "Recall/Sensitivity: 0.7600\n",
      "Specificity: 0.7428\n",
      "AUPRC: 0.1984\n",
      "Confusion Matrix:\n",
      "TN: 361 | FP: 125\n",
      "FN: 6 | TP: 19\n",
      "\n",
      "======================================== Fold 8/10 ========================================\n",
      "\n",
      "Fold 8 Metrics:\n",
      "Accuracy: 0.7006\n",
      "F1 Score: 0.2073\n",
      "Precision: 0.1190\n",
      "Recall/Sensitivity: 0.8000\n",
      "Specificity: 0.6955\n",
      "AUPRC: 0.2346\n",
      "Confusion Matrix:\n",
      "TN: 338 | FP: 148\n",
      "FN: 5 | TP: 20\n",
      "\n",
      "======================================== Fold 9/10 ========================================\n",
      "\n",
      "Fold 9 Metrics:\n",
      "Accuracy: 0.7339\n",
      "F1 Score: 0.2000\n",
      "Precision: 0.1172\n",
      "Recall/Sensitivity: 0.6800\n",
      "Specificity: 0.7366\n",
      "AUPRC: 0.1762\n",
      "Confusion Matrix:\n",
      "TN: 358 | FP: 128\n",
      "FN: 8 | TP: 17\n",
      "\n",
      "======================================== Fold 10/10 ========================================\n",
      "\n",
      "Fold 10 Metrics:\n",
      "Accuracy: 0.6712\n",
      "F1 Score: 0.1765\n",
      "Precision: 0.1000\n",
      "Recall/Sensitivity: 0.7500\n",
      "Specificity: 0.6674\n",
      "AUPRC: 0.1360\n",
      "Confusion Matrix:\n",
      "TN: 325 | FP: 162\n",
      "FN: 6 | TP: 18\n",
      "\n",
      "Plotting confusion matrix for best fold: 3\n",
      "\n",
      "==================================================\n",
      "Final Cross-Validation Results:\n",
      "==================================================\n",
      "Average Accuracy: 0.7084 ± 0.0197\n",
      "Average F1 Score: 0.2059 ± 0.0169\n",
      "Average Precision: 0.1188 ± 0.0101\n",
      "Average Recall/Sensitivity: 0.7750 ± 0.0723\n",
      "Average Specificity: 0.7050 ± 0.0220\n",
      "Average AUPRC: 0.1896 ± 0.0379\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import (accuracy_score, f1_score, precision_score,\n",
    "                             recall_score, average_precision_score,\n",
    "                             precision_recall_curve, auc, confusion_matrix)\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import interpolate\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# 设置中文字体\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']  # 设置中文字体为黑体\n",
    "plt.rcParams['axes.unicode_minus'] = False  # 解决负号显示问题\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, fold, dpi=720):\n",
    "    \"\"\"绘制正方形混淆矩阵\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    cm_percentage = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100  # 百分比表示\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    ax = sns.heatmap(cm_percentage, annot=False, fmt='.2f', cmap='Blues', square=True, cbar=False,\n",
    "                     linewidths=2, linecolor='black')\n",
    "\n",
    "    # 在每个格子中显示个数和百分比\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            # 判断字体颜色，深色背景用白色字体，浅色背景用黑色字体\n",
    "            text_color = 'white' if cm_percentage[i, j] > 50 else 'black'\n",
    "            ax.text(j + 0.5, i + 0.5, f'{cm[i, j]}\\n({cm_percentage[i, j]:.2f}%)',\n",
    "                    color=text_color, ha='center', va='center', fontsize=14, fontweight='bold')\n",
    "\n",
    "    # 添加中文标签\n",
    "    plt.xlabel('预测类别', fontsize=16, fontweight='bold')\n",
    "    plt.ylabel('实际类别', fontsize=16, fontweight='bold')\n",
    "    plt.xticks(ticks=np.arange(cm.shape[1]) + 0.5, labels=['0', '1'], fontsize=14, fontweight='bold')\n",
    "    plt.yticks(ticks=np.arange(cm.shape[0]) + 0.5, labels=['0', '1'], fontsize=14, fontweight='bold')\n",
    "\n",
    "    # 调整布局，减少空白边缘\n",
    "    plt.subplots_adjust(left=0.1, right=0.9, top=0.9, bottom=0.1)\n",
    "\n",
    "    # 保存混淆矩阵图\n",
    "    plt.savefig(f'RF_best_fold_confusion_matrix_fold{fold}.png', dpi=dpi)\n",
    "    plt.close()\n",
    "\n",
    "def interpolate_pr_curve(precision, recall, num_points=100):\n",
    "    \"\"\"将PR曲线插值到固定长度的点\"\"\"\n",
    "    if len(precision) < 2 or len(recall) < 2:\n",
    "        return np.linspace(0, 1, num_points), np.linspace(1, 0, num_points)\n",
    "\n",
    "    # 确保recall是单调递增的\n",
    "    sorted_indices = np.argsort(recall)\n",
    "    recall = np.array(recall)[sorted_indices]\n",
    "    precision = np.array(precision)[sorted_indices]\n",
    "\n",
    "    # 插值\n",
    "    f = interpolate.interp1d(recall, precision, bounds_error=False, fill_value=(precision[0], precision[-1]))\n",
    "    new_recall = np.linspace(0, 1, num_points)\n",
    "    new_precision = f(new_recall)\n",
    "    return new_precision, new_recall\n",
    "\n",
    "def calculate_metrics(y_true, y_pred, y_scores):\n",
    "    \"\"\"计算评估指标\"\"\"\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_true, y_pred),\n",
    "        'f1': f1_score(y_true, y_pred, zero_division=0),\n",
    "        'precision': precision_score(y_true, y_pred, zero_division=0),\n",
    "        'recall': recall_score(y_true, y_pred, zero_division=0),\n",
    "        'auprc': average_precision_score(y_true, y_scores)\n",
    "    }\n",
    "\n",
    "    # 添加混淆矩阵信息\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    metrics.update({\n",
    "        'confusion_matrix': {\n",
    "            'TN': tn, 'FP': fp, 'FN': fn, 'TP': tp\n",
    "        },\n",
    "        'specificity': tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    })\n",
    "    return metrics\n",
    "\n",
    "def plot_pr_curve(y_true, y_scores, fold, save_path=None):\n",
    "    \"\"\"绘制PR曲线并返回插值后的数据\"\"\"\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_scores)\n",
    "    auprc = auc(recall, precision)\n",
    "\n",
    "    # 插值到固定长度\n",
    "    interp_precision, interp_recall = interpolate_pr_curve(precision, recall)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(recall, precision, label=f'Fold {fold} (AUPRC = {auprc:.3f})')\n",
    "    plt.xlabel('召回率', fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('精确率', fontsize=14, fontweight='bold')\n",
    "    plt.title(f'PR Curve (Fold {fold})', fontsize=16, fontweight='bold')\n",
    "    plt.legend()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "    return interp_precision, interp_recall, auprc\n",
    "\n",
    "def train_rf_model(X_train, y_train, X_val, y_val):\n",
    "    \"\"\"训练Random Forest模型\"\"\"\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=100, \n",
    "        random_state=42, \n",
    "        class_weight='balanced', \n",
    "        n_jobs=-1,\n",
    "        max_depth=5,\n",
    "        min_samples_split=5,\n",
    "        min_samples_leaf=2\n",
    "    )\n",
    "    \n",
    "    # 训练模型\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "def cross_validate(X, y, n_splits=10):\n",
    "    \"\"\"执行交叉验证\"\"\"\n",
    "    kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    results = []\n",
    "    all_interp_precisions = []\n",
    "    interp_recall = np.linspace(0, 1, 100)  # 固定100个recall点\n",
    "    \n",
    "    best_fold_metrics = None\n",
    "    best_fold = -1\n",
    "    best_y_true = None\n",
    "    best_y_pred = None\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(kf.split(X, y), 1):\n",
    "        print(f'\\n{\"=\" * 40} Fold {fold}/{n_splits} {\"=\" * 40}')\n",
    "\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "        # 标准化\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "        # SMOTE过采样\n",
    "        smote = SMOTE(random_state=42, k_neighbors=min(5, sum(y_train == 1) - 1))\n",
    "        X_res, y_res = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "        # 训练模型\n",
    "        model = train_rf_model(X_res, y_res, X_test_scaled, y_test)\n",
    "\n",
    "        # 预测\n",
    "        y_scores = model.predict_proba(X_test_scaled)[:, 1]  # 获取正类的预测概率\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "        # 计算指标\n",
    "        metrics = calculate_metrics(y_test, y_pred, y_scores)\n",
    "        results.append(metrics)\n",
    "        \n",
    "        # 跟踪最佳折叠（根据AUPRC）\n",
    "        if best_fold_metrics is None or metrics['auprc'] > best_fold_metrics['auprc']:\n",
    "            best_fold_metrics = metrics\n",
    "            best_fold = fold\n",
    "            best_y_true = y_test\n",
    "            best_y_pred = y_pred\n",
    "\n",
    "        # PR曲线\n",
    "        interp_precision, _, _ = plot_pr_curve(\n",
    "            y_test, y_scores, fold,\n",
    "            save_path=f'pr_curve_fold{fold}.png'\n",
    "        )\n",
    "        all_interp_precisions.append(interp_precision)\n",
    "\n",
    "        # 打印结果\n",
    "        print(f\"\\nFold {fold} Metrics:\")\n",
    "        print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
    "        print(f\"F1 Score: {metrics['f1']:.4f}\")\n",
    "        print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "        print(f\"Recall/Sensitivity: {metrics['recall']:.4f}\")\n",
    "        print(f\"Specificity: {metrics['specificity']:.4f}\")\n",
    "        print(f\"AUPRC: {metrics['auprc']:.4f}\")\n",
    "        print(f\"Confusion Matrix:\")\n",
    "        print(f\"TN: {metrics['confusion_matrix']['TN']} | FP: {metrics['confusion_matrix']['FP']}\")\n",
    "        print(f\"FN: {metrics['confusion_matrix']['FN']} | TP: {metrics['confusion_matrix']['TP']}\")\n",
    "\n",
    "    # 绘制最佳折的混淆矩阵\n",
    "    if best_y_true is not None and best_y_pred is not None:\n",
    "        print(f\"\\nPlotting confusion matrix for best fold: {best_fold}\")\n",
    "        plot_confusion_matrix(best_y_true, best_y_pred, best_fold, dpi=720)\n",
    "\n",
    "    # 计算平均指标\n",
    "    avg_metrics = {\n",
    "        'accuracy': np.mean([r['accuracy'] for r in results]),\n",
    "        'f1': np.mean([r['f1'] for r in results]),\n",
    "        'precision': np.mean([r['precision'] for r in results]),\n",
    "        'recall': np.mean([r['recall'] for r in results]),\n",
    "        'specificity': np.mean([r['specificity'] for r in results]),\n",
    "        'auprc': np.mean([r['auprc'] for r in results]),\n",
    "    }\n",
    "\n",
    "    std_metrics = {\n",
    "        'accuracy': np.std([r['accuracy'] for r in results]),\n",
    "        'f1': np.std([r['f1'] for r in results]),\n",
    "        'precision': np.std([r['precision'] for r in results]),\n",
    "        'recall': np.std([r['recall'] for r in results]),\n",
    "        'specificity': np.std([r['specificity'] for r in results]),\n",
    "        'auprc': np.std([r['auprc'] for r in results]),\n",
    "    }\n",
    "\n",
    "    # 绘制平均PR曲线\n",
    "    if all_interp_precisions:\n",
    "        mean_precision = np.mean(all_interp_precisions, axis=0)\n",
    "        mean_auprc = auc(interp_recall, mean_precision)\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        for i, prec in enumerate(all_interp_precisions, 1):\n",
    "            plt.plot(interp_recall, prec, alpha=0.2, label=f'Fold {i}')\n",
    "\n",
    "        plt.plot(interp_recall, mean_precision, 'r-',\n",
    "                 linewidth=3, label=f'Mean (AUPRC = {mean_auprc:.3f})')\n",
    "        plt.xlabel('召回率', fontsize=14, fontweight='bold')\n",
    "        plt.ylabel('精确率', fontsize=14, fontweight='bold')\n",
    "        plt.title('平均 Precision-Recall 曲线', fontsize=16, fontweight='bold')\n",
    "        plt.legend()\n",
    "        plt.savefig('average_pr_curve.png')\n",
    "        plt.close()\n",
    "\n",
    "    return avg_metrics, std_metrics\n",
    "\n",
    "def main():\n",
    "    # 加载数据\n",
    "    data = pd.read_csv('preparations/stroke_output.csv')\n",
    "\n",
    "    # 预处理\n",
    "    categorical_cols = ['ever_married', 'work_type', 'smoking_status']\n",
    "    data[categorical_cols] = data[categorical_cols].astype('category')\n",
    "    X = data.drop('stroke', axis=1)\n",
    "    y = data['stroke'].values\n",
    "\n",
    "    # 独热编码\n",
    "    X = pd.get_dummies(X, columns=categorical_cols)\n",
    "    X = X.values.astype(np.float32)\n",
    "\n",
    "    # 执行交叉验证\n",
    "    print(\"Starting cross-validation...\")\n",
    "    avg_metrics, std_metrics = cross_validate(X, y, n_splits=10)\n",
    "\n",
    "    # 打印最终结果\n",
    "    print('\\n' + '=' * 50)\n",
    "    print('Final Cross-Validation Results:')\n",
    "    print('=' * 50)\n",
    "    print(f\"Average Accuracy: {avg_metrics['accuracy']:.4f} ± {std_metrics['accuracy']:.4f}\")\n",
    "    print(f\"Average F1 Score: {avg_metrics['f1']:.4f} ± {std_metrics['f1']:.4f}\")\n",
    "    print(f\"Average Precision: {avg_metrics['precision']:.4f} ± {std_metrics['precision']:.4f}\")\n",
    "    print(f\"Average Recall/Sensitivity: {avg_metrics['recall']:.4f} ± {std_metrics['recall']:.4f}\")\n",
    "    print(f\"Average Specificity: {avg_metrics['specificity']:.4f} ± {std_metrics['specificity']:.4f}\")\n",
    "    print(f\"Average AUPRC: {avg_metrics['auprc']:.4f} ± {std_metrics['auprc']:.4f}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "59b0fe38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting cross-validation...\n",
      "\n",
      "======================================== Fold 1/10 ========================================\n",
      "Best Parameters: {'C': 10, 'gamma': 'auto', 'kernel': 'rbf'}\n",
      "\n",
      "Fold 1 Metrics:\n",
      "Accuracy: 0.8141\n",
      "F1 Score: 0.2149\n",
      "Precision: 0.1354\n",
      "Recall/Sensitivity: 0.5200\n",
      "Specificity: 0.8292\n",
      "AUPRC: 0.2136\n",
      "Confusion Matrix: {'TN': 403, 'FP': 83, 'FN': 12, 'TP': 13}\n",
      "\n",
      "======================================== Fold 2/10 ========================================\n",
      "Best Parameters: {'C': 10, 'gamma': 'auto', 'kernel': 'rbf'}\n",
      "\n",
      "Fold 2 Metrics:\n",
      "Accuracy: 0.8121\n",
      "F1 Score: 0.1864\n",
      "Precision: 0.1183\n",
      "Recall/Sensitivity: 0.4400\n",
      "Specificity: 0.8313\n",
      "AUPRC: 0.1647\n",
      "Confusion Matrix: {'TN': 404, 'FP': 82, 'FN': 14, 'TP': 11}\n",
      "\n",
      "======================================== Fold 3/10 ========================================\n",
      "Best Parameters: {'C': 10, 'gamma': 'auto', 'kernel': 'rbf'}\n",
      "\n",
      "Fold 3 Metrics:\n",
      "Accuracy: 0.8278\n",
      "F1 Score: 0.2542\n",
      "Precision: 0.1613\n",
      "Recall/Sensitivity: 0.6000\n",
      "Specificity: 0.8395\n",
      "AUPRC: 0.1843\n",
      "Confusion Matrix: {'TN': 408, 'FP': 78, 'FN': 10, 'TP': 15}\n",
      "\n",
      "======================================== Fold 4/10 ========================================\n",
      "Best Parameters: {'C': 10, 'gamma': 'auto', 'kernel': 'rbf'}\n",
      "\n",
      "Fold 4 Metrics:\n",
      "Accuracy: 0.8239\n",
      "F1 Score: 0.1176\n",
      "Precision: 0.0779\n",
      "Recall/Sensitivity: 0.2400\n",
      "Specificity: 0.8539\n",
      "AUPRC: 0.1411\n",
      "Confusion Matrix: {'TN': 415, 'FP': 71, 'FN': 19, 'TP': 6}\n",
      "\n",
      "======================================== Fold 5/10 ========================================\n",
      "Best Parameters: {'C': 10, 'gamma': 'auto', 'kernel': 'rbf'}\n",
      "\n",
      "Fold 5 Metrics:\n",
      "Accuracy: 0.8082\n",
      "F1 Score: 0.1695\n",
      "Precision: 0.1075\n",
      "Recall/Sensitivity: 0.4000\n",
      "Specificity: 0.8292\n",
      "AUPRC: 0.1685\n",
      "Confusion Matrix: {'TN': 403, 'FP': 83, 'FN': 15, 'TP': 10}\n",
      "\n",
      "======================================== Fold 6/10 ========================================\n",
      "Best Parameters: {'C': 10, 'gamma': 'auto', 'kernel': 'rbf'}\n",
      "\n",
      "Fold 6 Metrics:\n",
      "Accuracy: 0.7945\n",
      "F1 Score: 0.1732\n",
      "Precision: 0.1078\n",
      "Recall/Sensitivity: 0.4400\n",
      "Specificity: 0.8128\n",
      "AUPRC: 0.1187\n",
      "Confusion Matrix: {'TN': 395, 'FP': 91, 'FN': 14, 'TP': 11}\n",
      "\n",
      "======================================== Fold 7/10 ========================================\n",
      "Best Parameters: {'C': 10, 'gamma': 'auto', 'kernel': 'rbf'}\n",
      "\n",
      "Fold 7 Metrics:\n",
      "Accuracy: 0.8160\n",
      "F1 Score: 0.1455\n",
      "Precision: 0.0941\n",
      "Recall/Sensitivity: 0.3200\n",
      "Specificity: 0.8416\n",
      "AUPRC: 0.1548\n",
      "Confusion Matrix: {'TN': 409, 'FP': 77, 'FN': 17, 'TP': 8}\n",
      "\n",
      "======================================== Fold 8/10 ========================================\n",
      "Best Parameters: {'C': 10, 'gamma': 'auto', 'kernel': 'rbf'}\n",
      "\n",
      "Fold 8 Metrics:\n",
      "Accuracy: 0.8278\n",
      "F1 Score: 0.2542\n",
      "Precision: 0.1613\n",
      "Recall/Sensitivity: 0.6000\n",
      "Specificity: 0.8395\n",
      "AUPRC: 0.1378\n",
      "Confusion Matrix: {'TN': 408, 'FP': 78, 'FN': 10, 'TP': 15}\n",
      "\n",
      "======================================== Fold 9/10 ========================================\n",
      "Best Parameters: {'C': 10, 'gamma': 'auto', 'kernel': 'rbf'}\n",
      "\n",
      "Fold 9 Metrics:\n",
      "Accuracy: 0.8297\n",
      "F1 Score: 0.2301\n",
      "Precision: 0.1477\n",
      "Recall/Sensitivity: 0.5200\n",
      "Specificity: 0.8457\n",
      "AUPRC: 0.1575\n",
      "Confusion Matrix: {'TN': 411, 'FP': 75, 'FN': 12, 'TP': 13}\n",
      "\n",
      "======================================== Fold 10/10 ========================================\n",
      "Best Parameters: {'C': 10, 'gamma': 'auto', 'kernel': 'rbf'}\n",
      "\n",
      "Fold 10 Metrics:\n",
      "Accuracy: 0.7906\n",
      "F1 Score: 0.1440\n",
      "Precision: 0.0891\n",
      "Recall/Sensitivity: 0.3750\n",
      "Specificity: 0.8111\n",
      "AUPRC: 0.1021\n",
      "Confusion Matrix: {'TN': 395, 'FP': 92, 'FN': 15, 'TP': 9}\n",
      "\n",
      "Plotting confusion matrix for best fold: 1\n",
      "\n",
      "==================================================\n",
      "Final Cross-Validation Results:\n",
      "==================================================\n",
      "Average Accuracy: 0.8145 ± 0.0130\n",
      "Average F1 Score: 0.1890 ± 0.0453\n",
      "Average Precision: 0.1201 ± 0.0285\n",
      "Average Recall/Sensitivity: 0.4455 ± 0.1113\n",
      "Average Specificity: 0.8334 ± 0.0129\n",
      "Average AUPRC: 0.1543 ± 0.0303\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import (accuracy_score, f1_score, precision_score,\n",
    "                             recall_score, average_precision_score,\n",
    "                             precision_recall_curve, auc, confusion_matrix)\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import interpolate\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# 设置中文字体\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']  # 设置中文字体为黑体\n",
    "plt.rcParams['axes.unicode_minus'] = False  # 解决负号显示问题\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, fold, dpi=720):\n",
    "    \"\"\"绘制正方形混淆矩阵\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    cm_percentage = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100  # 百分比表示\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    ax = sns.heatmap(cm_percentage, annot=False, fmt='.2f', cmap='Blues', square=True, cbar=False,\n",
    "                     linewidths=2, linecolor='black')\n",
    "\n",
    "    # 在每个格子中显示个数和百分比\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            # 判断字体颜色，深色背景用白色字体，浅色背景用黑色字体\n",
    "            text_color = 'white' if cm_percentage[i, j] > 50 else 'black'\n",
    "            ax.text(j + 0.5, i + 0.5, f'{cm[i, j]}\\n({cm_percentage[i, j]:.2f}%)',\n",
    "                    color=text_color, ha='center', va='center', fontsize=14, fontweight='bold')\n",
    "\n",
    "    # 添加中文标签\n",
    "    plt.xlabel('预测类别', fontsize=16, fontweight='bold')\n",
    "    plt.ylabel('实际类别', fontsize=16, fontweight='bold')\n",
    "    plt.xticks(ticks=np.arange(cm.shape[1]) + 0.5, labels=['0', '1'], fontsize=14, fontweight='bold')\n",
    "    plt.yticks(ticks=np.arange(cm.shape[0]) + 0.5, labels=['0', '1'], fontsize=14, fontweight='bold')\n",
    "\n",
    "    # 调整布局，减少空白边缘\n",
    "    plt.subplots_adjust(left=0.1, right=0.9, top=0.9, bottom=0.1)\n",
    "\n",
    "    # 保存混淆矩阵图\n",
    "    plt.savefig(f'SVM_best_fold_confusion_matrix_fold{fold}.png', dpi=dpi)\n",
    "    plt.close()\n",
    "\n",
    "def interpolate_pr_curve(precision, recall, num_points=100):\n",
    "    \"\"\"将PR曲线插值到固定长度的点\"\"\"\n",
    "    if len(precision) < 2 or len(recall) < 2:\n",
    "        return np.linspace(0, 1, num_points), np.linspace(1, 0, num_points)\n",
    "\n",
    "    # 确保recall是单调递增的\n",
    "    sorted_indices = np.argsort(recall)\n",
    "    recall = np.array(recall)[sorted_indices]\n",
    "    precision = np.array(precision)[sorted_indices]\n",
    "\n",
    "    # 插值\n",
    "    f = interpolate.interp1d(recall, precision, bounds_error=False, fill_value=(precision[0], precision[-1]))\n",
    "    new_recall = np.linspace(0, 1, num_points)\n",
    "    new_precision = f(new_recall)\n",
    "    return new_precision, new_recall\n",
    "\n",
    "def calculate_metrics(y_true, y_pred, y_scores):\n",
    "    \"\"\"计算评估指标\"\"\"\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_true, y_pred),\n",
    "        'f1': f1_score(y_true, y_pred, zero_division=0),\n",
    "        'precision': precision_score(y_true, y_pred, zero_division=0),\n",
    "        'recall': recall_score(y_true, y_pred, zero_division=0),\n",
    "        'auprc': average_precision_score(y_true, y_scores)\n",
    "    }\n",
    "\n",
    "    # 添加混淆矩阵信息\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    metrics.update({\n",
    "        'confusion_matrix': {\n",
    "            'TN': tn, 'FP': fp, 'FN': fn, 'TP': tp\n",
    "        },\n",
    "        'specificity': tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    })\n",
    "    return metrics\n",
    "\n",
    "def plot_pr_curve(y_true, y_scores, fold, save_path=None):\n",
    "    \"\"\"绘制PR曲线并返回插值后的数据\"\"\"\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_scores)\n",
    "    auprc = auc(recall, precision)\n",
    "\n",
    "    # 插值到固定长度\n",
    "    interp_precision, interp_recall = interpolate_pr_curve(precision, recall)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(recall, precision, label=f'Fold {fold} (AUPRC = {auprc:.3f})')\n",
    "    plt.xlabel('召回率', fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('精确率', fontsize=14, fontweight='bold')\n",
    "    plt.title(f'PR Curve (Fold {fold})', fontsize=16, fontweight='bold')\n",
    "    plt.legend()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "    return interp_precision, interp_recall, auprc\n",
    "\n",
    "def train_svm_model(X_train, y_train, X_val, y_val):\n",
    "    \"\"\"训练SVM模型\"\"\"\n",
    "    svm = SVC(probability=True, random_state=42)\n",
    "\n",
    "    # 使用网格搜索调参来寻找最优参数\n",
    "    param_grid = {\n",
    "        'C': [0.1, 1, 10],\n",
    "        'kernel': ['linear', 'rbf'],\n",
    "        'gamma': ['scale', 'auto']\n",
    "    }\n",
    "    grid_search = GridSearchCV(svm, param_grid, cv=3, scoring='accuracy', n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "\n",
    "    # 使用最优参数训练SVM\n",
    "    best_model = grid_search.best_estimator_\n",
    "    best_model.fit(X_train, y_train)\n",
    "\n",
    "    return best_model\n",
    "\n",
    "def cross_validate(X, y, n_splits=10):\n",
    "    \"\"\"执行交叉验证\"\"\"\n",
    "    kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    results = []\n",
    "    all_interp_precisions = []\n",
    "    interp_recall = np.linspace(0, 1, 100)  # 固定100个recall点\n",
    "    \n",
    "    best_fold_metrics = None\n",
    "    best_fold = -1\n",
    "    best_y_true = None\n",
    "    best_y_pred = None\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(kf.split(X, y), 1):\n",
    "        print(f'\\n{\"=\" * 40} Fold {fold}/{n_splits} {\"=\" * 40}')\n",
    "\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "        # 标准化\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "        # SMOTE过采样\n",
    "        smote = SMOTE(random_state=42, k_neighbors=min(5, sum(y_train == 1) - 1))\n",
    "        X_res, y_res = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "        # 训练SVM模型\n",
    "        model = train_svm_model(X_res, y_res, X_test_scaled, y_test)\n",
    "\n",
    "        # 预测\n",
    "        y_scores = model.predict_proba(X_test_scaled)[:, 1]  # 取正类的预测概率\n",
    "        y_pred = (y_scores >= 0.5).astype(int)\n",
    "\n",
    "        # 计算指标\n",
    "        metrics = calculate_metrics(y_test, y_pred, y_scores)\n",
    "        results.append(metrics)\n",
    "        \n",
    "        # 跟踪最佳折叠（根据AUPRC）\n",
    "        if best_fold_metrics is None or metrics['auprc'] > best_fold_metrics['auprc']:\n",
    "            best_fold_metrics = metrics\n",
    "            best_fold = fold\n",
    "            best_y_true = y_test\n",
    "            best_y_pred = y_pred\n",
    "\n",
    "        # PR曲线\n",
    "        interp_precision, _, _ = plot_pr_curve(\n",
    "            y_test, y_scores, fold,\n",
    "            save_path=f'pr_curve_fold{fold}.png'\n",
    "        )\n",
    "        all_interp_precisions.append(interp_precision)\n",
    "\n",
    "        # 打印结果\n",
    "        print(f\"\\nFold {fold} Metrics:\")\n",
    "        print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
    "        print(f\"F1 Score: {metrics['f1']:.4f}\")\n",
    "        print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "        print(f\"Recall/Sensitivity: {metrics['recall']:.4f}\")\n",
    "        print(f\"Specificity: {metrics['specificity']:.4f}\")\n",
    "        print(f\"AUPRC: {metrics['auprc']:.4f}\")\n",
    "        print(f\"Confusion Matrix: {metrics['confusion_matrix']}\")\n",
    "\n",
    "    # 绘制最佳折的混淆矩阵\n",
    "    if best_y_true is not None and best_y_pred is not None:\n",
    "        print(f\"\\nPlotting confusion matrix for best fold: {best_fold}\")\n",
    "        plot_confusion_matrix(best_y_true, best_y_pred, best_fold, dpi=720)\n",
    "\n",
    "    # 计算平均指标\n",
    "    avg_metrics = {\n",
    "        'accuracy': np.mean([r['accuracy'] for r in results]),\n",
    "        'f1': np.mean([r['f1'] for r in results]),\n",
    "        'precision': np.mean([r['precision'] for r in results]),\n",
    "        'recall': np.mean([r['recall'] for r in results]),\n",
    "        'specificity': np.mean([r['specificity'] for r in results]),\n",
    "        'auprc': np.mean([r['auprc'] for r in results]),\n",
    "    }\n",
    "\n",
    "    std_metrics = {\n",
    "        'accuracy': np.std([r['accuracy'] for r in results]),\n",
    "        'f1': np.std([r['f1'] for r in results]),\n",
    "        'precision': np.std([r['precision'] for r in results]),\n",
    "        'recall': np.std([r['recall'] for r in results]),\n",
    "        'specificity': np.std([r['specificity'] for r in results]),\n",
    "        'auprc': np.std([r['auprc'] for r in results]),\n",
    "    }\n",
    "\n",
    "    # 绘制平均PR曲线\n",
    "    if all_interp_precisions:\n",
    "        mean_precision = np.mean(all_interp_precisions, axis=0)\n",
    "        mean_auprc = auc(interp_recall, mean_precision)\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        for i, prec in enumerate(all_interp_precisions, 1):\n",
    "            plt.plot(interp_recall, prec, alpha=0.2, label=f'Fold {i}')\n",
    "\n",
    "        plt.plot(interp_recall, mean_precision, 'r-',\n",
    "                 linewidth=3, label=f'Mean (AUPRC = {mean_auprc:.3f})')\n",
    "        plt.xlabel('召回率', fontsize=14, fontweight='bold')\n",
    "        plt.ylabel('精确率', fontsize=14, fontweight='bold')\n",
    "        plt.title('平均 Precision-Recall 曲线', fontsize=16, fontweight='bold')\n",
    "        plt.legend()\n",
    "        plt.savefig('average_pr_curve.png')\n",
    "        plt.close()\n",
    "\n",
    "    return avg_metrics, std_metrics\n",
    "\n",
    "def main():\n",
    "    # 加载数据\n",
    "    data = pd.read_csv('preparations/stroke_output.csv')\n",
    "\n",
    "    # 预处理\n",
    "    categorical_cols = ['ever_married', 'work_type', 'smoking_status']\n",
    "    data[categorical_cols] = data[categorical_cols].astype('category')\n",
    "    X = data.drop('stroke', axis=1)\n",
    "    y = data['stroke'].values\n",
    "\n",
    "    # 独热编码\n",
    "    X = pd.get_dummies(X, columns=categorical_cols)\n",
    "    X = X.values.astype(np.float32)\n",
    "\n",
    "    # 执行交叉验证\n",
    "    print(\"Starting cross-validation...\")\n",
    "    avg_metrics, std_metrics = cross_validate(X, y, n_splits=10)\n",
    "\n",
    "    # 打印最终结果\n",
    "    print('\\n' + '=' * 50)\n",
    "    print('Final Cross-Validation Results:')\n",
    "    print('=' * 50)\n",
    "    print(f\"Average Accuracy: {avg_metrics['accuracy']:.4f} ± {std_metrics['accuracy']:.4f}\")\n",
    "    print(f\"Average F1 Score: {avg_metrics['f1']:.4f} ± {std_metrics['f1']:.4f}\")\n",
    "    print(f\"Average Precision: {avg_metrics['precision']:.4f} ± {std_metrics['precision']:.4f}\")\n",
    "    print(f\"Average Recall/Sensitivity: {avg_metrics['recall']:.4f} ± {std_metrics['recall']:.4f}\")\n",
    "    print(f\"Average Specificity: {avg_metrics['specificity']:.4f} ± {std_metrics['specificity']:.4f}\")\n",
    "    print(f\"Average AUPRC: {avg_metrics['auprc']:.4f} ± {std_metrics['auprc']:.4f}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "65af5d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting cross-validation...\n",
      "\n",
      "======================================== Fold 1/10 ========================================\n",
      "[0]\ttrain-auc:0.88631\ttrain-logloss:0.66888\teval-auc:0.82753\teval-logloss:0.67050\n",
      "[50]\ttrain-auc:0.95287\ttrain-logloss:0.33305\teval-auc:0.83539\teval-logloss:0.39765\n",
      "[100]\ttrain-auc:0.97344\ttrain-logloss:0.26400\teval-auc:0.82749\teval-logloss:0.34244\n",
      "[150]\ttrain-auc:0.98386\ttrain-logloss:0.21969\teval-auc:0.82074\teval-logloss:0.30879\n",
      "[200]\ttrain-auc:0.98950\ttrain-logloss:0.19017\teval-auc:0.80996\teval-logloss:0.28692\n",
      "[250]\ttrain-auc:0.99321\ttrain-logloss:0.16653\teval-auc:0.80222\teval-logloss:0.27429\n",
      "[300]\ttrain-auc:0.99541\ttrain-logloss:0.14761\teval-auc:0.79514\teval-logloss:0.26700\n",
      "[350]\ttrain-auc:0.99678\ttrain-logloss:0.13198\teval-auc:0.79761\teval-logloss:0.25917\n",
      "[400]\ttrain-auc:0.99773\ttrain-logloss:0.11844\teval-auc:0.79366\teval-logloss:0.25722\n",
      "[450]\ttrain-auc:0.99833\ttrain-logloss:0.10852\teval-auc:0.79078\teval-logloss:0.25423\n",
      "[500]\ttrain-auc:0.99881\ttrain-logloss:0.09896\teval-auc:0.78840\teval-logloss:0.25217\n",
      "[550]\ttrain-auc:0.99915\ttrain-logloss:0.09071\teval-auc:0.78848\teval-logloss:0.25106\n",
      "[600]\ttrain-auc:0.99940\ttrain-logloss:0.08358\teval-auc:0.78815\teval-logloss:0.25058\n",
      "[621]\ttrain-auc:0.99948\ttrain-logloss:0.08113\teval-auc:0.78848\teval-logloss:0.25061\n",
      "\n",
      "Fold 1 Metrics:\n",
      "Accuracy: 0.8924\n",
      "F1 Score: 0.2667\n",
      "Precision: 0.2000\n",
      "Recall/Sensitivity: 0.4000\n",
      "Specificity: 0.9177\n",
      "AUPRC: 0.2046\n",
      "Confusion Matrix:\n",
      "TN: 446 | FP: 40\n",
      "FN: 15 | TP: 10\n",
      "\n",
      "======================================== Fold 2/10 ========================================\n",
      "[0]\ttrain-auc:0.88337\ttrain-logloss:0.66810\teval-auc:0.78922\teval-logloss:0.66928\n",
      "[50]\ttrain-auc:0.96334\ttrain-logloss:0.31857\teval-auc:0.77350\teval-logloss:0.38184\n",
      "[100]\ttrain-auc:0.97815\ttrain-logloss:0.24925\teval-auc:0.76337\teval-logloss:0.33545\n",
      "[150]\ttrain-auc:0.98670\ttrain-logloss:0.20992\teval-auc:0.75152\teval-logloss:0.30640\n",
      "[200]\ttrain-auc:0.99185\ttrain-logloss:0.17881\teval-auc:0.74560\teval-logloss:0.28819\n",
      "[250]\ttrain-auc:0.99465\ttrain-logloss:0.15613\teval-auc:0.73992\teval-logloss:0.27432\n",
      "[300]\ttrain-auc:0.99644\ttrain-logloss:0.13836\teval-auc:0.73753\teval-logloss:0.26761\n",
      "[350]\ttrain-auc:0.99761\ttrain-logloss:0.12269\teval-auc:0.73745\teval-logloss:0.26133\n",
      "[400]\ttrain-auc:0.99826\ttrain-logloss:0.11164\teval-auc:0.73663\teval-logloss:0.25907\n",
      "[450]\ttrain-auc:0.99876\ttrain-logloss:0.10185\teval-auc:0.73621\teval-logloss:0.25672\n",
      "[500]\ttrain-auc:0.99912\ttrain-logloss:0.09291\teval-auc:0.73877\teval-logloss:0.25542\n",
      "[550]\ttrain-auc:0.99938\ttrain-logloss:0.08537\teval-auc:0.74033\teval-logloss:0.25412\n",
      "[584]\ttrain-auc:0.99950\ttrain-logloss:0.08042\teval-auc:0.74132\teval-logloss:0.25520\n",
      "\n",
      "Fold 2 Metrics:\n",
      "Accuracy: 0.9100\n",
      "F1 Score: 0.1786\n",
      "Precision: 0.1613\n",
      "Recall/Sensitivity: 0.2000\n",
      "Specificity: 0.9465\n",
      "AUPRC: 0.1226\n",
      "Confusion Matrix:\n",
      "TN: 460 | FP: 26\n",
      "FN: 20 | TP: 5\n",
      "\n",
      "======================================== Fold 3/10 ========================================\n",
      "[0]\ttrain-auc:0.88968\ttrain-logloss:0.66872\teval-auc:0.87724\teval-logloss:0.67012\n",
      "[50]\ttrain-auc:0.95352\ttrain-logloss:0.33522\teval-auc:0.87276\teval-logloss:0.39737\n",
      "[100]\ttrain-auc:0.97264\ttrain-logloss:0.26484\teval-auc:0.87185\teval-logloss:0.33330\n",
      "[150]\ttrain-auc:0.98433\ttrain-logloss:0.21872\teval-auc:0.86626\teval-logloss:0.29156\n",
      "[200]\ttrain-auc:0.99040\ttrain-logloss:0.18739\teval-auc:0.85737\teval-logloss:0.26779\n",
      "[250]\ttrain-auc:0.99379\ttrain-logloss:0.16327\teval-auc:0.85119\teval-logloss:0.25093\n",
      "[300]\ttrain-auc:0.99563\ttrain-logloss:0.14504\teval-auc:0.85202\teval-logloss:0.24097\n",
      "[350]\ttrain-auc:0.99687\ttrain-logloss:0.13097\teval-auc:0.84996\teval-logloss:0.23209\n",
      "[400]\ttrain-auc:0.99769\ttrain-logloss:0.11898\teval-auc:0.84634\teval-logloss:0.22567\n",
      "[450]\ttrain-auc:0.99838\ttrain-logloss:0.10866\teval-auc:0.84823\teval-logloss:0.21991\n",
      "[500]\ttrain-auc:0.99885\ttrain-logloss:0.09968\teval-auc:0.84667\teval-logloss:0.21558\n",
      "[550]\ttrain-auc:0.99915\ttrain-logloss:0.09232\teval-auc:0.84675\teval-logloss:0.21257\n",
      "[600]\ttrain-auc:0.99936\ttrain-logloss:0.08575\teval-auc:0.84519\teval-logloss:0.21108\n",
      "[650]\ttrain-auc:0.99955\ttrain-logloss:0.07948\teval-auc:0.84305\teval-logloss:0.21009\n",
      "[700]\ttrain-auc:0.99969\ttrain-logloss:0.07403\teval-auc:0.84066\teval-logloss:0.21118\n",
      "[730]\ttrain-auc:0.99975\ttrain-logloss:0.07098\teval-auc:0.84008\teval-logloss:0.21061\n",
      "\n",
      "Fold 3 Metrics:\n",
      "Accuracy: 0.8982\n",
      "F1 Score: 0.1875\n",
      "Precision: 0.1538\n",
      "Recall/Sensitivity: 0.2400\n",
      "Specificity: 0.9321\n",
      "AUPRC: 0.2182\n",
      "Confusion Matrix:\n",
      "TN: 453 | FP: 33\n",
      "FN: 19 | TP: 6\n",
      "\n",
      "======================================== Fold 4/10 ========================================\n",
      "[0]\ttrain-auc:0.89098\ttrain-logloss:0.66737\teval-auc:0.78646\teval-logloss:0.67126\n",
      "[50]\ttrain-auc:0.95644\ttrain-logloss:0.32358\teval-auc:0.81239\teval-logloss:0.38549\n",
      "[100]\ttrain-auc:0.97474\ttrain-logloss:0.25618\teval-auc:0.80033\teval-logloss:0.33727\n",
      "[150]\ttrain-auc:0.98460\ttrain-logloss:0.21437\teval-auc:0.79045\teval-logloss:0.30502\n",
      "[200]\ttrain-auc:0.99009\ttrain-logloss:0.18459\teval-auc:0.77992\teval-logloss:0.28557\n",
      "[250]\ttrain-auc:0.99358\ttrain-logloss:0.16023\teval-auc:0.77671\teval-logloss:0.27290\n",
      "[300]\ttrain-auc:0.99569\ttrain-logloss:0.14177\teval-auc:0.77572\teval-logloss:0.26303\n",
      "[350]\ttrain-auc:0.99694\ttrain-logloss:0.12726\teval-auc:0.77547\teval-logloss:0.25582\n",
      "[400]\ttrain-auc:0.99784\ttrain-logloss:0.11566\teval-auc:0.77440\teval-logloss:0.25232\n",
      "[450]\ttrain-auc:0.99843\ttrain-logloss:0.10542\teval-auc:0.77802\teval-logloss:0.24863\n",
      "[500]\ttrain-auc:0.99882\ttrain-logloss:0.09618\teval-auc:0.77704\teval-logloss:0.24794\n",
      "[550]\ttrain-auc:0.99912\ttrain-logloss:0.08896\teval-auc:0.77679\teval-logloss:0.24755\n",
      "[586]\ttrain-auc:0.99931\ttrain-logloss:0.08380\teval-auc:0.77753\teval-logloss:0.24721\n",
      "\n",
      "Fold 4 Metrics:\n",
      "Accuracy: 0.8924\n",
      "F1 Score: 0.1270\n",
      "Precision: 0.1053\n",
      "Recall/Sensitivity: 0.1600\n",
      "Specificity: 0.9300\n",
      "AUPRC: 0.1235\n",
      "Confusion Matrix:\n",
      "TN: 452 | FP: 34\n",
      "FN: 21 | TP: 4\n",
      "\n",
      "======================================== Fold 5/10 ========================================\n",
      "[0]\ttrain-auc:0.88310\ttrain-logloss:0.66903\teval-auc:0.81926\teval-logloss:0.66928\n",
      "[50]\ttrain-auc:0.95784\ttrain-logloss:0.33041\teval-auc:0.83210\teval-logloss:0.38557\n",
      "[100]\ttrain-auc:0.97448\ttrain-logloss:0.26163\teval-auc:0.80683\teval-logloss:0.33783\n",
      "[150]\ttrain-auc:0.98448\ttrain-logloss:0.21893\teval-auc:0.78848\teval-logloss:0.30961\n",
      "[200]\ttrain-auc:0.99082\ttrain-logloss:0.18615\teval-auc:0.77737\teval-logloss:0.28799\n",
      "[250]\ttrain-auc:0.99428\ttrain-logloss:0.16173\teval-auc:0.77029\teval-logloss:0.27625\n",
      "[300]\ttrain-auc:0.99633\ttrain-logloss:0.14301\teval-auc:0.76741\teval-logloss:0.26997\n",
      "[350]\ttrain-auc:0.99752\ttrain-logloss:0.12713\teval-auc:0.76198\teval-logloss:0.26440\n",
      "[400]\ttrain-auc:0.99820\ttrain-logloss:0.11528\teval-auc:0.76165\teval-logloss:0.26064\n",
      "[445]\ttrain-auc:0.99869\ttrain-logloss:0.10542\teval-auc:0.75366\teval-logloss:0.26235\n",
      "\n",
      "Fold 5 Metrics:\n",
      "Accuracy: 0.8865\n",
      "F1 Score: 0.0938\n",
      "Precision: 0.0769\n",
      "Recall/Sensitivity: 0.1200\n",
      "Specificity: 0.9259\n",
      "AUPRC: 0.1051\n",
      "Confusion Matrix:\n",
      "TN: 450 | FP: 36\n",
      "FN: 22 | TP: 3\n",
      "\n",
      "======================================== Fold 6/10 ========================================\n",
      "[0]\ttrain-auc:0.88328\ttrain-logloss:0.66839\teval-auc:0.79333\teval-logloss:0.66986\n",
      "[50]\ttrain-auc:0.95677\ttrain-logloss:0.32560\teval-auc:0.81387\teval-logloss:0.37412\n",
      "[100]\ttrain-auc:0.97756\ttrain-logloss:0.25539\teval-auc:0.79984\teval-logloss:0.32803\n",
      "[150]\ttrain-auc:0.98688\ttrain-logloss:0.21095\teval-auc:0.78453\teval-logloss:0.30803\n",
      "[200]\ttrain-auc:0.99231\ttrain-logloss:0.17820\teval-auc:0.77951\teval-logloss:0.28959\n",
      "[250]\ttrain-auc:0.99505\ttrain-logloss:0.15513\teval-auc:0.77572\teval-logloss:0.27896\n",
      "[300]\ttrain-auc:0.99642\ttrain-logloss:0.13800\teval-auc:0.77276\teval-logloss:0.27395\n",
      "[350]\ttrain-auc:0.99751\ttrain-logloss:0.12326\teval-auc:0.76996\teval-logloss:0.26819\n",
      "[400]\ttrain-auc:0.99820\ttrain-logloss:0.11162\teval-auc:0.76889\teval-logloss:0.26410\n",
      "[450]\ttrain-auc:0.99868\ttrain-logloss:0.10115\teval-auc:0.76329\teval-logloss:0.26317\n",
      "[500]\ttrain-auc:0.99906\ttrain-logloss:0.09280\teval-auc:0.76140\teval-logloss:0.26261\n",
      "[550]\ttrain-auc:0.99933\ttrain-logloss:0.08505\teval-auc:0.76280\teval-logloss:0.26162\n",
      "[600]\ttrain-auc:0.99951\ttrain-logloss:0.07882\teval-auc:0.76420\teval-logloss:0.25897\n",
      "[650]\ttrain-auc:0.99965\ttrain-logloss:0.07321\teval-auc:0.76412\teval-logloss:0.26005\n",
      "[654]\ttrain-auc:0.99965\ttrain-logloss:0.07280\teval-auc:0.76560\teval-logloss:0.25964\n",
      "\n",
      "Fold 6 Metrics:\n",
      "Accuracy: 0.8787\n",
      "F1 Score: 0.0606\n",
      "Precision: 0.0488\n",
      "Recall/Sensitivity: 0.0800\n",
      "Specificity: 0.9198\n",
      "AUPRC: 0.1048\n",
      "Confusion Matrix:\n",
      "TN: 447 | FP: 39\n",
      "FN: 23 | TP: 2\n",
      "\n",
      "======================================== Fold 7/10 ========================================\n",
      "[0]\ttrain-auc:0.88810\ttrain-logloss:0.66889\teval-auc:0.83815\teval-logloss:0.66975\n",
      "[50]\ttrain-auc:0.95889\ttrain-logloss:0.32186\teval-auc:0.82016\teval-logloss:0.37073\n",
      "[100]\ttrain-auc:0.97359\ttrain-logloss:0.25884\teval-auc:0.81654\teval-logloss:0.32089\n",
      "[150]\ttrain-auc:0.98544\ttrain-logloss:0.21281\teval-auc:0.79802\teval-logloss:0.29058\n",
      "[200]\ttrain-auc:0.99029\ttrain-logloss:0.18427\teval-auc:0.78337\teval-logloss:0.27247\n",
      "[250]\ttrain-auc:0.99396\ttrain-logloss:0.15943\teval-auc:0.78074\teval-logloss:0.25778\n",
      "[300]\ttrain-auc:0.99608\ttrain-logloss:0.13991\teval-auc:0.78016\teval-logloss:0.24653\n",
      "[350]\ttrain-auc:0.99724\ttrain-logloss:0.12613\teval-auc:0.77350\teval-logloss:0.24348\n",
      "[400]\ttrain-auc:0.99802\ttrain-logloss:0.11448\teval-auc:0.77317\teval-logloss:0.24008\n",
      "[450]\ttrain-auc:0.99858\ttrain-logloss:0.10474\teval-auc:0.77284\teval-logloss:0.23691\n",
      "[500]\ttrain-auc:0.99894\ttrain-logloss:0.09645\teval-auc:0.77276\teval-logloss:0.23432\n",
      "[550]\ttrain-auc:0.99925\ttrain-logloss:0.08810\teval-auc:0.77152\teval-logloss:0.23353\n",
      "[600]\ttrain-auc:0.99947\ttrain-logloss:0.08151\teval-auc:0.77259\teval-logloss:0.23262\n",
      "[650]\ttrain-auc:0.99963\ttrain-logloss:0.07575\teval-auc:0.77276\teval-logloss:0.23210\n",
      "[700]\ttrain-auc:0.99972\ttrain-logloss:0.07057\teval-auc:0.77342\teval-logloss:0.23226\n",
      "[750]\ttrain-auc:0.99980\ttrain-logloss:0.06588\teval-auc:0.77695\teval-logloss:0.23035\n",
      "[800]\ttrain-auc:0.99987\ttrain-logloss:0.06147\teval-auc:0.77654\teval-logloss:0.23135\n",
      "[815]\ttrain-auc:0.99988\ttrain-logloss:0.06031\teval-auc:0.77646\teval-logloss:0.23129\n",
      "\n",
      "Fold 7 Metrics:\n",
      "Accuracy: 0.9159\n",
      "F1 Score: 0.1569\n",
      "Precision: 0.1538\n",
      "Recall/Sensitivity: 0.1600\n",
      "Specificity: 0.9547\n",
      "AUPRC: 0.1155\n",
      "Confusion Matrix:\n",
      "TN: 464 | FP: 22\n",
      "FN: 21 | TP: 4\n",
      "\n",
      "======================================== Fold 8/10 ========================================\n",
      "[0]\ttrain-auc:0.87856\ttrain-logloss:0.66963\teval-auc:0.85885\teval-logloss:0.67065\n",
      "[50]\ttrain-auc:0.95496\ttrain-logloss:0.33234\teval-auc:0.85778\teval-logloss:0.38932\n",
      "[100]\ttrain-auc:0.97326\ttrain-logloss:0.26449\teval-auc:0.84239\teval-logloss:0.33722\n",
      "[150]\ttrain-auc:0.98418\ttrain-logloss:0.22079\teval-auc:0.83358\teval-logloss:0.30328\n",
      "[200]\ttrain-auc:0.98984\ttrain-logloss:0.18956\teval-auc:0.82914\teval-logloss:0.27668\n",
      "[250]\ttrain-auc:0.99331\ttrain-logloss:0.16499\teval-auc:0.82165\teval-logloss:0.26332\n",
      "[300]\ttrain-auc:0.99548\ttrain-logloss:0.14559\teval-auc:0.82041\teval-logloss:0.25288\n",
      "[350]\ttrain-auc:0.99672\ttrain-logloss:0.13139\teval-auc:0.81811\teval-logloss:0.24700\n",
      "[400]\ttrain-auc:0.99762\ttrain-logloss:0.11901\teval-auc:0.82148\teval-logloss:0.24119\n",
      "[450]\ttrain-auc:0.99828\ttrain-logloss:0.10875\teval-auc:0.81926\teval-logloss:0.23782\n",
      "[500]\ttrain-auc:0.99875\ttrain-logloss:0.09989\teval-auc:0.82008\teval-logloss:0.23521\n",
      "[550]\ttrain-auc:0.99910\ttrain-logloss:0.09177\teval-auc:0.82041\teval-logloss:0.23267\n",
      "[600]\ttrain-auc:0.99934\ttrain-logloss:0.08522\teval-auc:0.82115\teval-logloss:0.23067\n",
      "[650]\ttrain-auc:0.99951\ttrain-logloss:0.07954\teval-auc:0.82016\teval-logloss:0.22971\n",
      "[681]\ttrain-auc:0.99961\ttrain-logloss:0.07586\teval-auc:0.81819\teval-logloss:0.22968\n",
      "\n",
      "Fold 8 Metrics:\n",
      "Accuracy: 0.9022\n",
      "F1 Score: 0.2424\n",
      "Precision: 0.1951\n",
      "Recall/Sensitivity: 0.3200\n",
      "Specificity: 0.9321\n",
      "AUPRC: 0.1725\n",
      "Confusion Matrix:\n",
      "TN: 453 | FP: 33\n",
      "FN: 17 | TP: 8\n",
      "\n",
      "======================================== Fold 9/10 ========================================\n",
      "[0]\ttrain-auc:0.89438\ttrain-logloss:0.66742\teval-auc:0.77609\teval-logloss:0.67100\n",
      "[50]\ttrain-auc:0.96045\ttrain-logloss:0.31903\teval-auc:0.79893\teval-logloss:0.40265\n",
      "[100]\ttrain-auc:0.97863\ttrain-logloss:0.24843\teval-auc:0.80716\teval-logloss:0.34287\n",
      "[150]\ttrain-auc:0.98752\ttrain-logloss:0.20638\teval-auc:0.80025\teval-logloss:0.31062\n",
      "[200]\ttrain-auc:0.99189\ttrain-logloss:0.17793\teval-auc:0.79440\teval-logloss:0.28786\n",
      "[250]\ttrain-auc:0.99458\ttrain-logloss:0.15466\teval-auc:0.79877\teval-logloss:0.27243\n",
      "[300]\ttrain-auc:0.99616\ttrain-logloss:0.13754\teval-auc:0.80025\teval-logloss:0.26132\n",
      "[350]\ttrain-auc:0.99742\ttrain-logloss:0.12156\teval-auc:0.79926\teval-logloss:0.25443\n",
      "[400]\ttrain-auc:0.99814\ttrain-logloss:0.10971\teval-auc:0.79728\teval-logloss:0.24941\n",
      "[450]\ttrain-auc:0.99868\ttrain-logloss:0.09980\teval-auc:0.79556\teval-logloss:0.24539\n",
      "[500]\ttrain-auc:0.99905\ttrain-logloss:0.09119\teval-auc:0.79877\teval-logloss:0.24174\n",
      "[550]\ttrain-auc:0.99931\ttrain-logloss:0.08360\teval-auc:0.79753\teval-logloss:0.24189\n",
      "[576]\ttrain-auc:0.99940\ttrain-logloss:0.08021\teval-auc:0.79605\teval-logloss:0.24227\n",
      "\n",
      "Fold 9 Metrics:\n",
      "Accuracy: 0.9061\n",
      "F1 Score: 0.2258\n",
      "Precision: 0.1892\n",
      "Recall/Sensitivity: 0.2800\n",
      "Specificity: 0.9383\n",
      "AUPRC: 0.1592\n",
      "Confusion Matrix:\n",
      "TN: 456 | FP: 30\n",
      "FN: 18 | TP: 7\n",
      "\n",
      "======================================== Fold 10/10 ========================================\n",
      "[0]\ttrain-auc:0.88660\ttrain-logloss:0.66773\teval-auc:0.77614\teval-logloss:0.67142\n",
      "[50]\ttrain-auc:0.95830\ttrain-logloss:0.32028\teval-auc:0.79013\teval-logloss:0.40966\n",
      "[100]\ttrain-auc:0.97824\ttrain-logloss:0.25067\teval-auc:0.78679\teval-logloss:0.34900\n",
      "[150]\ttrain-auc:0.98666\ttrain-logloss:0.20975\teval-auc:0.78089\teval-logloss:0.31409\n",
      "[200]\ttrain-auc:0.99144\ttrain-logloss:0.18024\teval-auc:0.78422\teval-logloss:0.28947\n",
      "[250]\ttrain-auc:0.99427\ttrain-logloss:0.15916\teval-auc:0.78131\teval-logloss:0.27828\n",
      "[300]\ttrain-auc:0.99598\ttrain-logloss:0.14053\teval-auc:0.77823\teval-logloss:0.26922\n",
      "[350]\ttrain-auc:0.99713\ttrain-logloss:0.12543\teval-auc:0.77738\teval-logloss:0.26248\n",
      "[400]\ttrain-auc:0.99793\ttrain-logloss:0.11308\teval-auc:0.77900\teval-logloss:0.25692\n",
      "[450]\ttrain-auc:0.99843\ttrain-logloss:0.10325\teval-auc:0.77755\teval-logloss:0.25559\n",
      "[500]\ttrain-auc:0.99884\ttrain-logloss:0.09388\teval-auc:0.77601\teval-logloss:0.25386\n",
      "[550]\ttrain-auc:0.99915\ttrain-logloss:0.08634\teval-auc:0.77353\teval-logloss:0.25467\n",
      "[570]\ttrain-auc:0.99924\ttrain-logloss:0.08358\teval-auc:0.77319\teval-logloss:0.25377\n",
      "\n",
      "Fold 10 Metrics:\n",
      "Accuracy: 0.8904\n",
      "F1 Score: 0.0667\n",
      "Precision: 0.0556\n",
      "Recall/Sensitivity: 0.0833\n",
      "Specificity: 0.9302\n",
      "AUPRC: 0.1066\n",
      "Confusion Matrix:\n",
      "TN: 453 | FP: 34\n",
      "FN: 22 | TP: 2\n",
      "\n",
      "Plotting confusion matrix for best fold: 3\n",
      "\n",
      "==================================================\n",
      "Final Cross-Validation Results:\n",
      "==================================================\n",
      "Average Accuracy: 0.8973 ± 0.0108\n",
      "Average F1 Score: 0.1606 ± 0.0690\n",
      "Average Precision: 0.1340 ± 0.0549\n",
      "Average Recall/Sensitivity: 0.2043 ± 0.0999\n",
      "Average Specificity: 0.9327 ± 0.0108\n",
      "Average AUPRC: 0.1433 ± 0.0404\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import (accuracy_score, f1_score, precision_score,\n",
    "                           recall_score, average_precision_score,\n",
    "                           precision_recall_curve, auc, confusion_matrix)\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import interpolate\n",
    "\n",
    "# 设置中文字体\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']  # 设置中文字体为黑体\n",
    "plt.rcParams['axes.unicode_minus'] = False  # 解决负号显示问题\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, fold, dpi=720):\n",
    "    \"\"\"绘制正方形混淆矩阵\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    cm_percentage = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100  # 百分比表示\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    ax = sns.heatmap(cm_percentage, annot=False, fmt='.2f', cmap='Blues', square=True, cbar=False,\n",
    "                     linewidths=2, linecolor='black')\n",
    "\n",
    "    # 在每个格子中显示个数和百分比\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            # 判断字体颜色，深色背景用白色字体，浅色背景用黑色字体\n",
    "            text_color = 'white' if cm_percentage[i, j] > 50 else 'black'\n",
    "            ax.text(j + 0.5, i + 0.5, f'{cm[i, j]}\\n({cm_percentage[i, j]:.2f}%)',\n",
    "                    color=text_color, ha='center', va='center', fontsize=14, fontweight='bold')\n",
    "\n",
    "    # 添加中文标签\n",
    "    plt.xlabel('预测类别', fontsize=16, fontweight='bold')\n",
    "    plt.ylabel('实际类别', fontsize=16, fontweight='bold')\n",
    "    plt.xticks(ticks=np.arange(cm.shape[1]) + 0.5, labels=['0', '1'], fontsize=14, fontweight='bold')\n",
    "    plt.yticks(ticks=np.arange(cm.shape[0]) + 0.5, labels=['0', '1'], fontsize=14, fontweight='bold')\n",
    "\n",
    "    # 调整布局，减少空白边缘\n",
    "    plt.subplots_adjust(left=0.1, right=0.9, top=0.9, bottom=0.1)\n",
    "\n",
    "    # 保存混淆矩阵图\n",
    "    plt.savefig(f'XGB_best_fold_confusion_matrix_fold{fold}.png', dpi=dpi)\n",
    "    plt.close()\n",
    "\n",
    "def interpolate_pr_curve(precision, recall, num_points=100):\n",
    "    \"\"\"将PR曲线插值到固定长度的点\"\"\"\n",
    "    if len(precision) < 2 or len(recall) < 2:\n",
    "        return np.linspace(0, 1, num_points), np.linspace(1, 0, num_points)\n",
    "\n",
    "    # 确保recall是单调递增的\n",
    "    sorted_indices = np.argsort(recall)\n",
    "    recall = np.array(recall)[sorted_indices]\n",
    "    precision = np.array(precision)[sorted_indices]\n",
    "\n",
    "    # 插值\n",
    "    f = interpolate.interp1d(recall, precision, bounds_error=False, fill_value=(precision[0], precision[-1]))\n",
    "    new_recall = np.linspace(0, 1, num_points)\n",
    "    new_precision = f(new_recall)\n",
    "    return new_precision, new_recall\n",
    "\n",
    "def calculate_metrics(y_true, y_pred, y_scores):\n",
    "    \"\"\"计算评估指标\"\"\"\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_true, y_pred),\n",
    "        'f1': f1_score(y_true, y_pred, zero_division=0),\n",
    "        'precision': precision_score(y_true, y_pred, zero_division=0),\n",
    "        'recall': recall_score(y_true, y_pred, zero_division=0),\n",
    "        'auprc': average_precision_score(y_true, y_scores)\n",
    "    }\n",
    "\n",
    "    # 添加混淆矩阵信息\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    metrics.update({\n",
    "        'confusion_matrix': {\n",
    "            'TN': tn, 'FP': fp, 'FN': fn, 'TP': tp\n",
    "        },\n",
    "        'specificity': tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    })\n",
    "    return metrics\n",
    "\n",
    "def plot_pr_curve(y_true, y_scores, fold, save_path=None):\n",
    "    \"\"\"绘制PR曲线并返回插值后的数据\"\"\"\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_scores)\n",
    "    auprc = auc(recall, precision)\n",
    "\n",
    "    # 插值到固定长度\n",
    "    interp_precision, interp_recall = interpolate_pr_curve(precision, recall)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(recall, precision, label=f'Fold {fold} (AUPRC = {auprc:.3f})')\n",
    "    plt.xlabel('召回率', fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('精确率', fontsize=14, fontweight='bold')\n",
    "    plt.title(f'PR Curve (Fold {fold})', fontsize=16, fontweight='bold')\n",
    "    plt.legend()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "    return interp_precision, interp_recall, auprc\n",
    "\n",
    "def train_xgb_model(X_train, y_train, X_val, y_val):\n",
    "    \"\"\"训练XGBoost模型\"\"\"\n",
    "    params = {\n",
    "        'objective': 'binary:logistic',\n",
    "        'eval_metric': ['auc', 'logloss'],\n",
    "        'learning_rate': 0.05,\n",
    "        'max_depth': 6,\n",
    "        'subsample': 0.8,\n",
    "        'colsample_bytree': 0.8,\n",
    "        'scale_pos_weight': float(np.sum(y_train == 0)) / np.sum(y_train == 1),  # 处理类别不平衡\n",
    "        'seed': 42,\n",
    "        'n_jobs': -1\n",
    "    }\n",
    "\n",
    "    # 将数据转换为DMatrix格式\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    dval = xgb.DMatrix(X_val, label=y_val)\n",
    "\n",
    "    # 训练XGBoost模型\n",
    "    watchlist = [(dtrain, 'train'), (dval, 'eval')]\n",
    "    model = xgb.train(\n",
    "        params,\n",
    "        dtrain,\n",
    "        num_boost_round=1000,\n",
    "        evals=watchlist,\n",
    "        early_stopping_rounds=50,\n",
    "        verbose_eval=50\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def cross_validate(X, y, n_splits=10):\n",
    "    \"\"\"执行交叉验证\"\"\"\n",
    "    kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    results = []\n",
    "    all_interp_precisions = []\n",
    "    interp_recall = np.linspace(0, 1, 100)  # 固定100个recall点\n",
    "    \n",
    "    best_fold_metrics = None\n",
    "    best_fold = -1\n",
    "    best_y_true = None\n",
    "    best_y_pred = None\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(kf.split(X, y), 1):\n",
    "        print(f'\\n{\"=\" * 40} Fold {fold}/{n_splits} {\"=\" * 40}')\n",
    "\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "        # 标准化\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "        # SMOTE过采样\n",
    "        smote = SMOTE(random_state=42, k_neighbors=min(5, sum(y_train == 1) - 1))\n",
    "        X_res, y_res = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "        # 训练模型\n",
    "        model = train_xgb_model(X_res, y_res, X_test_scaled, y_test)\n",
    "\n",
    "        # 预测\n",
    "        dtest = xgb.DMatrix(X_test_scaled)\n",
    "        y_scores = model.predict(dtest)\n",
    "        y_pred = (y_scores >= 0.5).astype(int)\n",
    "\n",
    "        # 计算指标\n",
    "        metrics = calculate_metrics(y_test, y_pred, y_scores)\n",
    "        results.append(metrics)\n",
    "        \n",
    "        # 跟踪最佳折叠（根据AUPRC）\n",
    "        if best_fold_metrics is None or metrics['auprc'] > best_fold_metrics['auprc']:\n",
    "            best_fold_metrics = metrics\n",
    "            best_fold = fold\n",
    "            best_y_true = y_test\n",
    "            best_y_pred = y_pred\n",
    "\n",
    "        # PR曲线\n",
    "        interp_precision, _, _ = plot_pr_curve(\n",
    "            y_test, y_scores, fold,\n",
    "            save_path=f'pr_curve_fold{fold}.png'\n",
    "        )\n",
    "        all_interp_precisions.append(interp_precision)\n",
    "\n",
    "        # 打印结果\n",
    "        print(f\"\\nFold {fold} Metrics:\")\n",
    "        print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
    "        print(f\"F1 Score: {metrics['f1']:.4f}\")\n",
    "        print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "        print(f\"Recall/Sensitivity: {metrics['recall']:.4f}\")\n",
    "        print(f\"Specificity: {metrics['specificity']:.4f}\")\n",
    "        print(f\"AUPRC: {metrics['auprc']:.4f}\")\n",
    "        print(f\"Confusion Matrix:\")\n",
    "        print(f\"TN: {metrics['confusion_matrix']['TN']} | FP: {metrics['confusion_matrix']['FP']}\")\n",
    "        print(f\"FN: {metrics['confusion_matrix']['FN']} | TP: {metrics['confusion_matrix']['TP']}\")\n",
    "\n",
    "    # 绘制最佳折的混淆矩阵\n",
    "    if best_y_true is not None and best_y_pred is not None:\n",
    "        print(f\"\\nPlotting confusion matrix for best fold: {best_fold}\")\n",
    "        plot_confusion_matrix(best_y_true, best_y_pred, best_fold, dpi=720)\n",
    "\n",
    "    # 计算平均指标\n",
    "    avg_metrics = {\n",
    "        'accuracy': np.mean([r['accuracy'] for r in results]),\n",
    "        'f1': np.mean([r['f1'] for r in results]),\n",
    "        'precision': np.mean([r['precision'] for r in results]),\n",
    "        'recall': np.mean([r['recall'] for r in results]),\n",
    "        'specificity': np.mean([r['specificity'] for r in results]),\n",
    "        'auprc': np.mean([r['auprc'] for r in results]),\n",
    "    }\n",
    "\n",
    "    std_metrics = {\n",
    "        'accuracy': np.std([r['accuracy'] for r in results]),\n",
    "        'f1': np.std([r['f1'] for r in results]),\n",
    "        'precision': np.std([r['precision'] for r in results]),\n",
    "        'recall': np.std([r['recall'] for r in results]),\n",
    "        'specificity': np.std([r['specificity'] for r in results]),\n",
    "        'auprc': np.std([r['auprc'] for r in results]),\n",
    "    }\n",
    "\n",
    "    # 绘制平均PR曲线\n",
    "    if all_interp_precisions:\n",
    "        mean_precision = np.mean(all_interp_precisions, axis=0)\n",
    "        mean_auprc = auc(interp_recall, mean_precision)\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        for i, prec in enumerate(all_interp_precisions, 1):\n",
    "            plt.plot(interp_recall, prec, alpha=0.2, label=f'Fold {i}')\n",
    "\n",
    "        plt.plot(interp_recall, mean_precision, 'r-',\n",
    "                 linewidth=3, label=f'Mean (AUPRC = {mean_auprc:.3f})')\n",
    "        plt.xlabel('召回率', fontsize=14, fontweight='bold')\n",
    "        plt.ylabel('精确率', fontsize=14, fontweight='bold')\n",
    "        plt.title('平均 Precision-Recall 曲线', fontsize=16, fontweight='bold')\n",
    "        plt.legend()\n",
    "        plt.savefig('average_pr_curve.png')\n",
    "        plt.close()\n",
    "\n",
    "    return avg_metrics, std_metrics\n",
    "\n",
    "def main():\n",
    "    # 加载数据\n",
    "    data = pd.read_csv('preparations/stroke_output.csv')\n",
    "\n",
    "    # 预处理\n",
    "    categorical_cols = ['ever_married', 'work_type', 'smoking_status']\n",
    "    data[categorical_cols] = data[categorical_cols].astype('category')\n",
    "    X = data.drop('stroke', axis=1)\n",
    "    y = data['stroke'].values\n",
    "\n",
    "    # 独热编码\n",
    "    X = pd.get_dummies(X, columns=categorical_cols)\n",
    "    X = X.values.astype(np.float32)\n",
    "\n",
    "    # 执行交叉验证\n",
    "    print(\"Starting cross-validation...\")\n",
    "    avg_metrics, std_metrics = cross_validate(X, y, n_splits=10)\n",
    "\n",
    "    # 打印最终结果\n",
    "    print('\\n' + '=' * 50)\n",
    "    print('Final Cross-Validation Results:')\n",
    "    print('=' * 50)\n",
    "    print(f\"Average Accuracy: {avg_metrics['accuracy']:.4f} ± {std_metrics['accuracy']:.4f}\")\n",
    "    print(f\"Average F1 Score: {avg_metrics['f1']:.4f} ± {std_metrics['f1']:.4f}\")\n",
    "    print(f\"Average Precision: {avg_metrics['precision']:.4f} ± {std_metrics['precision']:.4f}\")\n",
    "    print(f\"Average Recall/Sensitivity: {avg_metrics['recall']:.4f} ± {std_metrics['recall']:.4f}\")\n",
    "    print(f\"Average Specificity: {avg_metrics['specificity']:.4f} ± {std_metrics['specificity']:.4f}\")\n",
    "    print(f\"Average AUPRC: {avg_metrics['auprc']:.4f} ± {std_metrics['auprc']:.4f}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
